{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a62pERh-Lasc"
      ],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZzKFuddj9FawcuD8Mjm2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/kfpcomponent/blob/main/KaggleDatasetFetcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is the development workflow for kubeflow pipeline component of the same name as this notebook. Refer https://github.com/ShaswataJash/kfpcomponent"
      ],
      "metadata": {
        "id": "_fMb4fVRFsPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install required softwares"
      ],
      "metadata": {
        "id": "HYI5LKdFLCTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "id": "JoSae8JMvVgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a"
      ],
      "metadata": {
        "id": "ZZTBdke9vme4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "P4orJiv6orBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install fuse as dependency for rclone. Additionally, install curl, unzip for rclone installer to work\n",
        "!apt-get update \\\n",
        "    && apt-get install --no-install-recommends -y curl fuse unzip \\\n",
        "    && echo \"user_allow_other\" >> /etc/fuse.conf \\\n",
        "    && curl https://rclone.org/install.sh | bash \\\n",
        "    && apt-get -y remove --purge curl unzip \\\n",
        "    && rm -rf /var/lib/apt/lists/* \\\n",
        "    && rclone --version"
      ],
      "metadata": {
        "id": "3p89zTHfYqwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lus4TEd-8DbB"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Develop source code files"
      ],
      "metadata": {
        "id": "EMVyzkX_LIoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kaggle_download.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import tempfile\n",
        "import subprocess\n",
        "\n",
        "for arg in sys.argv:\n",
        "    print(arg)\n",
        "sys.stdout.flush()\n",
        "\n",
        "parser = argparse.ArgumentParser(description='kubeflow pipeline component to download competition or dataset files from kaggle')\n",
        "parser.add_argument('--log-level', default='INFO', choices=['ERROR', 'INFO', 'DEBUG'])\n",
        "parser.add_argument('--bypass-rclone-for-output-data', default=False, action=\"store_true\", help='whether output csv file should be written like local file - rclone is completely bypassed')\n",
        "parser.add_argument('--rclone-environment-var', type=str, default= '{}', help='json formatted key-value pairs of strings which will be set as environment variables before executing rclone commands')\n",
        "parser.add_argument('--kaggle-environment-var', type=str, default= '{}', help='json formatted key-value pairs of strings which will be set as environment variables before executing kaggle commands')\n",
        "parser.add_argument('--kaggle-resource-type', choices=['competitions', 'datasets'])\n",
        "parser.add_argument('--kaggle-resource-name', type=str, help='name of the the kaggle resource name') #refer: https://github.com/Kaggle/kaggle-api\n",
        "parser.add_argument('--output-datasource-directory-mountable', default=False, action=\"store_true\", help='whether output csv file will be written in mountable remote location when rclone is used')\n",
        "parser.add_argument('--output-datasource-directory', type=str, help='the directory/bucket path holding the kaggle downloaded files')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "#keeping the log format same as used in pycaret for consistency (refer: https://github.com/pycaret/pycaret/blob/master/pycaret/internal/logging.py)\n",
        "logging.basicConfig(level=args.log_level, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "\n",
        "#sanity check of arguments\n",
        "if args.bypass_rclone_for_output_data:\n",
        "    assert args.output_datasource_directory_mountable == False\n",
        "    \n",
        "if args.bypass_rclone_for_output_data:\n",
        "    assert args.rclone_environment_var == '{}'\n",
        "\n",
        "#setting rclone related env\n",
        "try:\n",
        "    rclone_config = json.loads(args.rclone_environment_var)\n",
        "    logging.info(\"rclone_config: type=%s content=%s\", type(rclone_config), rclone_config)\n",
        "    for item in rclone_config.items():\n",
        "        #converting explicitely item[1] to str because rclone config can have nested json. In that case, item[1] will be of dictonary type\n",
        "        #replacing quote with double quote to make the values json compatible (note for string without ', below replacement has no effect)\n",
        "        os.environ[item[0]] = str(item[1]).replace('\\'', '\"')\n",
        "        logging.debug('%s => %s', item[0], os.getenv(item[0]))\n",
        "except BaseException as err:\n",
        "    logging.error(\"rclone configuration loading related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while loading rclone_config\")  \n",
        "\n",
        "#setting kaggle related env\n",
        "try:\n",
        "    kaggle_config = json.loads(args.kaggle_environment_var)\n",
        "    logging.info(\"kaggle_config: type=%s content=%s\", type(kaggle_config), kaggle_config)\n",
        "    for item in kaggle_config.items():\n",
        "        #converting explicitely item[1] to str because kaggle config can have nested json. In that case, item[1] will be of dictonary type\n",
        "        #replacing quote with double quote to make the values json compatible (note for string without ', below replacement has no effect)\n",
        "        os.environ[item[0]] = str(item[1]).replace('\\'', '\"')\n",
        "        logging.debug('%s => %s', item[0], os.getenv(item[0]))\n",
        "except BaseException as err:\n",
        "    logging.error(\"kaggle configuration loading related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while loading kaggle_config\")  \n",
        "\n",
        "#temporary directory creation\n",
        "try:\n",
        "    if not args.bypass_rclone_for_output_data:\n",
        "        local_datastore_write_dir = tempfile.mkdtemp(prefix=\"my_local_write-\")\n",
        "        logging.debug('local_datastore_write_dir:%s',local_datastore_write_dir)\n",
        "except BaseException as err:\n",
        "    logging.error(\"temporary directory creation related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while creating temporary directories\")\n",
        "\n",
        "#output file handling\n",
        "if not args.bypass_rclone_for_output_data:\n",
        "    if args.output_datasource_directory_mountable:\n",
        "        output_data_write_cmd = \"rclone -v mount remotewrite:\" + args.output_datasource_directory + ' ' + local_datastore_write_dir + ' --daemon'\n",
        "        logging.info(output_data_write_cmd)\n",
        "        output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        logging.info(output_data_write_call.stdout)\n",
        "        if output_data_write_call.returncode != 0:\n",
        "            logging.error(\"Error in rclone, errorcode=%s\", output_data_write_call.returncode)\n",
        "            sys.stdout.flush()\n",
        "            sys.exit(\"Forceful exit as rclone returned error in context of mounted writing\")\n",
        "\n",
        "#handling of kaggle interaction\n",
        "try:\n",
        "    if args.bypass_rclone_for_output_data:\n",
        "        os.makedirs(args.output_datasource_directory, exist_ok=True)\n",
        "    kaggle_files_to_download_dir = args.output_datasource_directory if args.bypass_rclone_for_output_data else local_datastore_write_dir\n",
        "    kaggle_write_cmd = \"kaggle \" + args.kaggle_resource_type + ' download -p ' + kaggle_files_to_download_dir + ' ' + args.kaggle_resource_name\n",
        "    logging.info(kaggle_write_cmd)\n",
        "    kaggle_write_call = subprocess.run(kaggle_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    logging.info(kaggle_write_call.stdout)\n",
        "    if kaggle_write_call.returncode != 0:\n",
        "        logging.error(\"Error in kaggle downlaod, errorcode=%s\", kaggle_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as kaggle download returned error\")\n",
        "except BaseException as err:\n",
        "    logging.error(\"kaggle download related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while kaggle download\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WU9Vh7P4P4_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docker size reduction tips:\n",
        "\n",
        "\n",
        "*   https://devopscube.com/reduce-docker-image-size/\n",
        "*   https://www.ecloudcontrol.com/best-practices-to-reduce-docker-images-size/\n",
        "\n"
      ],
      "metadata": {
        "id": "eQXf07hoPzxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.7.13-slim\n",
        "\n",
        "#install fuse as dependency for rclone. Additionally, install curl, unzip for rclone installer to work\n",
        "RUN apt-get update \\\n",
        "    && apt-get install --no-install-recommends -y curl fuse unzip \\\n",
        "    && echo \"user_allow_other\" >> /etc/fuse.conf \\\n",
        "    && curl https://rclone.org/install.sh | bash \\\n",
        "    && apt-get -y remove --purge curl unzip \\\n",
        "    && apt-get -y autoremove \\\n",
        "    && rm -rf /var/lib/apt/lists/* \\\n",
        "    && rclone --version\n",
        "\n",
        "#install kaggle client lib\n",
        "RUN python3 -m pip install kaggle==1.5.12\n",
        "    \n",
        "COPY src/kaggle_download.py /tmp"
      ],
      "metadata": {
        "id": "hk00hb780Qo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_tests.sh\n",
        "#!/bin/bash\n",
        "\n",
        "#In production kaggle.json should be created through kubernetes secret and mounted to KAGGLE_CONFIG_DIR environment var\n",
        "#For quick testing, KAGGLE_USERNAME and KAGGLE_KEY environment vars can be passed through --kaggle-environment-var (not recommended for production)\n",
        "\n",
        "#Test: download kaggle dataset\n",
        "python3 /tmp/kaggle_download.py --kaggle-resource-type 'datasets' --kaggle-resource-name 'anushonkar/network-anamoly-detection' \\\n",
        "    --kaggle-environment-var '{\"KAGGLE_CONFIG_DIR\":\"/mnt\"}' \\\n",
        "    --bypass-rclone-for-output-data --output-datasource-directory '/tmp/my_local_dir_for_test/' --log-level 'DEBUG'\n",
        "\n",
        "if [ $? -ne 0 ]\n",
        "then\n",
        "    exit 1\n",
        "else\n",
        "    echo \"============ test related to download kaggle-dataset done ===============\"\n",
        "fi\n",
        "\n",
        "#Test: download kaggle competitions\n",
        "#NOTE the kaggle-user need to accept competition rules before able to download competitions files \n",
        "python3 /tmp/kaggle_download.py --kaggle-resource-type 'competitions' --kaggle-resource-name 'tabular-playground-series-aug-2022' \\\n",
        "    --kaggle-environment-var '{\"KAGGLE_CONFIG_DIR\":\"/mnt\"}' \\\n",
        "    --bypass-rclone-for-output-data --output-datasource-directory '/tmp/my_local_dir_for_test/' --log-level 'DEBUG'\n",
        "\n",
        "if [ $? -ne 0 ]\n",
        "then\n",
        "    exit 1\n",
        "else\n",
        "    echo \"============ test related to download kaggle-competitions done ===============\"\n",
        "fi\n",
        "\n",
        "python /tmp/test_validation.py\n",
        "if [ $? -ne 0 ]\n",
        "then\n",
        "    exit 1\n",
        "else\n",
        "    exit 0\n",
        "fi"
      ],
      "metadata": {
        "id": "jzJg29vTVGcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_validation.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "from os.path import exists\n",
        "assert exists('/tmp/my_local_dir_for_test/network-anamoly-detection.zip') == True\n",
        "assert exists('/tmp/my_local_dir_for_test/tabular-playground-series-aug-2022.zip') == True\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/tmp/my_local_dir_for_test/network-anamoly-detection.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/my_local_dir_for_test/network-anamoly-detection')\n",
        "\n",
        "with zipfile.ZipFile('/tmp/my_local_dir_for_test/tabular-playground-series-aug-2022.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/my_local_dir_for_test/tabular-playground-series-aug-2022')\n",
        "\n",
        "\n",
        "import pandas\n",
        "\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/network-anamoly-detection/Train.txt')\n",
        "print (df.shape)\n",
        "assert len(df.index) > 10000 #check whether more than 10000 rows are present\n",
        "\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/tabular-playground-series-aug-2022/train.csv')\n",
        "print (df.shape)\n",
        "assert len(df.index) > 10000 #check whether more than 10000 rows are present\n",
        "\n",
        "print ('test-validation done successfully')"
      ],
      "metadata": {
        "id": "7maMc-RVqDV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#designing-a-pipeline-component\n",
        "*   https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py\n",
        "*   https://kubeflow-pipelines.readthedocs.io/en/stable/_modules/kfp/components/_structures.html\n",
        "\n"
      ],
      "metadata": {
        "id": "W7uyWGT6Sccr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component_output_as_artifact.yaml\n",
        "name: KaggleDatasetFetcherWhereOutputAsArtifact\n",
        "description: |\n",
        "    Download kaggle Competitions and Dataset files. Download will be zipped form.\n",
        "    For kaggle related API, refer https://github.com/Kaggle/kaggle-api\n",
        "    In production kaggle.json should be created through kubernetes secret and mounted to KAGGLE_CONFIG_DIR environment var.\n",
        "    For quick testing, KAGGLE_USERNAME and KAGGLE_KEY environment vars can be passed through --kaggle-environment-var (not recommended for production)\n",
        "    Output csv files are stored in output artifacts. Thus the csv files are written like locally mounted POSIX files.\n",
        "metadata:\n",
        "  annotations:\n",
        "    author: Shaswata Jash <29448766+ShaswataJash@users.noreply.github.com>\n",
        "    canonical_location: https://raw.githubusercontent.com/ShaswataJash/kfpcomponent/main/KaggleDatasetFetcher/component_output_as_artifact.yaml\n",
        "\n",
        "inputs:\n",
        "- name: log_level\n",
        "  type: String\n",
        "  description: 'choice amongst ERROR, INFO, DEBUG'\n",
        "  optional: true\n",
        "- name: kaggle_environment_var\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be set as environment variables before executing kaggle commands'\n",
        "- name: kaggle_resource_type\n",
        "  type: String \n",
        "  description: 'choice amongst competitions, datasets'\n",
        "- name: kaggle_resource_name\n",
        "  type: String\n",
        "  description: 'name of the competition or dataset that will be downloaded'\n",
        "\n",
        "outputs:\n",
        "- name: output_datasource_directory\n",
        "  description: 'absolute local directory where downloaded file will be stored when rclone is NOT used i.e. when output file is stored in output artifact of pipeline engine (e.g. argo)'\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: shasjash/kfpcomponents:KaggleDatasetFetcher_devlatest\n",
        "    command:\n",
        "    - python3 \n",
        "    - /tmp/kaggle_download.py\n",
        "    args:\n",
        "    - --bypass-rclone-for-output-data\n",
        "    - if:\n",
        "        cond: {isPresent: log_level}\n",
        "        then:\n",
        "        - --log-level\n",
        "        - {inputValue: log_level}\n",
        "    - --kaggle-environment-var\n",
        "    - {inputValue: kaggle_environment_var}\n",
        "    - --kaggle-resource-type\n",
        "    - {inputValue: kaggle_resource_type}\n",
        "    - --kaggle-resource-name\n",
        "    - {inputValue: kaggle_resource_name}\n",
        "    - --output-datasource-directory\n",
        "    - {outputPath: output_datasource_directory}"
      ],
      "metadata": {
        "id": "SBI4M_Blh8l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Software testing"
      ],
      "metadata": {
        "id": "9uiUWKu4LWUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/my_local_dir_for_test\n",
        "!chmod 544 run_tests.sh\n",
        "!cp kaggle_download.py /tmp\n",
        "!cp test_validation.py /tmp\n",
        "!./run_tests.sh"
      ],
      "metadata": {
        "id": "5i9h_W3T4H87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install kfp==1.8.12"
      ],
      "metadata": {
        "id": "jhPf6ZSOnv0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First validate the component.yaml file in http://www.yamllint.com/. Once component.yaml file is corrected, execute the below cell to finally check"
      ],
      "metadata": {
        "id": "tQalOIRBDzSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from kubernetes import client as k8s_client\n",
        "\n",
        "kaggle_download_op_out_to_artifact = kfp.components.load_component_from_file('component_output_as_artifact.yaml')\n",
        "\n",
        "@kfp.dsl.pipeline(name=\"testpipeline1\")\n",
        "def my_sample_pipeline():\n",
        "    op = kaggle_download_op_out_to_artifact(\n",
        "                                kaggle_environment_var = '{\"KAGGLE_CONFIG_DIR\":\"/mnt\"}', \n",
        "                                kaggle_resource_type = 'datasets',\n",
        "                                kaggle_resource_name = 'anushonkar/network-anamoly-detection',\n",
        "                                )\n",
        "    op.add_volume(k8s_client.V1Volume(\n",
        "        name=\"kaggle_json_volume\",\n",
        "        secret=k8s_client.V1SecretVolumeSource(secret_name=\"kaggle_json-secrets\")) #kaggle_json-secrets should contain kaggle.json\n",
        "    )\n",
        "    op.add_volume_mount(k8s_client.V1VolumeMount(\n",
        "                                          mount_path='/mnt',\n",
        "                                          name='kaggle_json_volume')\n",
        "    )\n",
        "    directory_where_files_downloaded = op.outputs['output_datasource_directory']\n",
        "\n",
        "\n",
        "kfp.compiler.Compiler().compile(pipeline_func=my_sample_pipeline,package_path='my_sample_pipeline.yaml')\n",
        "kfp.v2.compiler.Compiler().compile(pipeline_func=my_sample_pipeline,package_path='my_sample_pipeline_v2.json')\n"
      ],
      "metadata": {
        "id": "54vGWKvUDu8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from kubernetes import client as k8s_client\n",
        "\n",
        "kaggle_download_op_out_to_artifact = kfp.components.load_component_from_file('component_output_as_artifact.yaml')\n",
        "\n",
        "@kfp.dsl.pipeline(name=\"testpipeline2\")\n",
        "def my_sample_pipeline():\n",
        "    op = kaggle_download_op_out_to_artifact(\n",
        "                                kaggle_environment_var = '', \n",
        "                                kaggle_resource_type = 'datasets',\n",
        "                                kaggle_resource_name = 'anushonkar/network-anamoly-detection',\n",
        "                                )\n",
        "    op.add_volume(k8s_client.V1Volume(\n",
        "        name=\"kaggle_json_volume\",\n",
        "        secret=k8s_client.V1SecretVolumeSource(secret_name=\"kaggle_json-secrets\")) #kaggle_json-secrets should contain KAGGLE_USERNAME and KAGGLE_KEY\n",
        "    )\n",
        "    envs = [\n",
        "        (\"KAGGLE_USERNAME\", \"KAGGLE_USERNAME\"),\n",
        "        (\"KAGGLE_KEY\", \"KAGGLE_KEY\")\n",
        "    ]\n",
        "    for env_name, key in envs:\n",
        "        op.add_env_variable(\n",
        "            k8s_client.V1EnvVar(\n",
        "                name=env_name,\n",
        "                value_from=k8s_client.V1EnvVarSource(secret_key_ref=k8s_client.V1SecretKeySelector(\n",
        "                    name=\"kaggle_json-secrets\",\n",
        "                    key=key\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    directory_where_files_downloaded = op.outputs['output_datasource_directory']\n",
        "\n",
        "\n",
        "kfp.compiler.Compiler().compile(pipeline_func=my_sample_pipeline,package_path='my_sample_pipeline2.yaml')\n",
        "kfp.v2.compiler.Compiler().compile(pipeline_func=my_sample_pipeline,package_path='my_sample_pipeline2_v2.json')\n"
      ],
      "metadata": {
        "id": "Y2g_G0c3EuSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Push the code to github"
      ],
      "metadata": {
        "id": "a62pERh-Lasc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before commiting code to github, install github client (gh) by following instruction mentioned in https://github.com/cli/cli/blob/trunk/docs/install_linux.md (Choose Debian, Ubuntu Linux way of installation) \n",
        "\n",
        "Use the colab's 'Terminal' icon present in left vertical pane to open linux terminal to type commands. Once 'gh' is installed, type **$gh auth login** (refer https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git) to follow onscreen prompts. For colab, use **Paste an authentication token** option. Personal tokens can be generated in https://github.com/settings/tokens\n",
        "\n",
        "You can use Shift+Ctrl+v shortcut to paste any string in colab console"
      ],
      "metadata": {
        "id": "2Nix3R2joPdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "G12wQfVT1ann"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -Rf kfpcomponent"
      ],
      "metadata": {
        "id": "JVc_RmC9MrgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShaswataJash/kfpcomponent.git"
      ],
      "metadata": {
        "id": "S5LCKX0SXUxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow directory structure according to https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#organizing-the-component-files"
      ],
      "metadata": {
        "id": "AIwZrmfnCXPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir kfpcomponent/KaggleDatasetFetcher\n",
        "!mkdir kfpcomponent/KaggleDatasetFetcher/src\n",
        "!mkdir kfpcomponent/KaggleDatasetFetcher/tests"
      ],
      "metadata": {
        "id": "PWT-29j1XYhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it will ensure file is coped in git repo only if file content is changed by checking checksum of file content\n",
        "!rsync -c kaggle_download.py kfpcomponent/KaggleDatasetFetcher/src\n",
        "!rsync -c component_output_as_artifact.yaml kfpcomponent/KaggleDatasetFetcher/component_output_as_artifact.yaml\n",
        "!rsync -c test_validation.py kfpcomponent/KaggleDatasetFetcher/tests\n",
        "!rsync -c Dockerfile kfpcomponent/KaggleDatasetFetcher/\n",
        "!rsync -c run_tests.sh kfpcomponent/KaggleDatasetFetcher/"
      ],
      "metadata": {
        "id": "n3b7gcGhXde6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd kfpcomponent"
      ],
      "metadata": {
        "id": "1RUDYMnmXlIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add -A"
      ],
      "metadata": {
        "id": "iAyaTjClX9S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "hW4fzKBsYCGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For git-user who has set their email visibility as private, git provides alternate email address to use in web-based Git operations, e.g., edits and merges. The alias email can be viewed in https://github.com/settings/emails"
      ],
      "metadata": {
        "id": "bmHCKK7CzXNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"29448766+ShaswataJash@users.noreply.github.com\""
      ],
      "metadata": {
        "id": "FBLk0UVR04lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -a -m \"corrected component.yaml\""
      ],
      "metadata": {
        "id": "VbaNqafyYHV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "id": "uLZfkrleaqQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "LwhyFmmz5bnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}