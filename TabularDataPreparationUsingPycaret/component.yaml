name: TabularDataPreparationUsingPycaret
description: |
    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)
    Refer data-preparations-options in command line arguments. 
    pycaret internally uses pandas dataframe to read and write csv file. You can utilize options exposed by panda's read_csv() and to_csv(). 
    Refer additional-options-csv-parsing and additional-options-csv-writing in command line arguments
    Input and output csv files can be stored in rclone compatible storage. Both mount and copy mode are supported. (refer: https://rclone.org/)
    rclone configurations have to be shared through environment variables (refer: https://rclone.org/docs/#environment-variables). 
    Create rclone read and write configuration file name as 'REMOTEREAD' and 'REMOTEWRITE'. Because the same are used within code.
    So convention for creating any environment variables related to rclone should start either with 'RCLONE_CONFIG_REMOTEREAD' or 'RCLONE_CONFIG_REMOTEWRITE'.
    There are options to bypass rclone separately for input and output datasource. Such bypassing will be useful for the scenario like vertexAI pipelines where
    an external system emulates cloud storage (like gcs, minio, s3) as locally mounted storage.

inputs:
- {name: bypass-rclone-for-input-data, type: Boolean, default: false, description: 'whether input csv file should be read like local file - rclone is completely bypassed'}
- {name: bypass-rclone-for-output-data, type: Boolean, default: false, description: 'whether output csv file should be written like local file - rclone is completely bypassed'}
- {name: rclone-environment-var, type: String, default: '{}', description: 'json formatted key-value pairs of strings which will be set as environment variables before executing rclone commands'}
- {name: input-datasource-directory-mountable, type: Boolean, default: false, description: 'whether input csv file is present in mountable remote location'}
- {name: input-datasource-file-name, type: String, description: 'name of the csv file including file extension and the directory/bucket path holding the specific file(if any)'}
- {name: additional-options-csv-parsing, type: String, default: '{}', description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'}
- {name: type-of-data-analysis-task, type: String, description: 'choice amongst classification, regression, clustering, anomaly_detection'}
- {name: target-variable-name, type: String, description: 'for classification and regression, specify the column name holding target variable'}
- {name: target-emptyindicator, type: String, default: '', description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'}
- {name: data-preparations-options, type: String, default: '{}', description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'}
- {name: additional-options-csv-writing, type: String, default: '{}', description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'}
- {name: output-datasource-directory-mountable, type: Boolean, default: false, description: 'whether output csv file will be written in mountable remote location'}

outputs:
- {name: output-datasource-file-name, type: String, description: 'filename of the prepared data including the directory/bucket path holding the specific file(if any)'}

implementation:
  container:
    image: hub.docker.com/shasjash/kfpcomponents/TabularDataPreparationUsingPycaret_devlatest
    # command is a list of strings (command-line arguments). 
    # The YAML language has two syntaxes for lists and you can use either of them. 
    # Here we use the "flow syntax" - comma-separated strings inside square brackets.
    command: [
      python3, 
      # Path of the program inside the container
      /tmp/data_preparation.py,
      --bypass-rclone-for-input-data,
      {inputValue: bypass-rclone-for-input-data},
      --bypass-rclone-for-output-data,
      {inputValue: bypass-rclone-for-output-data},
      --rclone-environment-var,
      {inputValue: rclone-environment-var},
      --input-datasource-directory-mountable,
      {inputValue: input-datasource-directory-mountable},
      --input-datasource-file-name, 
      {inputPath: input-datasource-file-name},
      --additional-options-csv-parsing, 
      {inputValue: additional-options-csv-parsing},
      --type-of-data-analysis-task, 
      {inputValue: type-of-data-analysis-task},
      --target-variable-name, 
      {inputValue: target-variable-name},
      --target-emptyindicator, 
      {inputValue: target-emptyindicator},
      --data-preparations-options, 
      {inputValue: data-preparations-options},
      --additional-options-csv-writing, 
      {inputValue: additional-options-csv-writing},
      --output-datasource-directory-mountable, 
      {inputValue: output-datasource-directory-mountable},
      --output-datasource-file-name, 
      {outputPath: output-datasource-file-name},
    ]