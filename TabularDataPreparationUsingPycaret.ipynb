{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TabularDataPreparationUsingPycaret.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNb/4VU6C65hZUYLUboIYE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/kfpcomponent/blob/main/TabularDataPreparationUsingPycaret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoSae8JMvVgq",
        "outputId": "36a4ec70-7468-433e-cde9-83402fdf1993"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 8a647a2648cf 5.4.188+ #1 SMP Sun Apr 24 10:03:06 PDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZTBdke9vme4",
        "outputId": "eda5aa5a-96df-4d26-ea93-a4a09dc51d3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.5 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4orJiv6orBy",
        "outputId": "16059360-0d41-49f8-ee2e-fbebef442b09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lus4TEd-8DbB",
        "outputId": "d3ed9d9b-0c5c-4c91-dfa9-ccb66fed466f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycaret==2.3.10\n",
            "  Downloading pycaret-2.3.10-py3-none-any.whl (320 kB)\n",
            "\u001b[K     |████████████████████████████████| 320 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (5.5.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (7.7.0)\n",
            "Requirement already satisfied: spacy<2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (2.2.4)\n",
            "Requirement already satisfied: scipy<=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.2.5)\n",
            "Collecting lightgbm>=2.3.1\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.6.0)\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cufflinks>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.17.3)\n",
            "Requirement already satisfied: numba<0.55 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.51.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.3.5)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.11.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.15.3)\n",
            "Collecting pyod\n",
            "  Downloading pyod-1.0.1.tar.gz (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 55.3 MB/s \n",
            "\u001b[?25hCollecting kmodes>=0.10.1\n",
            "  Downloading kmodes-0.12.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting imbalanced-learn==0.7.0\n",
            "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yellowbrick>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.4)\n",
            "Collecting mlxtend>=0.17.0\n",
            "  Downloading mlxtend-0.20.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 45.5 MB/s \n",
            "\u001b[?25hCollecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 56.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (5.5.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.5.0)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.8 MB 669 kB/s \n",
            "\u001b[?25hCollecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: pyyaml<6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.1.0)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 19.0 MB/s \n",
            "\u001b[?25hCollecting pandas-profiling>=2.8.0\n",
            "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.7.0->pycaret==2.3.10) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->pycaret==2.3.10) (3.1.0)\n",
            "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (0.3.0)\n",
            "Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->pycaret==2.3.10) (6.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (0.8.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (3.6.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (5.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.3.1->pycaret==2.3.10) (0.37.1)\n",
            "Collecting mlxtend>=0.17.0\n",
            "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 33.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pycaret==2.3.10) (4.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.11.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (3.8.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<0.55->pycaret==2.3.10) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pycaret==2.3.10) (2022.1)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 37.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (2.11.3)\n",
            "Collecting markupsafe~=2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 13.0 MB/s \n",
            "\u001b[?25hCollecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Collecting tangled-up-in-unicode==0.2.0\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting multimethod>=1.4\n",
            "  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
            "Collecting pyyaml<6.0.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 26.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (4.64.0)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (0.5.1)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting scipy<=1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.4.1->pycaret==2.3.10) (8.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->pycaret==2.3.10) (0.2.5)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2022.5.18.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.7)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (2.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (0.9.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (23.0.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.7.0)\n",
            "Collecting yellowbrick>=1.0.1\n",
            "  Downloading yellowbrick-1.3.post1-py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 41.5 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.13.3\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (1.3.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (0.4.2)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.1.4)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.4.36)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 846 kB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (3.17.3)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.3.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (21.3)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
            "Collecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret==2.3.10) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret==2.3.10) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow->pycaret==2.3.10) (1.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret==2.3.10) (1.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.0.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.5.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow->pycaret==2.3.10) (0.14.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (0.16.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (2.8.1)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 42.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 58.8 MB/s \n",
            "\u001b[?25hCollecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod->pycaret==2.3.10) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod->pycaret==2.3.10) (0.5.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: htmlmin, imagehash, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=7be46233243ee4ab1b4e6536543ca4d00ab27bd8e8e89d1462456e87015f1929\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=24b3611518635b304453fcc73d2390d10a25c195aceabf377781ed544cb94d90\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=501666aefc63debc2d73e3435108487fcddfa81a3c93718ee588a955143474a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135617 sha256=3e78de43ff3139fdc833595aee3adfec7833aa97d4ee202c7a9c152dc2f0400c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.0.1-py3-none-any.whl size=147473 sha256=e4287d4145681a58770fbe9e31ec1395654cb3214d9a640d60ea4e1cc0dae2d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/c4/29/67ad87835b209f72e4706369c683741b09490f2829d64ea768\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=ef17ecadf9b13357d31620d0df0722d3ee3fc820f01766e91a9a78037be9afe6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=672fe3cb734f14578d94461b074ae6101d4a5c8625fff7b0fd80bc88b283f3f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built htmlmin imagehash databricks-cli pyLDAvis pyod umap-learn pynndescent\n",
            "Installing collected packages: markupsafe, numpy, tangled-up-in-unicode, smmap, scipy, multimethod, websocket-client, visions, scikit-learn, requests, pyjwt, Mako, imagehash, gitdb, querystring-parser, pyyaml, pynndescent, pydantic, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, yellowbrick, umap-learn, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: yellowbrick\n",
            "    Found existing installation: yellowbrick 1.4\n",
            "    Uninstalling yellowbrick-1.4:\n",
            "      Successfully uninstalled yellowbrick-1.4\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "  Attempting uninstall: mlxtend\n",
            "    Found existing installation: mlxtend 0.14.0\n",
            "    Uninstalling mlxtend-0.14.0:\n",
            "      Successfully uninstalled mlxtend-0.14.0\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.8.1\n",
            "    Uninstalling imbalanced-learn-0.8.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Boruta-0.3 Mako-1.2.0 alembic-1.8.0 databricks-cli-0.16.6 docker-5.0.3 funcy-1.17 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 kmodes-0.12.1 lightgbm-3.3.2 markupsafe-2.1.1 mlflow-1.26.1 mlxtend-0.19.0 multimethod-1.8 numpy-1.19.5 pandas-profiling-3.2.0 phik-0.12.2 prometheus-flask-exporter-0.20.2 pyLDAvis-3.2.2 pycaret-2.3.10 pydantic-1.9.1 pyjwt-2.4.0 pynndescent-0.5.7 pyod-1.0.1 pyyaml-5.4.1 querystring-parser-1.2.4 requests-2.27.1 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 tangled-up-in-unicode-0.2.0 umap-learn-0.5.3 visions-0.7.4 websocket-client-1.3.2 yellowbrick-1.3.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pycaret==2.3.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ref: https://github.com/pycaret/pycaret/issues/2490\n",
        "!pip install Jinja2==3.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKAa1jnEnDL3",
        "outputId": "ad32834a-1740-4bd4-d72c-74e38d14dd74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Jinja2==3.1.2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2==3.1.2) (2.1.1)\n",
            "Installing collected packages: Jinja2\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Jinja2-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ca-certificates fuse tzdata && \\\n",
        "  echo \"user_allow_other\" >> /etc/fuse.conf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p89zTHfYqwc",
        "outputId": "cf2373eb-4aca-403c-db80-31ce2c0f3345"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fuse is already the newest version (2.9.7-1ubuntu1).\n",
            "ca-certificates is already the newest version (20210119~18.04.2).\n",
            "tzdata is already the newest version (2022a-0ubuntu0.18.04).\n",
            "tzdata set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://rclone.org/install.sh | sudo bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I61CdpC5HywN",
        "outputId": "1529642a-bfab-4ef7-ff95-86900ee43f9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4497  100  4497    0     0   4763      0 --:--:-- --:--:-- --:--:--  4763\n",
            "Archive:  rclone-current-linux-amd64.zip\n",
            "   creating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/\n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/rclone  [binary]\n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/README.html  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/README.txt  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/git-log.txt  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/rclone.1  [text]  \n",
            "Purging old database entries in /usr/share/man...\n",
            "Processing manual pages under /usr/share/man...\n",
            "Purging old database entries in /usr/share/man/da...\n",
            "Processing manual pages under /usr/share/man/da...\n",
            "Purging old database entries in /usr/share/man/cs...\n",
            "Processing manual pages under /usr/share/man/cs...\n",
            "Purging old database entries in /usr/share/man/zh_TW...\n",
            "Processing manual pages under /usr/share/man/zh_TW...\n",
            "Purging old database entries in /usr/share/man/tr...\n",
            "Processing manual pages under /usr/share/man/tr...\n",
            "Purging old database entries in /usr/share/man/ja...\n",
            "Processing manual pages under /usr/share/man/ja...\n",
            "Purging old database entries in /usr/share/man/ru...\n",
            "Processing manual pages under /usr/share/man/ru...\n",
            "Purging old database entries in /usr/share/man/nl...\n",
            "Processing manual pages under /usr/share/man/nl...\n",
            "Purging old database entries in /usr/share/man/ko...\n",
            "Processing manual pages under /usr/share/man/ko...\n",
            "Purging old database entries in /usr/share/man/es...\n",
            "Processing manual pages under /usr/share/man/es...\n",
            "Purging old database entries in /usr/share/man/sv...\n",
            "Processing manual pages under /usr/share/man/sv...\n",
            "Purging old database entries in /usr/share/man/pt...\n",
            "Processing manual pages under /usr/share/man/pt...\n",
            "Purging old database entries in /usr/share/man/de...\n",
            "Processing manual pages under /usr/share/man/de...\n",
            "Purging old database entries in /usr/share/man/hu...\n",
            "Processing manual pages under /usr/share/man/hu...\n",
            "Purging old database entries in /usr/share/man/zh_CN...\n",
            "Processing manual pages under /usr/share/man/zh_CN...\n",
            "Purging old database entries in /usr/share/man/fr...\n",
            "Processing manual pages under /usr/share/man/fr...\n",
            "Purging old database entries in /usr/share/man/id...\n",
            "Processing manual pages under /usr/share/man/id...\n",
            "Purging old database entries in /usr/share/man/fi...\n",
            "Processing manual pages under /usr/share/man/fi...\n",
            "Purging old database entries in /usr/share/man/pl...\n",
            "Processing manual pages under /usr/share/man/pl...\n",
            "Purging old database entries in /usr/share/man/it...\n",
            "Processing manual pages under /usr/share/man/it...\n",
            "Purging old database entries in /usr/share/man/pt_BR...\n",
            "Processing manual pages under /usr/share/man/pt_BR...\n",
            "Purging old database entries in /usr/share/man/sr...\n",
            "Processing manual pages under /usr/share/man/sr...\n",
            "Processing manual pages under /usr/local/man...\n",
            "Updating index cache for path `/usr/local/man/man1'. Wait...done.\n",
            "Checking for stray cats under /usr/local/man...\n",
            "Checking for stray cats under /var/cache/man/oldlocal...\n",
            "1 man subdirectory contained newer manual pages.\n",
            "3 manual pages were added.\n",
            "0 stray cats were added.\n",
            "19 old database entries were purged.\n",
            "\n",
            "rclone v1.58.1 has successfully installed.\n",
            "Now run \"rclone config\" for setup. Check https://rclone.org/docs/ for more details.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATE THE rclone read and write configuration file name as 'REMOTEREAD' and 'REMOTEWRITE'. Because the same are used within code.\n",
        "## So convention for creating any environment variables related to rclone should start either with 'RCLONE_CONFIG_REMOTEREAD' or 'RCLONE_CONFIG_REMOTEWRITE'\n",
        "\n",
        "import os\n",
        "os.environ[\"RCLONE_CONFIG_REMOTEREAD_TYPE\"] = 'http'\n",
        "os.environ[\"RCLONE_CONFIG_REMOTEREAD_URL\"] = 'https://raw.githubusercontent.com/pycaret/datasets/main/data/common/'\n",
        "\n",
        "#https://registry.opendata.aws/humor-detection/\n",
        "#os.environ[\"RCLONE_CONFIG_REMOTEREAD_TYPE\"] = 's3'\n",
        "#os.environ[\"RCLONE_CONFIG_REMOTEREAD_PROVIDER\"] = 'AWS'\n",
        "#os.environ[\"RCLONE_CONFIG_REMOTEREAD_ACCESS_KEY_ID\"] = ''\n",
        "#os.environ[\"RCLONE_CONFIG_REMOTEREAD_SECRET_ACCESS_KEY\"] = ''\n",
        "#os.environ[\"RCLONE_CONFIG_REMOTEREAD_REGION\"] = 'us-west-2' \n",
        "#os.environ[\"RCLONE_CONFIG_REMOTEREAD_ENDPOINT\"] = ''\n",
        "\n",
        "os.environ[\"RCLONE_CONFIG_REMOTEWRITE_TYPE\"] = 'local'\n",
        "os.environ[\"RCLONE_CONFIG_REMOTEWRITE_NOUNC\"] = 'true'\n",
        "\n",
        "#refer https://rclone.org/faq/#:~:text=Can%20I%20use%20rclone%20with,for%20services%20reached%20over%20https%20).\n",
        "#os.environ[\"http_proxy\"]=''\n",
        "#os.environ[\"https_proxy\"]=''\n",
        "#os.environ[\"HTTP_PROXY\"]=''\n",
        "#os.environ[\"HTTPS_PROXY\"]=''"
      ],
      "metadata": {
        "id": "lm_fuiVwg2S5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_datasource_directory_mountable = False\n",
        "#input_datasource_directory_to_be_mounted = 'humor-detection-pds/'\n",
        "#input_datasource_file_name = 'Non-humours-biased.csv'"
      ],
      "metadata": {
        "id": "1AIHBCxn30vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_preparation.py\n",
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "\n",
        "# Defining and parsing the command-line arguments\n",
        "parser = argparse.ArgumentParser(description='kubeflow pipeline component to read csv file and prepare the data')\n",
        "parser.add_argument('--input-datasource-directory-mountable', default=False, action=\"store_true\", help='whether input csv file is present in mountable remote location')\n",
        "parser.add_argument('--input-datasource-directory-to-be-mounted', type=str, help='if input-datasource-directory-mountable=True, name of the mountable directory (e.g. bucket name for s3)')\n",
        "parser.add_argument('--input-datasource-file-name', type=str, help='name of the csv file including file extension (if any)')\n",
        "parser.add_argument('--additional-options-csv-parsing', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pandas.read_csv()')\n",
        "parser.add_argument('--type-of-data-analysis-task', choices=['classification', 'regression', 'clustering', 'anomaly_detection'])\n",
        "parser.add_argument('--target-variable-name', type=str, help='for classification and regression, specify the column name holding target variable')\n",
        "parser.add_argument('--target-emptyindicator', type=str, default='', help='if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc')\n",
        "parser.add_argument('--data-preparations-options', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pycaret setup() function')\n",
        "parser.add_argument('--additional-options-csv-writing', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pandas.to_csv()')\n",
        "parser.add_argument('--output-datasource-directory-mountable', default=False, action=\"store_true\", help='whether output csv file will be written in mountable remote location')\n",
        "parser.add_argument('--output-datasource-containing-directory', type=str, help='name of the directory (e.g. bucket name for s3) where csv file will be written')\n",
        "parser.add_argument('--output-datasource-file-name', type=str, help='filename of the prepared data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "import tempfile\n",
        "local_datastore_read_dir = tempfile.mkdtemp(prefix=\"my_local_read-\")\n",
        "print('local_datastore_read_dir:',local_datastore_read_dir)\n",
        "\n",
        "local_datastore_write_dir = tempfile.mkdtemp(prefix=\"my_local_write-\")\n",
        "print('local_datastore_write_dir:',local_datastore_write_dir)\n",
        "\n",
        "#input file handling\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "if args.input_datasource_directory_mountable:\n",
        "    input_data_read_cmd = \"rclone -v mount remoteread:\" + args.input_datasource_directory_to_be_mounted + ' ' + local_datastore_read_dir + ' --daemon'\n",
        "else:\n",
        "    input_data_read_cmd = \"rclone -v copy remoteread:\" + args.input_datasource_file_name + ' ' + local_datastore_read_dir\n",
        "input_data_read_call = subprocess.run(input_data_read_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(input_data_read_call.stdout)\n",
        "if input_data_read_call.returncode != 0:\n",
        "    print(\"Error in rclone, errorcode=\", input_data_read_call.returncode)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as rclone returned error in context of reading\")\n",
        "\n",
        "#output file handling\n",
        "if args.output_datasource_directory_mountable:\n",
        "    output_data_write_cmd = \"rclone -v mount remotewrite:\" + args.output_datasource_containing_directory + ' ' + local_datastore_write_dir + ' --daemon'\n",
        "    output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(output_data_write_call.stdout)\n",
        "    if output_data_write_call.returncode != 0:\n",
        "        print(\"Error in rclone, errorcode=\", output_data_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as rclone returned error in context of mounted writing\")\n",
        "\n",
        "#handling input csv file reading\n",
        "import pandas\n",
        "import json\n",
        "\n",
        "try:\n",
        "    parse_config = json.loads(args.additional_options_csv_parsing)\n",
        "    print('parse_config = (', type(parse_config), ')', parse_config)\n",
        "    parse_config['filepath_or_buffer'] = os.path.join(local_datastore_read_dir,args.input_datasource_file_name)\n",
        "    my_data = pandas.read_csv(**parse_config)\n",
        "    print(my_data)\n",
        "    \n",
        "except BaseException as err:\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while parsing input csv file\")\n",
        "\n",
        "#handling data preprocessing\n",
        "import pycaret\n",
        "try:\n",
        "    if os.path.exists(\"logs.log\"):\n",
        "        os.remove(\"logs.log\") #removing any content from log which pycaret will internally use for its own logging\n",
        "    print('pycaret version = ', pycaret.utils.version())\n",
        "    setup_config = json.loads(args.data_preparations_options)\n",
        "    print('setup_config = (', type(setup_config), ')', setup_config)\n",
        "    if args.type_of_data_analysis_task == 'classification':\n",
        "        import pycaret.classification\n",
        "        setup_fn = pycaret.classification.setup\n",
        "        get_config_fn = pycaret.classification.get_config\n",
        "        setup_config['target'] = args.target_variable_name\n",
        "        \n",
        "    elif args.type_of_data_analysis_task == 'regression':\n",
        "        import pycaret.regression\n",
        "        setup_fn = pycaret.regression.setup\n",
        "        get_config_fn = pycaret.regression.get_config\n",
        "        setup_config['target'] = args.target_variable_name\n",
        "\n",
        "    elif args.type_of_data_analysis_task == 'clustering':\n",
        "        import pycaret.clustering\n",
        "        setup_fn = pycaret.clustering.setup\n",
        "        get_config_fn = pycaret.clustering.get_config\n",
        "\n",
        "    elif args.type_of_data_analysis_task == 'anomaly':\n",
        "        import pycaret.anomaly\n",
        "        setup_fn = pycaret.anomaly.setup\n",
        "        get_config_fn = pycaret.anomaly.get_config\n",
        "        \n",
        "    #as part of pycaret's data cleaning the rows with target column = nan are not being cleaned up. Thus, cleaning those rows explicitely\n",
        "    if len(args.target_emptyindicator) > 0:\n",
        "        #ref: https://stackoverflow.com/questions/49291740/delete-rows-if-there-are-null-values-in-a-specific-column-in-pandas-dataframe\n",
        "        import numpy as np\n",
        "        my_data[args.target_variable_name] = my_data[args.target_variable_name].replace(args.target_emptyindicator, np.nan)\n",
        "        my_data = my_data.dropna(axis=0, subset=[args.target_variable_name])\n",
        "\n",
        "    setup_config['data'] = my_data\n",
        "    setup_config['log_experiment'] = False\n",
        "    setup_config['data_split_shuffle'] = False\n",
        "    setup_config['html'] = False\n",
        "    setup_config['silent'] = True\n",
        "    setup_fn(**setup_config)\n",
        "    #ref: https://www.kdnuggets.com/2020/11/5-things-doing-wrong-pycaret.html\n",
        "    X_transformed = get_config_fn('X')\n",
        "    my_transformed_data = X_transformed\n",
        "    if args.type_of_data_analysis_task == 'classification' or args.type_of_data_analysis_task == 'regression':\n",
        "        y_transformed = get_config_fn('y')\n",
        "        my_transformed_data = X_transformed.merge(y_transformed,left_index=True, right_index=True)\n",
        "    print(my_transformed_data)\n",
        "\n",
        "    pycaret.utils.get_system_logs() #this will print the pycaret's own log into console\n",
        "    \n",
        "except BaseException as err:\n",
        "    pycaret.utils.get_system_logs()\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while transforming input dataframe\")\n",
        "\n",
        "#handling output csv file writing\n",
        "try:\n",
        "    to_csv_config = json.loads(args.additional_options_csv_writing)\n",
        "    print('to_csv_config = (', type(to_csv_config), ')', to_csv_config)\n",
        "    to_csv_config['path_or_buf'] = os.path.join(local_datastore_write_dir,args.output_datasource_file_name)\n",
        "    my_transformed_data.to_csv(**to_csv_config)\n",
        "except BaseException as err:\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while trying to write prepared data\")\n",
        "\n",
        "if not args.output_datasource_directory_mountable:\n",
        "    output_data_write_cmd = \"rclone -v copy \" + os.path.join(local_datastore_write_dir,args.output_datasource_file_name) + \" remotewrite:\" + args.output_datasource_containing_directory + '/'\n",
        "    output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(output_data_write_call.stdout)\n",
        "    if output_data_write_call.returncode != 0:\n",
        "        print(\"Error in rclone, errorcode=\", output_data_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as rclone returned error in context of writing final csv file (copy mode)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU9Vh7P4P4_h",
        "outputId": "c9939fac-bcda-476e-fcae-80676c8ef80d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_preparation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.7\n",
        "RUN python3 -m pip install pycaret==2.3.10\n",
        "RUN python3 -m pip install Jinja2==3.1.2\n",
        "RUN apt update\n",
        "RUN apt-get -y install ca-certificates fuse tzdata && echo \"user_allow_other\" >> /etc/fuse.conf\n",
        "RUN curl https://rclone.org/install.sh | bash\n",
        "\n",
        "COPY src/data_preparation.py /tmp\n",
        "COPY tests/test_validation.py /tmp\n",
        "COPY run_tests.sh /tmp\n",
        "RUN chmod 544 /tmp/run_tests.sh"
      ],
      "metadata": {
        "id": "hk00hb780Qo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dfdc92-8889-4b4a-d170-17addffbba88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_tests.sh\n",
        "#!/bin/bash\n",
        "\n",
        "export RCLONE_CONFIG_REMOTEREAD_TYPE='http'\n",
        "export RCLONE_CONFIG_REMOTEREAD_URL='https://raw.githubusercontent.com/pycaret/datasets/main/data/common/'\n",
        "\n",
        "export RCLONE_CONFIG_REMOTEWRITE_TYPE='local'\n",
        "export RCLONE_CONFIG_REMOTEWRITE_NOUNC='true'\n",
        "\n",
        "mkdir /tmp/my_local_dir_for_test\n",
        "python /tmp/data_preparation.py --input-datasource-file-name 'CTG.csv' --additional-options-csv-parsing '{\"sep\":\",\" , \"header\":0}' \\\n",
        "    --type-of-data-analysis-task 'classification' --target-variable-name 'NSP' \\\n",
        "    --data-preparations-options '{\"ignore_low_variance\":true, \"remove_outliers\":true, \"remove_multicollinearity\":true, \"multicollinearity_threshold\":0.7}' \\\n",
        "    --output-datasource-directory-mountable --output-datasource-containing-directory '/tmp/my_local_dir_for_test' --output-datasource-file-name 'CTG_data-prep.csv'\n",
        "\n",
        "python /tmp/test_validation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzJg29vTVGcf",
        "outputId": "2b9b8dfd-9621-4a0f-f405-b799c96911e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_tests.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_validation.py\n",
        "#!/usr/bin/env python3\n",
        "import pandas\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/CTG_data-prep.csv')\n",
        "assert len(df.index) == 2126 #original data had 2129 rows, amongst that 3 rows have no target\n",
        "assert df.isnull().sum().sum() == 0 #pycaret will remove all missing values\n",
        "print ('test-validation done successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7maMc-RVqDV5",
        "outputId": "750c80e8-2c58-4ee4-a103-9f877a259b98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_validation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer: https://github.com/RealOrangeOne/docker-rclone-mount/blob/master/docker-compose.yml\n",
        "\n",
        "If we have to use mount feature of rclone, it needs to have fuse support in underneath linux kernel. For that we are adding SYS_ADMIN in capability. But note without using mount feature also, we can do testing. in that case, rclone will use only copy feature."
      ],
      "metadata": {
        "id": "YRSP8Vk8vyoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile docker-compose.test.yml\n",
        "services:\n",
        "  sut:\n",
        "    build: .\n",
        "    command: /tmp/run_tests.sh\n",
        "    cap_add:\n",
        "      - SYS_ADMIN\n",
        "    security_opt:\n",
        "      - apparmor:unconfined\n",
        "    devices:\n",
        "      - \"/dev/fuse:/dev/fuse\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVrXyQZhSIm_",
        "outputId": "6c0c5f80-8e9a-434a-c9b5-f9fc82d360e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing docker-compose.test.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py"
      ],
      "metadata": {
        "id": "W7uyWGT6Sccr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component.yaml\n",
        "name: TabularDataPreparationUsingPycaret\n",
        "description: |\n",
        "    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)\n",
        "    pycaret internally uses pandas dataframe to read and write csv file. \n",
        "    Input and processed csv file are stored in rclone compatible storage. Both mount and copy mode are supported. (refer: https://rclone.org/)\n",
        "    rclone configurations have to be shared through environment variables (refer: https://rclone.org/docs/#environment-variables). \n",
        "    Thus, before using this component in kubeflow pipeline, those environment variables have to be set from pipeline. \n",
        "\n",
        "inputs:\n",
        "- {name: input-datasource-directory-mountable, optional, description: 'whether input csv file is present in mountable remote location'}\n",
        "- {name: input-datasource-directory-to-be-mounted, type: String, description: 'if input-datasource-directory-mountable=True, name of the mountable directory (e.g. bucket name for s3)'}\n",
        "- {name: input-datasource-file-name, type: String, description: 'name of the csv file including file extension (if any)'}\n",
        "- {name: additional-options-csv-parsing, type: String, default= '{}', description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'}\n",
        "- {name: type-of-data-analysis-task, type: List, description: 'choice amongst classification, regression, clustering, anomaly_detection'}\n",
        "- {name: target-variable-name, type: String, description: 'for classification and regression, specify the column name holding target variable'}\n",
        "- {name: target-emptyindicator, type: String, default='', description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'}\n",
        "- {name: data-preparations-options, type: String, default= '{}', description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'}\n",
        "- {name: additional-options-csv-writing, type: String, default= '{}', description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'}\n",
        "- {name: output-datasource-directory-mountable, optional, description: 'whether output csv file will be written in mountable remote location'}\n",
        "- {name: output-datasource-containing-directory, type: String, description: 'name of the directory (e.g. bucket name for s3) where csv file will be written'}\n",
        "- {name: output-datasource-file-name, type: String, description: 'filename of the prepared data'}\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: hub.docker.com/shasjash/kfpcomponents/TabularDataPreparationUsingPycaret_devlatest\n",
        "    # command is a list of strings (command-line arguments). \n",
        "    # The YAML language has two syntaxes for lists and you can use either of them. \n",
        "    # Here we use the \"flow syntax\" - comma-separated strings inside square brackets.\n",
        "    command: [\n",
        "      python3, \n",
        "      # Path of the program inside the container\n",
        "      /tmp/data_preparation.py,\n",
        "      --input-datasource-directory-mountable,\n",
        "      {inputValue: input-datasource-directory-mountable},\n",
        "      --input-datasource-directory-to-be-mounted, \n",
        "      {inputValue: input-datasource-directory-to-be-mounted},\n",
        "      --input-datasource-file-name, \n",
        "      {inputValue: input-datasource-file-name},\n",
        "      --additional-options-csv-parsing, \n",
        "      {inputValue: additional-options-csv-parsing},\n",
        "      --type-of-data-analysis-task, \n",
        "      {inputValue: type-of-data-analysis-task},\n",
        "      --target-variable-name, \n",
        "      {inputValue: target-variable-name},\n",
        "      --target-emptyindicator, \n",
        "      {inputValue: target-emptyindicator},\n",
        "      --data-preparations-options, \n",
        "      {inputValue:data-preparations-options},\n",
        "      --additional-options-csv-writing, \n",
        "      {inputValue: additional-options-csv-writing},\n",
        "      --output-datasource-directory-mountable, \n",
        "      {inputValue: output-datasource-directory-mountable},\n",
        "      --output-datasource-containing-directory, \n",
        "      {inputValue: output-datasource-containing-directory},\n",
        "      --output-datasource-file-name, \n",
        "      {inputValue: output-datasource-file-name},\n",
        "    ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhXccnmvIo7u",
        "outputId": "9cc7abee-fde5-4bf2-a362-98465186fb31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing component.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us simulate testing what will be done by docker-hub infrastructure as part of auto-testing by using docker-compose.test.yml present in github source repository."
      ],
      "metadata": {
        "id": "i3TK5UnMvfMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/my_local_dir_for_test\n",
        "!chmod 544 run_tests.sh\n",
        "!cp data_preparation.py /tmp\n",
        "!cp test_validation.py /tmp\n",
        "!./run_tests.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9h_W3T4H87",
        "outputId": "ba54d78c-1a53-4f04-edf4-00959273ddd1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "local_datastore_read_dir: /tmp/my_local_read-85r2efvv\n",
            "local_datastore_write_dir: /tmp/my_local_write-_0amglb4\n",
            "2022/06/06 05:36:15 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "2022/06/06 05:36:15 INFO  : CTG.csv: Copied (new)\n",
            "2022/06/06 05:36:15 INFO  : \n",
            "Transferred:   \t  277.715 KiB / 277.715 KiB, 100%, 0 B/s, ETA -\n",
            "Transferred:            1 / 1, 100%\n",
            "Elapsed time:         0.1s\n",
            "\n",
            "\n",
            "2022/06/06 05:36:15 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "\n",
            "parse_config = ( <class 'dict'> ) {'sep': ',', 'header': 0}\n",
            "          FileName  ...  NSP\n",
            "0     Variab10.txt  ...  2.0\n",
            "1       Fmcs_1.txt  ...  1.0\n",
            "2       Fmcs_1.txt  ...  1.0\n",
            "3       Fmcs_1.txt  ...  1.0\n",
            "4       Fmcs_1.txt  ...  1.0\n",
            "...            ...  ...  ...\n",
            "2124  S8001045.dsp  ...  2.0\n",
            "2125  S8001045.dsp  ...  1.0\n",
            "2126           NaN  ...  NaN\n",
            "2127           NaN  ...  NaN\n",
            "2128           NaN  ...  NaN\n",
            "\n",
            "[2129 rows x 40 columns]\n",
            "pycaret version =  2.3.10\n",
            "setup_config = ( <class 'dict'> ) {'ignore_low_variance': True, 'remove_outliers': True, 'remove_multicollinearity': True, 'multicollinearity_threshold': 0.7}\n",
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n",
            "Setup Succesfully Completed!\n",
            "                               Description             Value\n",
            "0                               session_id              4522\n",
            "1                                   Target               NSP\n",
            "2                              Target Type        Multiclass\n",
            "3                            Label Encoded              None\n",
            "4                            Original Data        (2129, 40)\n",
            "5                           Missing Values              True\n",
            "6                         Numeric Features                25\n",
            "7                     Categorical Features                13\n",
            "8                         Ordinal Features             False\n",
            "9                High Cardinality Features             False\n",
            "10                 High Cardinality Method              None\n",
            "11                   Transformed Train Set        (1415, 86)\n",
            "12                    Transformed Test Set         (636, 86)\n",
            "13                      Shuffle Train-Test             False\n",
            "14                     Stratify Train-Test             False\n",
            "15                          Fold Generator   StratifiedKFold\n",
            "16                             Fold Number                10\n",
            "17                                CPU Jobs                -1\n",
            "18                                 Use GPU             False\n",
            "19                          Log Experiment             False\n",
            "20                         Experiment Name  clf-default-name\n",
            "21                                     USI              d02d\n",
            "22                         Imputation Type            simple\n",
            "23          Iterative Imputation Iteration              None\n",
            "24                         Numeric Imputer              mean\n",
            "25      Iterative Imputation Numeric Model              None\n",
            "26                     Categorical Imputer          constant\n",
            "27  Iterative Imputation Categorical Model              None\n",
            "28           Unknown Categoricals Handling    least_frequent\n",
            "29                               Normalize             False\n",
            "30                        Normalize Method              None\n",
            "31                          Transformation             False\n",
            "32                   Transformation Method              None\n",
            "33                                     PCA             False\n",
            "34                              PCA Method              None\n",
            "35                          PCA Components              None\n",
            "36                     Ignore Low Variance              True\n",
            "37                     Combine Rare Levels             False\n",
            "38                    Rare Level Threshold              None\n",
            "39                         Numeric Binning             False\n",
            "40                         Remove Outliers              True\n",
            "41                      Outliers Threshold              0.05\n",
            "42                Remove Multicollinearity              True\n",
            "43             Multicollinearity Threshold               0.7\n",
            "44             Remove Perfect Collinearity              True\n",
            "45                              Clustering             False\n",
            "46                    Clustering Iteration              None\n",
            "47                     Polynomial Features             False\n",
            "48                       Polynomial Degree              None\n",
            "49                    Trignometry Features             False\n",
            "50                    Polynomial Threshold              None\n",
            "51                          Group Features             False\n",
            "52                       Feature Selection             False\n",
            "53                Feature Selection Method           classic\n",
            "54            Features Selection Threshold              None\n",
            "55                     Feature Interaction             False\n",
            "56                           Feature Ratio             False\n",
            "57                   Interaction Threshold              None\n",
            "58                           Fix Imbalance             False\n",
            "59                    Fix Imbalance Method             SMOTE\n",
            "           e  ...  NSP\n",
            "0      357.0  ...  2.0\n",
            "1      632.0  ...  1.0\n",
            "2      779.0  ...  1.0\n",
            "3     1192.0  ...  1.0\n",
            "4     1147.0  ...  1.0\n",
            "...      ...  ...  ...\n",
            "2121  2867.0  ...  2.0\n",
            "2122  2867.0  ...  2.0\n",
            "2123  2596.0  ...  2.0\n",
            "2124  3049.0  ...  2.0\n",
            "2125  3415.0  ...  1.0\n",
            "\n",
            "[2126 rows x 87 columns]\n",
            "['2022-06-06 05', '36', '19,040', 'INFO', 'PyCaret Supervised Module']\n",
            "['2022-06-06 05', '36', '19,040', 'INFO', 'ML Usecase', 'classification']\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'version 2.3.10']\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'Initializing setup()']\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', \"setup(target=NSP, ml_usecase=classification, available_plots={'parameter'\", \"'Hyperparameters', 'auc'\", \"'AUC', 'confusion_matrix'\", \"'Confusion Matrix', 'threshold'\", \"'Threshold', 'pr'\", \"'Precision Recall', 'error'\", \"'Prediction Error', 'class_report'\", \"'Class Report', 'rfe'\", \"'Feature Selection', 'learning'\", \"'Learning Curve', 'manifold'\", \"'Manifold Learning', 'calibration'\", \"'Calibration Curve', 'vc'\", \"'Validation Curve', 'dimension'\", \"'Dimensions', 'feature'\", \"'Feature Importance', 'feature_all'\", \"'Feature Importance (All)', 'boundary'\", \"'Decision Boundary', 'lift'\", \"'Lift Chart', 'gain'\", \"'Gain Chart', 'tree'\", \"'Decision Tree', 'ks'\", \"'KS Statistic Plot'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=constant, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=None, numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=None, normalize=False, normalize_method=zscore, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=True, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=True, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=True, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=stratifiedkfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=False, session_id=None, log_experiment=False, experiment_name=None, experiment_custom_tags=None, log_plots=False, log_profile=False, log_data=False, silent=True, verbose=True, profile=False, profile_kwargs=None, display=None)\"]\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'Checking environment']\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'python_version', '3.7.13']\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'python_build', \"('default', 'Apr 24 2022 01\", '04', \"09')\"]\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'machine', 'x86_64']\n",
            "['2022-06-06 05', '36', '19,041', 'INFO', 'platform', 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'Memory', 'svmem(total=13617737728, available=11965612032, percent=12.1, used=1432809472, free=9871081472, active=1946861568, inactive=1500782592, buffers=135036928, cached=2178809856, shared=1236992, slab=221650944)']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'Physical Core', '1']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'Logical Core', '2']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'Checking libraries']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'pd==1.3.5']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'numpy==1.19.5']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'sklearn==0.23.2']\n",
            "['2022-06-06 05', '36', '19,042', 'INFO', 'lightgbm==3.3.2']\n",
            "['2022-06-06 05', '36', '19,042', 'WARNING', 'catboost not found']\n",
            "['2022-06-06 05', '36', '19,052', 'INFO', 'xgboost==0.90']\n",
            "['2022-06-06 05', '36', '19,052', 'INFO', 'mlflow==1.26.1']\n",
            "['2022-06-06 05', '36', '19,052', 'INFO', 'Checking Exceptions']\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'Declaring global variables']\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'USI', 'd02d']\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'pycaret_globals', \"{'experiment__', 'gpu_param', 'imputation_classifier', '_ml_usecase', 'create_model_container', 'pycaret_globals', 'fold_generator', '_gpu_n_jobs_param', 'y_train', 'transform_target_param', 'log_plots_param', '_all_models', 'iterative_imputation_iters_param', 'html_param', 'USI', 'seed', '_available_plots', 'fold_shuffle_param', '_all_metrics', 'fold_groups_param', 'y_test', 'y', 'fix_imbalance_param', 'display_container', 'X', 'n_jobs_param', 'logging_param', '_all_models_internal', 'exp_name_log', 'fold_param', 'dashboard_logger', 'X_train', 'master_model_container', '_internal_pipeline', 'imputation_regressor', 'target_param', 'fold_groups_param_full', 'transform_target_method_param', 'X_test', 'data_before_preprocess', 'prep_pipe', 'fix_imbalance_method_param', 'stratify_param'}\"]\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'Importing libraries']\n",
            "['2022-06-06 05', '36', '19,053', 'INFO', 'Copying data for preprocessing']\n",
            "['2022-06-06 05', '36', '19,054', 'INFO', 'Declaring preprocessing parameters']\n",
            "['2022-06-06 05', '36', '19,058', 'INFO', 'Creating preprocessing pipeline']\n",
            "['2022-06-06 05', '36', '19,079', 'INFO', 'Preprocessing pipeline created successfully']\n",
            "['2022-06-06 05', '36', '19,080', 'ERROR', '(Process Exit)', \"setup has been interupted with user command 'quit'. setup must rerun.\"]\n",
            "['2022-06-06 05', '36', '19,080', 'INFO', 'Creating global containers']\n",
            "['2022-06-06 05', '36', '19,081', 'INFO', 'Internal pipeline', \"Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\"]\n",
            "['2022-06-06 05', '38', '32,685', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-06 05', '38', '32,686', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-06 05', '38', '32,768', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-06 05', '38', '32,768', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-06 05', '38', '32,769', 'INFO', 'Creating grid variables']\n",
            "['2022-06-06 05', '38', '32,830', 'INFO', 'create_model_container', '0']\n",
            "['2022-06-06 05', '38', '32,830', 'INFO', 'master_model_container', '0']\n",
            "['2022-06-06 05', '38', '32,830', 'INFO', 'display_container', '1']\n",
            "['2022-06-06 05', '38', '32,837', 'INFO', 'Pipeline(memory=None,']\n",
            "[\"steps=[('dtypes',\"]\n",
            "['DataTypes_Auto_infer(categorical_features=[],']\n",
            "['display_types=False, features_todrop=[],']\n",
            "['id_columns=[],']\n",
            "[\"ml_usecase='classification',\"]\n",
            "[\"numerical_features=[], target='NSP',\"]\n",
            "['time_features=[])),']\n",
            "[\"('imputer',\"]\n",
            "[\"Simple_Imputer(categorical_strategy='not_available',\"]\n",
            "['fill_value_categorical=None,']\n",
            "['fill_value_numerical=None,']\n",
            "['numeric_strateg...']\n",
            "[\"('dummy', Dummify(target='NSP')),\"]\n",
            "[\"('fix_perfect', Remove_100(target='NSP')),\"]\n",
            "[\"('clean_names', Clean_Colum_Names()),\"]\n",
            "[\"('feature_select', 'passthrough'),\"]\n",
            "[\"('fix_multi',\"]\n",
            "['Fix_multicollinearity(correlation_with_target_preference=None,']\n",
            "['correlation_with_target_threshold=0.0,']\n",
            "[\"target_variable='NSP', threshold=0.7)),\"]\n",
            "[\"('dfs', 'passthrough'), ('pca', 'passthrough')],\"]\n",
            "['verbose=False)']\n",
            "['2022-06-06 05', '38', '32,837', 'INFO', 'setup() succesfully completed......................................']\n",
            "['2022-06-06 05', '38', '32,947', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-06 05', '38', '32,948', 'INFO', 'get_config(variable=X)']\n",
            "['2022-06-06 05', '38', '33,065', 'INFO', 'Global variable', 'X returned as            e  ...  Date_is_month_start_0']\n",
            "['0      357.0  ...                    0.0']\n",
            "['1      632.0  ...                    1.0']\n",
            "['2      779.0  ...                    1.0']\n",
            "['3     1192.0  ...                    1.0']\n",
            "['4     1147.0  ...                    1.0']\n",
            "['...      ...  ...                    ...']\n",
            "['2121  2867.0  ...                    1.0']\n",
            "['2122  2867.0  ...                    1.0']\n",
            "['2123  2596.0  ...                    1.0']\n",
            "['2124  3049.0  ...                    1.0']\n",
            "['2125  3415.0  ...                    1.0']\n",
            "['[2126 rows x 86 columns]']\n",
            "['2022-06-06 05', '38', '33,065', 'INFO', 'get_config() succesfully completed......................................']\n",
            "['2022-06-06 05', '38', '33,066', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-06 05', '38', '33,066', 'INFO', 'get_config(variable=y)']\n",
            "['2022-06-06 05', '38', '33,067', 'INFO', 'Global variable', 'y returned as 0       2.0']\n",
            "['1       1.0']\n",
            "['2       1.0']\n",
            "['3       1.0']\n",
            "['4       1.0']\n",
            "['...']\n",
            "['2121    2.0']\n",
            "['2122    2.0']\n",
            "['2123    2.0']\n",
            "['2124    2.0']\n",
            "['2125    1.0']\n",
            "['Name', 'NSP, Length', '2126, dtype', 'float32']\n",
            "['2022-06-06 05', '38', '33,067', 'INFO', 'get_config() succesfully completed......................................']\n",
            "to_csv_config = ( <class 'dict'> ) {}\n",
            "test-validation done successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before commiting code to github, install github client (gh) by following instruction mentioned in https://github.com/cli/cli/blob/trunk/docs/install_linux.md (Choose Debian, Ubuntu Linux way of installation) \n",
        "\n",
        "Use the colab's 'Terminal' icon present in left vertical pane to open linux terminal to type commands. Once 'gh' is installed, type **$gh auth login** (refer https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git) to follow onscreen prompts. For colab, use **Paste an authentication token** option. Personal tokens can be generated in https://github.com/settings/tokens\n",
        "\n",
        "You can use Shift+Ctrl+v shortcut to paste any string in colab console"
      ],
      "metadata": {
        "id": "2Nix3R2joPdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G12wQfVT1ann",
        "outputId": "37dbff06-e36f-40fe-d28d-f897ac318221"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -Rf kfpcomponent"
      ],
      "metadata": {
        "id": "JVc_RmC9MrgG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShaswataJash/kfpcomponent.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5LCKX0SXUxs",
        "outputId": "6480865b-c292-4e3b-891f-de7ca198f717"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kfpcomponent'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 81 (delta 34), reused 65 (delta 26), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow directory structure according to https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#organizing-the-component-files"
      ],
      "metadata": {
        "id": "AIwZrmfnCXPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret\n",
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret/src\n",
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret/tests"
      ],
      "metadata": {
        "id": "PWT-29j1XYhi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it will ensure file is coped in git repo only if file content is changed by checking checksum of file content\n",
        "!rsync -c data_preparation.py kfpcomponent/TabularDataPreparationUsingPycaret/src\n",
        "!rsync -c component.yaml kfpcomponent/TabularDataPreparationUsingPycaret/component.yaml\n",
        "!rsync -c test_validation.py kfpcomponent/TabularDataPreparationUsingPycaret/tests\n",
        "!rsync -c Dockerfile kfpcomponent/TabularDataPreparationUsingPycaret/\n",
        "!rsync -c run_tests.sh kfpcomponent/TabularDataPreparationUsingPycaret/\n",
        "!rsync -c docker-compose.test.yml kfpcomponent/TabularDataPreparationUsingPycaret/"
      ],
      "metadata": {
        "id": "n3b7gcGhXde6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd kfpcomponent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RUDYMnmXlIW",
        "outputId": "2b6c5023-0f18-4416-96e5-f76bb6dc912e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kfpcomponent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add -A"
      ],
      "metadata": {
        "id": "iAyaTjClX9S0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW4fzKBsYCGO",
        "outputId": "23d1b641-8b7c-4dfc-a5ac-7fe4de75d1bd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mnew file:   TabularDataPreparationUsingPycaret/Dockerfile\u001b[m\n",
            "\t\u001b[32mnew file:   TabularDataPreparationUsingPycaret/component.yaml\u001b[m\n",
            "\t\u001b[32mnew file:   TabularDataPreparationUsingPycaret/docker-compose.test.yml\u001b[m\n",
            "\t\u001b[32mnew file:   TabularDataPreparationUsingPycaret/run_tests.sh\u001b[m\n",
            "\t\u001b[32mnew file:   TabularDataPreparationUsingPycaret/src/data_preparation.py\u001b[m\n",
            "\t\u001b[32mnew file:   TabularDataPreparationUsingPycaret/tests/test_validation.py\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For git-user who has set their email visibility as private, git provides alternate email address to use in web-based Git operations, e.g., edits and merges. The alias email can be viewed in https://github.com/settings/emails"
      ],
      "metadata": {
        "id": "bmHCKK7CzXNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"29448766+ShaswataJash@users.noreply.github.com\""
      ],
      "metadata": {
        "id": "FBLk0UVR04lj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -a -m \"first commit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbaNqafyYHV7",
        "outputId": "5f85af1b-6f8a-40cc-b24c-af00f9ec1ede"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main ed99201] first commit\n",
            " 6 files changed, 244 insertions(+)\n",
            " create mode 100644 TabularDataPreparationUsingPycaret/Dockerfile\n",
            " create mode 100644 TabularDataPreparationUsingPycaret/component.yaml\n",
            " create mode 100644 TabularDataPreparationUsingPycaret/docker-compose.test.yml\n",
            " create mode 100755 TabularDataPreparationUsingPycaret/run_tests.sh\n",
            " create mode 100644 TabularDataPreparationUsingPycaret/src/data_preparation.py\n",
            " create mode 100644 TabularDataPreparationUsingPycaret/tests/test_validation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLZfkrleaqQ3",
        "outputId": "3900613e-2c69-4bca-8502-7ffcd4a2b2b5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 11, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  11% (1/9)   \rCompressing objects:  22% (2/9)   \rCompressing objects:  33% (3/9)   \rCompressing objects:  44% (4/9)   \rCompressing objects:  55% (5/9)   \rCompressing objects:  66% (6/9)   \rCompressing objects:  77% (7/9)   \rCompressing objects:  88% (8/9)   \rCompressing objects: 100% (9/9)   \rCompressing objects: 100% (9/9), done.\n",
            "Writing objects:   9% (1/11)   \rWriting objects:  18% (2/11)   \rWriting objects:  27% (3/11)   \rWriting objects:  36% (4/11)   \rWriting objects:  45% (5/11)   \rWriting objects:  54% (6/11)   \rWriting objects:  63% (7/11)   \rWriting objects:  72% (8/11)   \rWriting objects:  81% (9/11)   \rWriting objects:  90% (10/11)   \rWriting objects: 100% (11/11)   \rWriting objects: 100% (11/11), 5.05 KiB | 5.05 MiB/s, done.\n",
            "Total 11 (delta 0), reused 0 (delta 0)\n",
            "To https://github.com/ShaswataJash/kfpcomponent.git\n",
            "   16d2782..ed99201  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwhyFmmz5bnu",
        "outputId": "f92c31ab-bc9e-4366-f09a-d5ad00dacc80"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}