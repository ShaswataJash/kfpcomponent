{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TabularDataPreparationUsingPycaret.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXmdJxO3pKUFweCKSF89Zy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/kfpcomponent/blob/main/TabularDataPreparationUsingPycaret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is the development workflow for kubeflow pipeline component of the same name as this notebook. Refer https://github.com/ShaswataJash/kfpcomponent"
      ],
      "metadata": {
        "id": "_fMb4fVRFsPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install required softwares"
      ],
      "metadata": {
        "id": "HYI5LKdFLCTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoSae8JMvVgq",
        "outputId": "f6e05500-1fb0-4575-e04b-70abd85389ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 108a0453d1b0 5.4.188+ #1 SMP Sun Apr 24 10:03:06 PDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZTBdke9vme4",
        "outputId": "488e732b-1f64-4db2-fc16-bb860473210b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.5 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4orJiv6orBy",
        "outputId": "ef8f54ae-b3e6-4d81-8fde-d7620760f3ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lus4TEd-8DbB",
        "outputId": "bc1aec8d-7aaf-4f68-b220-fa4f8411ae07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycaret==2.3.10\n",
            "  Downloading pycaret-2.3.10-py3-none-any.whl (320 kB)\n",
            "\u001b[K     |████████████████████████████████| 320 kB 22.6 MB/s \n",
            "\u001b[?25hCollecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 34.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.4.1)\n",
            "Collecting mlxtend>=0.17.0\n",
            "  Downloading mlxtend-0.20.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.6.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (5.5.0)\n",
            "Collecting pandas-profiling>=2.8.0\n",
            "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting lightgbm>=2.3.1\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cufflinks>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.17.3)\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting spacy<2.4.0\n",
            "  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.5.0)\n",
            "Requirement already satisfied: numba<0.55 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.51.2)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.3.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.7)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.8 MB 49.1 MB/s \n",
            "\u001b[?25hCollecting pyod\n",
            "  Downloading pyod-1.0.1.tar.gz (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 81.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (5.5.0)\n",
            "Requirement already satisfied: yellowbrick>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.4)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.15.3)\n",
            "Collecting kmodes>=0.10.1\n",
            "  Downloading kmodes-0.12.1-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.1.0)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 59.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (7.7.0)\n",
            "Requirement already satisfied: pyyaml<6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.13)\n",
            "Collecting imbalanced-learn==0.7.0\n",
            "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.7.0->pycaret==2.3.10) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->pycaret==2.3.10) (3.1.0)\n",
            "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (0.3.0)\n",
            "Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->pycaret==2.3.10) (5.2.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (5.4.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (5.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.3.1->pycaret==2.3.10) (0.37.1)\n",
            "Collecting mlxtend>=0.17.0\n",
            "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pycaret==2.3.10) (4.1.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.11.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (3.8.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<0.55->pycaret==2.3.10) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pycaret==2.3.10) (2022.1)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting multimethod>=1.4\n",
            "  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (4.64.0)\n",
            "Collecting pyyaml<6.0.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting markupsafe~=2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (1.8.2)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (0.5.1)\n",
            "Collecting tangled-up-in-unicode==0.2.0\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 58.3 MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (2.11.3)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting scipy<=1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.4.1->pycaret==2.3.10) (8.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->pycaret==2.3.10) (0.2.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2022.6.15)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (3.0.6)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (0.9.1)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (0.7.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (2.0.6)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (23.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.7.0)\n",
            "Collecting yellowbrick>=1.0.1\n",
            "  Downloading yellowbrick-1.3.post1-py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.13.3\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 233 kB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (1.3.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (0.4)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (0.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (21.3)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (7.1.2)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 60.1 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (3.17.3)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.4.37)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.1.4)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.0 MB/s \n",
            "\u001b[?25hCollecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret==2.3.10) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret==2.3.10) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow->pycaret==2.3.10) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret==2.3.10) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pycaret==2.3.10) (2022.6.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow->pycaret==2.3.10) (0.14.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (0.0)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 54.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod->pycaret==2.3.10) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod->pycaret==2.3.10) (0.5.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: htmlmin, imagehash, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=68c69615ee532191bf99510b340b8e4794d49cb45257402c41d588f61d6d2fc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=05467fa346f4301473f1d021c97603fb88af5971e1ec2f95903491c846ff2f50\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.0-py3-none-any.whl size=141960 sha256=a74384e85fe60e3fd4e585cb6b21cf62abb1a27a66d5d641915026000730f12b\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/c3/db/33705569425fd2bdc9ea73051a8053fa26965c2bce8a146747\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135617 sha256=8a2d4441ce65daa8a5a1cbed6563cb059057de09d3f74808616444e9f59712e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.0.1-py3-none-any.whl size=147473 sha256=fd830bb4e480bb12123e4109789c9791a6e45f612fa89c5a075cedb8fd5bb8bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/c4/29/67ad87835b209f72e4706369c683741b09490f2829d64ea768\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=84bc5ed3a89297edc751d6fcd4e59b909a001f04dd5a67663417fdcb4932a237\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=3742fc5f186e66eed2c4907ab3030993db721e5bd3a624e11fcc286bbf010f15\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built htmlmin imagehash databricks-cli pyLDAvis pyod umap-learn pynndescent\n",
            "Installing collected packages: markupsafe, numpy, tangled-up-in-unicode, smmap, scipy, multimethod, websocket-client, visions, srsly, scikit-learn, requests, pyjwt, plac, Mako, imagehash, gitdb, catalogue, thinc, querystring-parser, pyyaml, pynndescent, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, yellowbrick, umap-learn, spacy, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.3\n",
            "    Uninstalling srsly-2.4.3:\n",
            "      Successfully uninstalled srsly-2.4.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.7\n",
            "    Uninstalling catalogue-2.0.7:\n",
            "      Successfully uninstalled catalogue-2.0.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: yellowbrick\n",
            "    Found existing installation: yellowbrick 1.4\n",
            "    Uninstalling yellowbrick-1.4:\n",
            "      Successfully uninstalled yellowbrick-1.4\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.3.1\n",
            "    Uninstalling spacy-3.3.1:\n",
            "      Successfully uninstalled spacy-3.3.1\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "  Attempting uninstall: mlxtend\n",
            "    Found existing installation: mlxtend 0.14.0\n",
            "    Uninstalling mlxtend-0.14.0:\n",
            "      Successfully uninstalled mlxtend-0.14.0\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.8.1\n",
            "    Uninstalling imbalanced-learn-0.8.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "en-core-web-sm 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 2.3.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Boruta-0.3 Mako-1.2.0 alembic-1.8.0 catalogue-1.0.0 databricks-cli-0.17.0 docker-5.0.3 funcy-1.17 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 kmodes-0.12.1 lightgbm-3.3.2 markupsafe-2.1.1 mlflow-1.26.1 mlxtend-0.19.0 multimethod-1.8 numpy-1.19.5 pandas-profiling-3.2.0 phik-0.12.2 plac-1.1.3 prometheus-flask-exporter-0.20.2 pyLDAvis-3.2.2 pycaret-2.3.10 pyjwt-2.4.0 pynndescent-0.5.7 pyod-1.0.1 pyyaml-5.4.1 querystring-parser-1.2.4 requests-2.28.0 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 spacy-2.3.7 srsly-1.0.5 tangled-up-in-unicode-0.2.0 thinc-7.4.5 umap-learn-0.5.3 visions-0.7.4 websocket-client-1.3.3 yellowbrick-1.3.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pycaret==2.3.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ref: https://github.com/pycaret/pycaret/issues/2490\n",
        "!pip install Jinja2==3.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKAa1jnEnDL3",
        "outputId": "1210badf-3742-467e-b649-304527bbdaca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Jinja2==3.1.2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2==3.1.2) (2.1.1)\n",
            "Installing collected packages: Jinja2\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Jinja2-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ca-certificates fuse tzdata curl unzip && \\\n",
        "  echo \"user_allow_other\" >> /etc/fuse.conf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p89zTHfYqwc",
        "outputId": "3d298e5e-82a4-44e4-e305-2590a92d2132"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fuse is already the newest version (2.9.7-1ubuntu1).\n",
            "ca-certificates is already the newest version (20211016~18.04.1).\n",
            "curl is already the newest version (7.58.0-2ubuntu3.18).\n",
            "tzdata is already the newest version (2022a-0ubuntu0.18.04).\n",
            "tzdata set to manually installed.\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://rclone.org/install.sh | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGe2hZo0U7DL",
        "outputId": "13eba8ec-ac4d-4094-a7ed-c52d7d5fe493"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4497  100  4497    0     0  11442      0 --:--:-- --:--:-- --:--:-- 11442\n",
            "Archive:  rclone-current-linux-amd64.zip\n",
            "   creating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/\n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/rclone  [binary]\n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/README.html  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/README.txt  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/git-log.txt  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/rclone.1  [text]  \n",
            "Purging old database entries in /usr/share/man...\n",
            "Processing manual pages under /usr/share/man...\n",
            "Purging old database entries in /usr/share/man/id...\n",
            "Processing manual pages under /usr/share/man/id...\n",
            "Purging old database entries in /usr/share/man/sv...\n",
            "Processing manual pages under /usr/share/man/sv...\n",
            "Purging old database entries in /usr/share/man/nl...\n",
            "Processing manual pages under /usr/share/man/nl...\n",
            "Purging old database entries in /usr/share/man/ru...\n",
            "Processing manual pages under /usr/share/man/ru...\n",
            "Purging old database entries in /usr/share/man/fi...\n",
            "Processing manual pages under /usr/share/man/fi...\n",
            "Purging old database entries in /usr/share/man/da...\n",
            "Processing manual pages under /usr/share/man/da...\n",
            "Purging old database entries in /usr/share/man/zh_CN...\n",
            "Processing manual pages under /usr/share/man/zh_CN...\n",
            "Purging old database entries in /usr/share/man/hu...\n",
            "Processing manual pages under /usr/share/man/hu...\n",
            "Purging old database entries in /usr/share/man/ja...\n",
            "Processing manual pages under /usr/share/man/ja...\n",
            "Purging old database entries in /usr/share/man/it...\n",
            "Processing manual pages under /usr/share/man/it...\n",
            "Purging old database entries in /usr/share/man/de...\n",
            "Processing manual pages under /usr/share/man/de...\n",
            "Purging old database entries in /usr/share/man/tr...\n",
            "Processing manual pages under /usr/share/man/tr...\n",
            "Purging old database entries in /usr/share/man/fr...\n",
            "Processing manual pages under /usr/share/man/fr...\n",
            "Purging old database entries in /usr/share/man/ko...\n",
            "Processing manual pages under /usr/share/man/ko...\n",
            "Purging old database entries in /usr/share/man/cs...\n",
            "Processing manual pages under /usr/share/man/cs...\n",
            "Purging old database entries in /usr/share/man/pl...\n",
            "Processing manual pages under /usr/share/man/pl...\n",
            "Purging old database entries in /usr/share/man/zh_TW...\n",
            "Processing manual pages under /usr/share/man/zh_TW...\n",
            "Purging old database entries in /usr/share/man/es...\n",
            "Processing manual pages under /usr/share/man/es...\n",
            "Purging old database entries in /usr/share/man/pt...\n",
            "Processing manual pages under /usr/share/man/pt...\n",
            "Purging old database entries in /usr/share/man/sr...\n",
            "Processing manual pages under /usr/share/man/sr...\n",
            "Purging old database entries in /usr/share/man/pt_BR...\n",
            "Processing manual pages under /usr/share/man/pt_BR...\n",
            "Processing manual pages under /usr/local/man...\n",
            "Updating index cache for path `/usr/local/man/man1'. Wait...done.\n",
            "Checking for stray cats under /usr/local/man...\n",
            "Checking for stray cats under /var/cache/man/oldlocal...\n",
            "1 man subdirectory contained newer manual pages.\n",
            "3 manual pages were added.\n",
            "0 stray cats were added.\n",
            "19 old database entries were purged.\n",
            "\n",
            "rclone v1.58.1 has successfully installed.\n",
            "Now run \"rclone config\" for setup. Check https://rclone.org/docs/ for more details.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rclone --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byk9g7UWUtJO",
        "outputId": "ec863570-a8c7-4291-dfe7-1f040b9c8939"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rclone v1.58.1\n",
            "- os/version: ubuntu 18.04 (64 bit)\n",
            "- os/kernel: 5.4.188+ (x86_64)\n",
            "- os/type: linux\n",
            "- os/arch: amd64\n",
            "- go/version: go1.17.9\n",
            "- go/linking: static\n",
            "- go/tags: none\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Develop source code files"
      ],
      "metadata": {
        "id": "EMVyzkX_LIoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_preparation.py\n",
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "\n",
        "# Defining and parsing the command-line arguments\n",
        "parser = argparse.ArgumentParser(description='kubeflow pipeline component to read csv file and prepare the data')\n",
        "parser.add_argument('--bypass-rclone-for-input-data', default=False, action=\"store_true\", help='whether input csv file should be read like local file - rclone is completely bypassed')\n",
        "parser.add_argument('--bypass-rclone-for-output-data', default=False, action=\"store_true\", help='whether output csv file should be written like local file - rclone is completely bypassed')\n",
        "parser.add_argument('--rclone-environment-var', type=str, default= '{}', help='json formatted key-value pairs of strings which will be set as environment variables before executing rclone commands')\n",
        "parser.add_argument('--input-datasource-directory-mountable', default=False, action=\"store_true\", help='whether input csv file is present in mountable remote location when rclone is used')\n",
        "parser.add_argument('--input-datasource-file-name', type=str, default='', help='name of the csv file including file extension and the directory/bucket path holding the specific file(if any) when rclone is used')\n",
        "parser.add_argument('--additional-options-csv-parsing', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pandas.read_csv()')\n",
        "parser.add_argument('--type-of-data-analysis-task', choices=['classification', 'regression', 'clustering', 'anomaly_detection'])\n",
        "parser.add_argument('--target-variable-name', type=str, help='for classification and regression, specify the column name holding target variable')\n",
        "parser.add_argument('--target-emptyindicator', type=str, default='', help='if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc')\n",
        "parser.add_argument('--data-preparations-options', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pycaret setup() function')\n",
        "parser.add_argument('--additional-options-csv-writing', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pandas.to_csv()')\n",
        "parser.add_argument('--output-datasource-directory-mountable', default=False, action=\"store_true\", help='whether output csv file will be written in mountable remote location when rclone is used')\n",
        "parser.add_argument('--output-datasource-file-name', type=str, default='', help='filename of the prepared data including the directory/bucket path holding the specific file(if any) when rclone is used')\n",
        "parser.add_argument('--input-datasource-local-file-path-when-rclone-bypassed', type=str, default='', help='absolute local path of the input csv file when rclone is NOT used i.e. when bypass-rclone-for-input-data is enabled')\n",
        "parser.add_argument('--output-datasource-local-file-path-when-rclone-bypassed', type=str, default= '', help='absolute local path of the output csv file when rclone is NOT used i.e. when bypass-rclone-for-output-data is enabled')\n",
        "args = parser.parse_args()\n",
        "\n",
        "#sanity check of arguments\n",
        "if args.bypass_rclone_for_input_data:\n",
        "    args.input_datasource_directory_mountable = False\n",
        "    args.input_datasource_file_name = None\n",
        "else:\n",
        "    args.input_datasource_local_file_path_when_rclone_bypassed = None\n",
        "\n",
        "if args.bypass_rclone_for_output_data:\n",
        "    args.output_datasource_directory_mountable = False\n",
        "    args.output_datasource_file_name = None\n",
        "else:\n",
        "    args.output_datasource_local_file_path_when_rclone_bypassed = None\n",
        "\n",
        "if args.bypass_rclone_for_input_data and args.bypass_rclone_for_output_data:\n",
        "    args.rclone_environment_var = '{}'\n",
        "\n",
        "#setting rclone related env\n",
        "import os\n",
        "import json\n",
        "rclone_config = json.loads(args.rclone_environment_var)\n",
        "print('rclone_config = (', type(rclone_config), ')', rclone_config)\n",
        "for item in rclone_config.items():\n",
        "    os.environ[item[0]] = item[1]\n",
        "\n",
        "#temporary directory creation\n",
        "import tempfile\n",
        "if not args.bypass_rclone_for_input_data:\n",
        "    local_datastore_read_dir = tempfile.mkdtemp(prefix=\"my_local_read-\")\n",
        "    print('local_datastore_read_dir:',local_datastore_read_dir)\n",
        "\n",
        "if not args.bypass_rclone_for_output_data:\n",
        "    local_datastore_write_dir = tempfile.mkdtemp(prefix=\"my_local_write-\")\n",
        "    print('local_datastore_write_dir:',local_datastore_write_dir)\n",
        "\n",
        "#input file handling\n",
        "import subprocess\n",
        "import sys\n",
        "import ntpath\n",
        "if args.input_datasource_directory_mountable:\n",
        "    input_data_read_cmd = \"rclone -v mount remoteread:\" + ntpath.dirname(args.input_datasource_file_name) + ' ' + local_datastore_read_dir + ' --daemon'\n",
        "else:\n",
        "    input_data_read_cmd = \"rclone -v copy remoteread:\" + args.input_datasource_file_name + ' ' + local_datastore_read_dir\n",
        "print(input_data_read_cmd)\n",
        "input_data_read_call = subprocess.run(input_data_read_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(input_data_read_call.stdout)\n",
        "if input_data_read_call.returncode != 0:\n",
        "    print(\"Error in rclone, errorcode=\", input_data_read_call.returncode)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as rclone returned error in context of reading\")\n",
        "\n",
        "#output file handling\n",
        "if args.output_datasource_directory_mountable:\n",
        "    output_data_write_cmd = \"rclone -v mount remotewrite:\" + ntpath.dirname(args.output_datasource_file_name) + ' ' + local_datastore_write_dir + ' --daemon'\n",
        "    print(output_data_write_cmd)\n",
        "    output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(output_data_write_call.stdout)\n",
        "    if output_data_write_call.returncode != 0:\n",
        "        print(\"Error in rclone, errorcode=\", output_data_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as rclone returned error in context of mounted writing\")\n",
        "\n",
        "#handling input csv file reading\n",
        "import pandas\n",
        "try:\n",
        "    parse_config = json.loads(args.additional_options_csv_parsing)\n",
        "    parse_config['filepath_or_buffer'] =  args.input_datasource_local_file_path_when_rclone_bypassed \\\n",
        "        if args.bypass_rclone_for_input_data else os.path.join(local_datastore_read_dir,ntpath.basename(args.input_datasource_file_name))\n",
        "    print('parse_config = (', type(parse_config), ')', parse_config)\n",
        "    my_data = pandas.read_csv(**parse_config)\n",
        "    print(my_data)\n",
        "    \n",
        "except BaseException as err:\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while parsing input csv file\")\n",
        "\n",
        "#handling data preprocessing\n",
        "import pycaret\n",
        "try:\n",
        "    if os.path.exists(\"logs.log\"):\n",
        "        os.remove(\"logs.log\") #removing any content from log which pycaret will internally use for its own logging\n",
        "    print('pycaret version = ', pycaret.utils.version())\n",
        "    setup_config = json.loads(args.data_preparations_options)\n",
        "    if args.type_of_data_analysis_task == 'classification':\n",
        "        import pycaret.classification\n",
        "        setup_fn = pycaret.classification.setup\n",
        "        get_config_fn = pycaret.classification.get_config\n",
        "        setup_config['target'] = args.target_variable_name\n",
        "        \n",
        "    elif args.type_of_data_analysis_task == 'regression':\n",
        "        import pycaret.regression\n",
        "        setup_fn = pycaret.regression.setup\n",
        "        get_config_fn = pycaret.regression.get_config\n",
        "        setup_config['target'] = args.target_variable_name\n",
        "\n",
        "    elif args.type_of_data_analysis_task == 'clustering':\n",
        "        import pycaret.clustering\n",
        "        setup_fn = pycaret.clustering.setup\n",
        "        get_config_fn = pycaret.clustering.get_config\n",
        "\n",
        "    elif args.type_of_data_analysis_task == 'anomaly':\n",
        "        import pycaret.anomaly\n",
        "        setup_fn = pycaret.anomaly.setup\n",
        "        get_config_fn = pycaret.anomaly.get_config\n",
        "        \n",
        "    #as part of pycaret's data cleaning the rows with target column = nan are not being cleaned up. Thus, cleaning those rows explicitely\n",
        "    if len(args.target_emptyindicator) > 0:\n",
        "        #ref: https://stackoverflow.com/questions/49291740/delete-rows-if-there-are-null-values-in-a-specific-column-in-pandas-dataframe\n",
        "        import numpy as np\n",
        "        my_data[args.target_variable_name] = my_data[args.target_variable_name].replace(args.target_emptyindicator, np.nan)\n",
        "        my_data = my_data.dropna(axis=0, subset=[args.target_variable_name])\n",
        "\n",
        "    setup_config['data'] = my_data\n",
        "    setup_config['log_experiment'] = False\n",
        "    setup_config['data_split_shuffle'] = False\n",
        "    setup_config['html'] = False\n",
        "    setup_config['silent'] = True\n",
        "    print('setup_config = (', type(setup_config), ')', setup_config)\n",
        "    setup_fn(**setup_config)\n",
        "    #ref: https://www.kdnuggets.com/2020/11/5-things-doing-wrong-pycaret.html\n",
        "    X_transformed = get_config_fn('X')\n",
        "    my_transformed_data = X_transformed\n",
        "    if args.type_of_data_analysis_task == 'classification' or args.type_of_data_analysis_task == 'regression':\n",
        "        y_transformed = get_config_fn('y')\n",
        "        my_transformed_data = X_transformed.merge(y_transformed,left_index=True, right_index=True)\n",
        "    \n",
        "    print(\"====== PREPARED DATA ====\")\n",
        "    print(my_transformed_data)\n",
        "    print(\"=========================\")\n",
        "\n",
        "    pycaret.utils.get_system_logs() #this will print the pycaret's own log into console\n",
        "    \n",
        "except BaseException as err:\n",
        "    pycaret.utils.get_system_logs()\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while transforming input dataframe\")\n",
        "\n",
        "#handling output csv file writing\n",
        "try:\n",
        "    to_csv_config = json.loads(args.additional_options_csv_writing)\n",
        "    to_csv_config['path_or_buf'] = args.output_datasource_local_file_path_when_rclone_bypassed \\\n",
        "        if args.bypass_rclone_for_output_data else os.path.join(local_datastore_write_dir,ntpath.basename(args.output_datasource_file_name))\n",
        "    print('to_csv_config = (', type(to_csv_config), ')', to_csv_config)\n",
        "    my_transformed_data.to_csv(**to_csv_config)\n",
        "except BaseException as err:\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while trying to write prepared data\")\n",
        "\n",
        "if args.bypass_rclone_for_output_data:\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(0)\n",
        "\n",
        "if not args.output_datasource_directory_mountable:\n",
        "    output_data_write_cmd = \"rclone -v copy \" + os.path.join(local_datastore_write_dir,ntpath.basename(args.output_datasource_file_name)) \\\n",
        "        + \" remotewrite:\" + args.output_datasource_file_name\n",
        "    print(output_data_write_cmd)\n",
        "    output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(output_data_write_call.stdout)\n",
        "    if output_data_write_call.returncode != 0:\n",
        "        print(\"Error in rclone, errorcode=\", output_data_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as rclone returned error in context of writing final csv file (copy mode)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU9Vh7P4P4_h",
        "outputId": "066d40d2-1809-45d3-e50b-9a03ceeea77c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_preparation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docker size reduction tips:\n",
        "\n",
        "\n",
        "*   https://devopscube.com/reduce-docker-image-size/\n",
        "*   https://www.ecloudcontrol.com/best-practices-to-reduce-docker-images-size/\n",
        "\n"
      ],
      "metadata": {
        "id": "eQXf07hoPzxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.7.13-slim\n",
        "\n",
        "RUN python3 -m pip install pycaret==2.3.10\n",
        "#installing jinja2 additionally due to Ref: https://github.com/pycaret/pycaret/issues/2490\n",
        "RUN python3 -m pip install Jinja2==3.1.2\n",
        "\n",
        "#install fuse as dependency for rclone. Additionally, install curl, unzip for rclone installer to work\n",
        "#libgomp1 installation for pycaret in python-slim\n",
        "RUN apt-get update \\\n",
        "    && apt-get install --no-install-recommends -y curl fuse libgomp1 unzip \\\n",
        "    && echo \"user_allow_other\" >> /etc/fuse.conf \\\n",
        "    && curl https://rclone.org/install.sh | bash \\\n",
        "    && apt-get -y remove --purge curl unzip \\\n",
        "    && rm -rf /var/lib/apt/lists/* \\\n",
        "    && rclone --version\n",
        "\n",
        "COPY src/data_preparation.py /tmp\n",
        "COPY tests/test_validation.py /tmp\n",
        "COPY run_tests.sh /tmp\n",
        "RUN chmod 544 /tmp/run_tests.sh"
      ],
      "metadata": {
        "id": "hk00hb780Qo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77584e9-843f-44ef-cb70-e65d76818b37"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_tests.sh\n",
        "#!/bin/bash\n",
        "\n",
        "mkdir /tmp/my_local_dir_for_test\n",
        "\n",
        "#Test: csv reading source from http, rclone read in copy\n",
        "python /tmp/data_preparation.py --rclone-environment-var '{\"RCLONE_CONFIG_REMOTEREAD_TYPE\":\"http\", \"RCLONE_CONFIG_REMOTEREAD_URL\":\"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/\"}' \\\n",
        "    --input-datasource-file-name 'CTG.csv' --additional-options-csv-parsing '{\"sep\":\",\" , \"header\":0}' \\\n",
        "    --type-of-data-analysis-task 'classification' --target-variable-name 'NSP' \\\n",
        "    --data-preparations-options '{\"ignore_low_variance\":true, \"remove_outliers\":true, \"remove_multicollinearity\":true, \"multicollinearity_threshold\":0.7}' \\\n",
        "    --bypass-rclone-for-output-data --output-datasource-local-file-path-when-rclone-bypassed '/tmp/my_local_dir_for_test/CTG_data-prep.csv' \\\n",
        "    --additional-options-csv-writing '{\"index\":false}'\n",
        "\n",
        "#https://registry.opendata.aws/humor-detection/\n",
        "#Test: csv reading source from s3(AWS provider), rclone read in mount\n",
        "python /tmp/data_preparation.py --rclone-environment-var '{\"RCLONE_CONFIG_REMOTEREAD_TYPE\":\"s3\", \"RCLONE_CONFIG_REMOTEREAD_PROVIDER\":\"AWS\", \"RCLONE_CONFIG_REMOTEREAD_REGION\":\"us-west-2\"}' \\\n",
        "    --input-datasource-directory-mountable --input-datasource-file-name 'humor-detection-pds/Non-humours-biased.csv' \\\n",
        "    --type-of-data-analysis-task 'classification' --target-variable-name 'label' \\\n",
        "    --data-preparations-options '{\"preprocess\":false, \"ignore_features\":[\"image_url\"]}' \\\n",
        "    --bypass-rclone-for-output-data --output-datasource-local-file-path-when-rclone-bypassed '/tmp/my_local_dir_for_test/Non-humours-biased_data-prep.csv' \\\n",
        "    --additional-options-csv-writing '{\"index\":false}'\n",
        "\n",
        "python /tmp/test_validation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzJg29vTVGcf",
        "outputId": "db4dfda5-b3a1-49e8-d550-72b61676502d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_tests.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_validation.py\n",
        "#!/usr/bin/env python3\n",
        "import pandas\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/CTG_data-prep.csv')\n",
        "assert len(df.index) == 2126 #original data had 2129 rows, amongst that 3 rows have no target\n",
        "assert df.isnull().sum().sum() == 0 #pycaret will remove all missing values\n",
        "\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/Non-humours-biased_data-prep.csv')\n",
        "assert len(df.columns) == 3 #original data had 4 columns, amongst that one will be dropped\n",
        "print ('test-validation done successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7maMc-RVqDV5",
        "outputId": "178af29b-97f9-4379-c99a-e9f761bc8059"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_validation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer: https://github.com/RealOrangeOne/docker-rclone-mount/blob/master/docker-compose.yml\n",
        "\n",
        "If we have to use mount feature of rclone, it needs to have fuse support in underneath linux kernel. For that we are adding SYS_ADMIN in capability. But note without using mount feature also, we can do testing. in that case, rclone will use only copy feature."
      ],
      "metadata": {
        "id": "YRSP8Vk8vyoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile docker-compose.test.yml\n",
        "services:\n",
        "  sut:\n",
        "    build: .\n",
        "    command: /tmp/run_tests.sh\n",
        "    cap_add:\n",
        "      - SYS_ADMIN\n",
        "    security_opt:\n",
        "      - apparmor:unconfined\n",
        "    devices:\n",
        "      - \"/dev/fuse:/dev/fuse\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVrXyQZhSIm_",
        "outputId": "ca7f7333-baef-4b55-ec26-97ed7f338d08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing docker-compose.test.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#designing-a-pipeline-component\n",
        "*   https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py\n",
        "*   https://kubeflow-pipelines.readthedocs.io/en/stable/_modules/kfp/components/_structures.html\n",
        "\n"
      ],
      "metadata": {
        "id": "W7uyWGT6Sccr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component_both_input_output_as_artifact.yaml\n",
        "name: TabularDataPreparationUsingPycaretWhereBothInputOutputAsArtifact\n",
        "description: |\n",
        "    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)\n",
        "    Refer data-preparations-options in command line arguments. \n",
        "    pycaret internally uses pandas dataframe to read and write csv file. You can utilize options exposed by panda's read_csv() and to_csv(). \n",
        "    Refer additional-options-csv-parsing and additional-options-csv-writing in command line arguments\n",
        "    Input and output csv files are stored in input and output artifacts. Thus the csv files are read or written like locally mounted POSIX files.\n",
        "\n",
        "inputs:\n",
        "- name: additional_options_csv_parsing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'\n",
        "  optional: true\n",
        "- name: type_of_data_analysis_task\n",
        "  type: String\n",
        "  description: 'choice amongst classification, regression, clustering, anomaly_detection'\n",
        "- name: target_variable_name \n",
        "  type: String\n",
        "  description: 'for classification and regression, specify the column name holding target variable'\n",
        "  optional: true\n",
        "- name: target_emptyindicator\n",
        "  type: String\n",
        "  description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'\n",
        "  optional: true\n",
        "- name: data_preparations_options\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'\n",
        "  optional: true\n",
        "- name: additional_options_csv_writing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'\n",
        "  optional: true\n",
        "- name: input_datasource_local_file_path_when_rclone_bypassed\n",
        "  description: 'absolute local path of the input csv file when rclone is NOT used i.e. when input csv file is stored in input artifact of pipeline engine (e.g. argo)'\n",
        "\n",
        "outputs:\n",
        "- name: output_datasource_local_file_path_when_rclone_bypassed\n",
        "  description: 'absolute local path of the output csv file when rclone is NOT used i.e. when output csv file is stored in output artifact of pipeline engine (e.g. argo)'\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: shasjash/kfpcomponents:TabularDataPreparationUsingPycaret_devlatest\n",
        "    command:\n",
        "    - python3 \n",
        "    - /tmp/data_preparation.py\n",
        "    args:\n",
        "    - --bypass-rclone-for-input-data\n",
        "    - --bypass-rclone-for-output-data\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_parsing}\n",
        "        then:\n",
        "        - --additional-options-csv-parsing\n",
        "        - {inputValue: additional_options_csv_parsing}\n",
        "    - --type-of-data-analysis-task \n",
        "    - {inputValue: type_of_data_analysis_task}\n",
        "    - if:\n",
        "        cond: {isPresent: target_variable_name}\n",
        "        then:\n",
        "        - --target-variable-name\n",
        "        - {inputValue: target_variable_name}\n",
        "    - if:\n",
        "        cond: {isPresent: target_emptyindicator}\n",
        "        then:\n",
        "        - --target-emptyindicator\n",
        "        - {inputValue: target_emptyindicator}\n",
        "    - if:\n",
        "        cond: {isPresent: data_preparations_options}\n",
        "        then:\n",
        "        - --data-preparations-options\n",
        "        - {inputValue: data_preparations_options}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_writing}\n",
        "        then:\n",
        "        - --additional-options-csv-writing\n",
        "        - {inputValue: additional_options_csv_writing}\n",
        "    - --input-datasource-local-file-path-when-rclone-bypassed\n",
        "    - {inputPath: input_datasource_local_file_path_when_rclone_bypassed}\n",
        "    - --output-datasource-local-file-path-when-rclone-bypassed\n",
        "    - {outputPath: output_datasource_local_file_path_when_rclone_bypassed}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhXccnmvIo7u",
        "outputId": "712cd799-d340-4464-bf42-fe8f7be95f25"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting component_both_input_output_as_artifact.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component_input_using_rclone_output_as_artifact.yaml\n",
        "name: TabularDataPreparationUsingPycaretWhereInputUsingRcloneOutputAsArtifact\n",
        "description: |\n",
        "    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)\n",
        "    Refer data-preparations-options in command line arguments. \n",
        "    pycaret internally uses pandas dataframe to read and write csv file. You can utilize options exposed by panda's read_csv() and to_csv(). \n",
        "    Refer additional-options-csv-parsing and additional-options-csv-writing in command line arguments\n",
        "    Input csv files can be stored in rclone compatible storage. Both mount and copy mode are supported. (refer: https://rclone.org/)\n",
        "    rclone configurations have to be shared through environment variables (refer: https://rclone.org/docs/#environment-variables). \n",
        "    Create rclone read configuration file name as 'REMOTEREAD' . Because the same is used within code.\n",
        "    So convention for creating any environment variables related to rclone-read should start with 'RCLONE_CONFIG_REMOTEREAD'.\n",
        "    Output csv files are stored in output artifacts. Thus the csv files are written like locally mounted POSIX files.\n",
        "\n",
        "inputs:\n",
        "- name: rclone_environment_var\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be set as environment variables before executing rclone commands'\n",
        "- name: input_datasource_directory_mountable\n",
        "  type: Boolean \n",
        "  description: 'whether input csv file is present in mountable remote location  when rclone is used'\n",
        "  optional: true\n",
        "- name: input_datasource_file_name\n",
        "  type: String\n",
        "  description: 'name of the csv file including file extension and the directory/bucket path holding the specific file(if any)  when rclone is used'\n",
        "- name: additional_options_csv_parsing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'\n",
        "  optional: true\n",
        "- name: type_of_data_analysis_task\n",
        "  type: String\n",
        "  description: 'choice amongst classification, regression, clustering, anomaly_detection'\n",
        "- name: target_variable_name \n",
        "  type: String\n",
        "  description: 'for classification and regression, specify the column name holding target variable'\n",
        "  optional: true\n",
        "- name: target_emptyindicator\n",
        "  type: String\n",
        "  description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'\n",
        "  optional: true\n",
        "- name: data_preparations_options\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'\n",
        "  optional: true\n",
        "- name: additional_options_csv_writing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'\n",
        "  optional: true\n",
        "\n",
        "outputs:\n",
        "- name: output_datasource_local_file_path_when_rclone_bypassed\n",
        "  description: 'absolute local path of the output csv file when rclone is NOT used i.e. when output csv file is stored in output artifact of pipeline engine (e.g. argo)'\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: shasjash/kfpcomponents:TabularDataPreparationUsingPycaret_devlatest\n",
        "    command:\n",
        "    - python3 \n",
        "    - /tmp/data_preparation.py\n",
        "    args:\n",
        "    - --bypass-rclone-for-output-data\n",
        "    - --rclone-environment-var\n",
        "    - {inputValue: rclone_environment_var}\n",
        "    - if:\n",
        "        cond: {isPresent: input_datasource_directory_mountable}\n",
        "        then:\n",
        "        - --input-datasource-directory-mountable\n",
        "        - {inputValue: input_datasource_directory_mountable}\n",
        "    - --input-datasource-file-name\n",
        "    - {inputValue: input_datasource_file_name}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_parsing}\n",
        "        then:\n",
        "        - --additional-options-csv-parsing\n",
        "        - {inputValue: additional_options_csv_parsing}\n",
        "    - --type-of-data-analysis-task \n",
        "    - {inputValue: type_of_data_analysis_task}\n",
        "    - if:\n",
        "        cond: {isPresent: target_variable_name}\n",
        "        then:\n",
        "        - --target-variable-name\n",
        "        - {inputValue: target_variable_name}\n",
        "    - if:\n",
        "        cond: {isPresent: target_emptyindicator}\n",
        "        then:\n",
        "        - --target-emptyindicator\n",
        "        - {inputValue: target_emptyindicator}\n",
        "    - if:\n",
        "        cond: {isPresent: data_preparations_options}\n",
        "        then:\n",
        "        - --data-preparations-options\n",
        "        - {inputValue: data_preparations_options}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_writing}\n",
        "        then:\n",
        "        - --additional-options-csv-writing\n",
        "        - {inputValue: additional_options_csv_writing}\n",
        "    - --output-datasource-local-file-path-when-rclone-bypassed\n",
        "    - {outputPath: output_datasource_local_file_path_when_rclone_bypassed}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBI4M_Blh8l5",
        "outputId": "0917c87f-c466-4b60-a3ff-cc3f90ff82f2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting component_input_using_rclone_output_as_artifact.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component_input_as_artifact_output_using_rclone.yaml\n",
        "name: TabularDataPreparationUsingPycaretWhereInputAsArtifactOutputUsingRclone\n",
        "description: |\n",
        "    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)\n",
        "    Refer data-preparations-options in command line arguments. \n",
        "    pycaret internally uses pandas dataframe to read and write csv file. You can utilize options exposed by panda's read_csv() and to_csv(). \n",
        "    Refer additional-options-csv-parsing and additional-options-csv-writing in command line arguments\n",
        "    Output csv files can be stored in rclone compatible storage. Both mount and copy mode are supported. (refer: https://rclone.org/)\n",
        "    rclone configurations have to be shared through environment variables (refer: https://rclone.org/docs/#environment-variables). \n",
        "    Create rclone write configuration file name as 'REMOTEWRITE'. Because the same is used within code.\n",
        "    So convention for creating any environment variables related to rclone should start either with 'RCLONE_CONFIG_REMOTEWRITE'.\n",
        "    Intput csv files are stored in intput artifacts. Thus the csv files are read like locally mounted POSIX files.\n",
        "\n",
        "inputs:\n",
        "- name: rclone_environment_var\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be set as environment variables before executing rclone commands'\n",
        "- name: additional_options_csv_parsing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'\n",
        "  optional: true\n",
        "- name: type_of_data_analysis_task\n",
        "  type: String\n",
        "  description: 'choice amongst classification, regression, clustering, anomaly_detection'\n",
        "- name: target_variable_name \n",
        "  type: String\n",
        "  description: 'for classification and regression, specify the column name holding target variable'\n",
        "  optional: true\n",
        "- name: target_emptyindicator\n",
        "  type: String\n",
        "  description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'\n",
        "  optional: true\n",
        "- name: data_preparations_options\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'\n",
        "  optional: true\n",
        "- name: additional_options_csv_writing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'\n",
        "  optional: true\n",
        "- name: output_datasource_directory_mountable\n",
        "  type: Boolean\n",
        "  description: 'whether output csv file will be written in mountable remote location  when rclone is used'\n",
        "  optional: true\n",
        "- name: output_datasource_file_name\n",
        "  type: String \n",
        "  description: 'filename of the prepared data including the directory/bucket path holding the specific file(if any) when rclone is used'\n",
        "- name: input_datasource_local_file_path_when_rclone_bypassed\n",
        "  description: 'absolute local path of the input csv file when rclone is NOT used i.e. when input csv file is stored in input artifact of pipeline engine (e.g. argo)'\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: shasjash/kfpcomponents:TabularDataPreparationUsingPycaret_devlatest\n",
        "    command:\n",
        "    - python3 \n",
        "    - /tmp/data_preparation.py\n",
        "    args:\n",
        "    - --bypass-rclone-for-input-data\n",
        "    - --rclone-environment-var\n",
        "    - {inputValue: rclone_environment_var}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_parsing}\n",
        "        then:\n",
        "        - --additional-options-csv-parsing\n",
        "        - {inputValue: additional_options_csv_parsing}\n",
        "    - --type-of-data-analysis-task \n",
        "    - {inputValue: type_of_data_analysis_task}\n",
        "    - if:\n",
        "        cond: {isPresent: target_variable_name}\n",
        "        then:\n",
        "        - --target-variable-name\n",
        "        - {inputValue: target_variable_name}\n",
        "    - if:\n",
        "        cond: {isPresent: target_emptyindicator}\n",
        "        then:\n",
        "        - --target-emptyindicator\n",
        "        - {inputValue: target_emptyindicator}\n",
        "    - if:\n",
        "        cond: {isPresent: data_preparations_options}\n",
        "        then:\n",
        "        - --data-preparations-options\n",
        "        - {inputValue: data_preparations_options}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_writing}\n",
        "        then:\n",
        "        - --additional-options-csv-writing\n",
        "        - {inputValue: additional_options_csv_writing}\n",
        "    - if:\n",
        "        cond: {isPresent: output_datasource_directory_mountable}\n",
        "        then:\n",
        "        - --output-datasource-directory-mountable\n",
        "        - {inputValue: output_datasource_directory_mountable}\n",
        "    - if:\n",
        "        cond: {isPresent: output_datasource_file_name}\n",
        "        then:\n",
        "        - --output-datasource-file-name\n",
        "        - {inputValue: output_datasource_file_name}\n",
        "    - --input-datasource-local-file-path-when-rclone-bypassed\n",
        "    - {inputPath: input_datasource_local_file_path_when_rclone_bypassed}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Iofu9sh9_a",
        "outputId": "3a3ebe88-c756-4381-cbd1-b7e844f66a2f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting component_input_as_artifact_output_using_rclone.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component_both_input_output_using_rclone.yaml\n",
        "name: TabularDataPreparationUsingPycaretWhereBothInputOutputUsingRclone\n",
        "description: |\n",
        "    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)\n",
        "    Refer data-preparations-options in command line arguments. \n",
        "    pycaret internally uses pandas dataframe to read and write csv file. You can utilize options exposed by panda's read_csv() and to_csv(). \n",
        "    Refer additional-options-csv-parsing and additional-options-csv-writing in command line arguments\n",
        "    Input and output csv files can be stored in rclone compatible storage. Both mount and copy mode are supported. (refer: https://rclone.org/)\n",
        "    rclone configurations have to be shared through environment variables (refer: https://rclone.org/docs/#environment-variables). \n",
        "    Create rclone read and write configuration file name as 'REMOTEREAD' and 'REMOTEWRITE'. Because the same are used within code.\n",
        "    So convention for creating any environment variables related to rclone should start either with 'RCLONE_CONFIG_REMOTEREAD' or 'RCLONE_CONFIG_REMOTEWRITE'.\n",
        "\n",
        "inputs:\n",
        "- name: rclone_environment_var\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be set as environment variables before executing rclone commands'\n",
        "- name: input_datasource_directory_mountable\n",
        "  type: Boolean \n",
        "  description: 'whether input csv file is present in mountable remote location  when rclone is used'\n",
        "  optional: true\n",
        "- name: input_datasource_file_name\n",
        "  type: String\n",
        "  description: 'name of the csv file including file extension and the directory/bucket path holding the specific file(if any)  when rclone is used'\n",
        "- name: additional_options_csv_parsing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'\n",
        "  optional: true\n",
        "- name: type_of_data_analysis_task\n",
        "  type: String\n",
        "  description: 'choice amongst classification, regression, clustering, anomaly_detection'\n",
        "- name: target_variable_name \n",
        "  type: String\n",
        "  description: 'for classification and regression, specify the column name holding target variable'\n",
        "  optional: true\n",
        "- name: target_emptyindicator\n",
        "  type: String\n",
        "  description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'\n",
        "  optional: true\n",
        "- name: data_preparations_options\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'\n",
        "  optional: true\n",
        "- name: additional_options_csv_writing\n",
        "  type: String\n",
        "  description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'\n",
        "  optional: true\n",
        "- name: output_datasource_directory_mountable\n",
        "  type: Boolean\n",
        "  description: 'whether output csv file will be written in mountable remote location  when rclone is used'\n",
        "  optional: true\n",
        "- name: output_datasource_file_name\n",
        "  type: String \n",
        "  description: 'filename of the prepared data including the directory/bucket path holding the specific file(if any) when rclone is used'\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: shasjash/kfpcomponents:TabularDataPreparationUsingPycaret_devlatest\n",
        "    command:\n",
        "    - python3 \n",
        "    - /tmp/data_preparation.py\n",
        "    args:\n",
        "    - --rclone-environment-var\n",
        "    - {inputValue: rclone_environment_var}\n",
        "    - if:\n",
        "        cond: {isPresent: input_datasource_directory_mountable}\n",
        "        then:\n",
        "        - --input-datasource-directory-mountable\n",
        "        - {inputValue: input_datasource_directory_mountable}\n",
        "    - --input-datasource-file-name\n",
        "    - {inputValue: input_datasource_file_name}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_parsing}\n",
        "        then:\n",
        "        - --additional-options-csv-parsing\n",
        "        - {inputValue: additional_options_csv_parsing}\n",
        "    - --type-of-data-analysis-task \n",
        "    - {inputValue: type_of_data_analysis_task}\n",
        "    - if:\n",
        "        cond: {isPresent: target_variable_name}\n",
        "        then:\n",
        "        - --target-variable-name\n",
        "        - {inputValue: target_variable_name}\n",
        "    - if:\n",
        "        cond: {isPresent: target_emptyindicator}\n",
        "        then:\n",
        "        - --target-emptyindicator\n",
        "        - {inputValue: target_emptyindicator}\n",
        "    - if:\n",
        "        cond: {isPresent: data_preparations_options}\n",
        "        then:\n",
        "        - --data-preparations-options\n",
        "        - {inputValue: data_preparations_options}\n",
        "    - if:\n",
        "        cond: {isPresent: additional_options_csv_writing}\n",
        "        then:\n",
        "        - --additional-options-csv-writing\n",
        "        - {inputValue: additional_options_csv_writing}\n",
        "    - if:\n",
        "        cond: {isPresent: output_datasource_directory_mountable}\n",
        "        then:\n",
        "        - --output-datasource-directory-mountable\n",
        "        - {inputValue: output_datasource_directory_mountable}\n",
        "    - --output-datasource-file-name\n",
        "    - {inputValue: output_datasource_file_name}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfdOj1Zh-vX",
        "outputId": "c0ebb556-f3b8-4b7c-edc5-987861fd128d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting component_both_input_output_using_rclone.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Software testing"
      ],
      "metadata": {
        "id": "9uiUWKu4LWUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us simulate testing what will be done by docker-hub infrastructure as part of auto-testing by using docker-compose.test.yml present in github source repository."
      ],
      "metadata": {
        "id": "i3TK5UnMvfMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/my_local_dir_for_test\n",
        "!chmod 544 run_tests.sh\n",
        "!cp data_preparation.py /tmp\n",
        "!cp test_validation.py /tmp\n",
        "!./run_tests.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9h_W3T4H87",
        "outputId": "2d8f6d1c-80e8-4e33-bb23-5f0a2389729a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rclone_config = ( <class 'dict'> ) {'RCLONE_CONFIG_REMOTEREAD_TYPE': 'http', 'RCLONE_CONFIG_REMOTEREAD_URL': 'https://raw.githubusercontent.com/pycaret/datasets/main/data/common/'}\n",
            "local_datastore_read_dir: /tmp/my_local_read-xzsjxnwb\n",
            "rclone -v copy remoteread:CTG.csv /tmp/my_local_read-xzsjxnwb\n",
            "2022/06/22 12:48:42 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "2022/06/22 12:48:43 INFO  : CTG.csv: Copied (new)\n",
            "2022/06/22 12:48:43 INFO  : \n",
            "Transferred:   \t  277.715 KiB / 277.715 KiB, 100%, 0 B/s, ETA -\n",
            "Transferred:            1 / 1, 100%\n",
            "Elapsed time:         0.2s\n",
            "\n",
            "\n",
            "parse_config = ( <class 'dict'> ) {'sep': ',', 'header': 0, 'filepath_or_buffer': '/tmp/my_local_read-xzsjxnwb/CTG.csv'}\n",
            "          FileName  ...  NSP\n",
            "0     Variab10.txt  ...  2.0\n",
            "1       Fmcs_1.txt  ...  1.0\n",
            "2       Fmcs_1.txt  ...  1.0\n",
            "3       Fmcs_1.txt  ...  1.0\n",
            "4       Fmcs_1.txt  ...  1.0\n",
            "...            ...  ...  ...\n",
            "2124  S8001045.dsp  ...  2.0\n",
            "2125  S8001045.dsp  ...  1.0\n",
            "2126           NaN  ...  NaN\n",
            "2127           NaN  ...  NaN\n",
            "2128           NaN  ...  NaN\n",
            "\n",
            "[2129 rows x 40 columns]\n",
            "pycaret version =  2.3.10\n",
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n",
            "setup_config = ( <class 'dict'> ) {'ignore_low_variance': True, 'remove_outliers': True, 'remove_multicollinearity': True, 'multicollinearity_threshold': 0.7, 'target': 'NSP', 'data':           FileName       Date      SegFile       b       e    LBE     LB   AC  \\\n",
            "0     Variab10.txt  12/1/1996  CTG0001.txt   240.0   357.0  120.0  120.0  0.0   \n",
            "1       Fmcs_1.txt   5/3/1996  CTG0002.txt     5.0   632.0  132.0  132.0  4.0   \n",
            "2       Fmcs_1.txt   5/3/1996  CTG0003.txt   177.0   779.0  133.0  133.0  2.0   \n",
            "3       Fmcs_1.txt   5/3/1996  CTG0004.txt   411.0  1192.0  134.0  134.0  2.0   \n",
            "4       Fmcs_1.txt   5/3/1996  CTG0005.txt   533.0  1147.0  132.0  132.0  4.0   \n",
            "...            ...        ...          ...     ...     ...    ...    ...  ...   \n",
            "2124  S8001045.dsp   6/6/1998  CTG2127.txt  1576.0  3049.0  140.0  140.0  1.0   \n",
            "2125  S8001045.dsp   6/6/1998  CTG2128.txt  2796.0  3415.0  142.0  142.0  1.0   \n",
            "2126           NaN        NaN          NaN     NaN     NaN    NaN    NaN  NaN   \n",
            "2127           NaN        NaN          NaN     NaN     NaN    NaN    NaN  NaN   \n",
            "2128           NaN        NaN          NaN     NaN     NaN    NaN    NaN  NaN   \n",
            "\n",
            "         FM    UC  ASTV  MSTV  ALTV  MLTV    DL   DS   DP   DR  Width    Min  \\\n",
            "0       0.0   0.0  73.0   0.5  43.0   2.4   0.0  0.0  0.0  0.0   64.0   62.0   \n",
            "1       0.0   4.0  17.0   2.1   0.0  10.4   2.0  0.0  0.0  0.0  130.0   68.0   \n",
            "2       0.0   5.0  16.0   2.1   0.0  13.4   2.0  0.0  0.0  0.0  130.0   68.0   \n",
            "3       0.0   6.0  16.0   2.4   0.0  23.0   2.0  0.0  0.0  0.0  117.0   53.0   \n",
            "4       0.0   5.0  16.0   2.4   0.0  19.9   0.0  0.0  0.0  0.0  117.0   53.0   \n",
            "...     ...   ...   ...   ...   ...   ...   ...  ...  ...  ...    ...    ...   \n",
            "2124    0.0   9.0  78.0   0.4  27.0   7.0   0.0  0.0  0.0  0.0   66.0  103.0   \n",
            "2125    1.0   5.0  74.0   0.4  36.0   5.0   0.0  0.0  0.0  0.0   42.0  117.0   \n",
            "2126    NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  NaN  NaN    NaN    NaN   \n",
            "2127    NaN   NaN   NaN   NaN   NaN   NaN   0.0  0.0  0.0  0.0    NaN    NaN   \n",
            "2128  564.0  23.0  87.0   7.0  91.0  50.7  16.0  1.0  4.0  0.0    NaN    NaN   \n",
            "\n",
            "        Max  Nmax  Nzeros   Mode   Mean  Median  Variance  Tendency    A    B  \\\n",
            "0     126.0   2.0     0.0  120.0  137.0   121.0      73.0       1.0  0.0  0.0   \n",
            "1     198.0   6.0     1.0  141.0  136.0   140.0      12.0       0.0  0.0  0.0   \n",
            "2     198.0   5.0     1.0  141.0  135.0   138.0      13.0       0.0  0.0  0.0   \n",
            "3     170.0  11.0     0.0  137.0  134.0   137.0      13.0       1.0  0.0  0.0   \n",
            "4     170.0   9.0     0.0  137.0  136.0   138.0      11.0       1.0  0.0  1.0   \n",
            "...     ...   ...     ...    ...    ...     ...       ...       ...  ...  ...   \n",
            "2124  169.0   6.0     0.0  152.0  147.0   151.0       4.0       1.0  0.0  0.0   \n",
            "2125  159.0   2.0     1.0  145.0  143.0   145.0       1.0       0.0  1.0  0.0   \n",
            "2126    NaN   NaN     NaN    NaN    NaN     NaN       NaN       NaN  NaN  NaN   \n",
            "2127    NaN   NaN     NaN    NaN    NaN     NaN       NaN       NaN  NaN  NaN   \n",
            "2128    NaN   NaN     NaN    NaN    NaN     NaN       NaN       NaN  NaN  NaN   \n",
            "\n",
            "        C    D    E   AD   DE   LD   FS  SUSP  CLASS  NSP  \n",
            "0     0.0  0.0  0.0  0.0  0.0  0.0  1.0   0.0    9.0  2.0  \n",
            "1     0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
            "2     0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
            "3     0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
            "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    2.0  1.0  \n",
            "...   ...  ...  ...  ...  ...  ...  ...   ...    ...  ...  \n",
            "2124  0.0  0.0  1.0  0.0  0.0  0.0  0.0   0.0    5.0  2.0  \n",
            "2125  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    1.0  1.0  \n",
            "2126  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
            "2127  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
            "2128  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
            "\n",
            "[2129 rows x 40 columns], 'log_experiment': False, 'data_split_shuffle': False, 'html': False, 'silent': True}\n",
            "Setup Succesfully Completed!\n",
            "                               Description             Value\n",
            "0                               session_id              2660\n",
            "1                                   Target               NSP\n",
            "2                              Target Type        Multiclass\n",
            "3                            Label Encoded              None\n",
            "4                            Original Data        (2129, 40)\n",
            "5                           Missing Values              True\n",
            "6                         Numeric Features                25\n",
            "7                     Categorical Features                13\n",
            "8                         Ordinal Features             False\n",
            "9                High Cardinality Features             False\n",
            "10                 High Cardinality Method              None\n",
            "11                   Transformed Train Set        (1415, 86)\n",
            "12                    Transformed Test Set         (636, 86)\n",
            "13                      Shuffle Train-Test             False\n",
            "14                     Stratify Train-Test             False\n",
            "15                          Fold Generator   StratifiedKFold\n",
            "16                             Fold Number                10\n",
            "17                                CPU Jobs                -1\n",
            "18                                 Use GPU             False\n",
            "19                          Log Experiment             False\n",
            "20                         Experiment Name  clf-default-name\n",
            "21                                     USI              c231\n",
            "22                         Imputation Type            simple\n",
            "23          Iterative Imputation Iteration              None\n",
            "24                         Numeric Imputer              mean\n",
            "25      Iterative Imputation Numeric Model              None\n",
            "26                     Categorical Imputer          constant\n",
            "27  Iterative Imputation Categorical Model              None\n",
            "28           Unknown Categoricals Handling    least_frequent\n",
            "29                               Normalize             False\n",
            "30                        Normalize Method              None\n",
            "31                          Transformation             False\n",
            "32                   Transformation Method              None\n",
            "33                                     PCA             False\n",
            "34                              PCA Method              None\n",
            "35                          PCA Components              None\n",
            "36                     Ignore Low Variance              True\n",
            "37                     Combine Rare Levels             False\n",
            "38                    Rare Level Threshold              None\n",
            "39                         Numeric Binning             False\n",
            "40                         Remove Outliers              True\n",
            "41                      Outliers Threshold              0.05\n",
            "42                Remove Multicollinearity              True\n",
            "43             Multicollinearity Threshold               0.7\n",
            "44             Remove Perfect Collinearity              True\n",
            "45                              Clustering             False\n",
            "46                    Clustering Iteration              None\n",
            "47                     Polynomial Features             False\n",
            "48                       Polynomial Degree              None\n",
            "49                    Trignometry Features             False\n",
            "50                    Polynomial Threshold              None\n",
            "51                          Group Features             False\n",
            "52                       Feature Selection             False\n",
            "53                Feature Selection Method           classic\n",
            "54            Features Selection Threshold              None\n",
            "55                     Feature Interaction             False\n",
            "56                           Feature Ratio             False\n",
            "57                   Interaction Threshold              None\n",
            "58                           Fix Imbalance             False\n",
            "59                    Fix Imbalance Method             SMOTE\n",
            "====== PREPARED DATA ====\n",
            "           e  ...  NSP\n",
            "0      357.0  ...  2.0\n",
            "1      632.0  ...  1.0\n",
            "2      779.0  ...  1.0\n",
            "3     1192.0  ...  1.0\n",
            "4     1147.0  ...  1.0\n",
            "...      ...  ...  ...\n",
            "2121  2867.0  ...  2.0\n",
            "2122  2867.0  ...  2.0\n",
            "2123  2596.0  ...  2.0\n",
            "2124  3049.0  ...  2.0\n",
            "2125  3415.0  ...  1.0\n",
            "\n",
            "[2126 rows x 87 columns]\n",
            "=========================\n",
            "['2022-06-22 12', '48', '46,866', 'INFO', 'PyCaret Supervised Module']\n",
            "['2022-06-22 12', '48', '46,866', 'INFO', 'ML Usecase', 'classification']\n",
            "['2022-06-22 12', '48', '46,866', 'INFO', 'version 2.3.10']\n",
            "['2022-06-22 12', '48', '46,866', 'INFO', 'Initializing setup()']\n",
            "['2022-06-22 12', '48', '46,866', 'INFO', \"setup(target=NSP, ml_usecase=classification, available_plots={'parameter'\", \"'Hyperparameters', 'auc'\", \"'AUC', 'confusion_matrix'\", \"'Confusion Matrix', 'threshold'\", \"'Threshold', 'pr'\", \"'Precision Recall', 'error'\", \"'Prediction Error', 'class_report'\", \"'Class Report', 'rfe'\", \"'Feature Selection', 'learning'\", \"'Learning Curve', 'manifold'\", \"'Manifold Learning', 'calibration'\", \"'Calibration Curve', 'vc'\", \"'Validation Curve', 'dimension'\", \"'Dimensions', 'feature'\", \"'Feature Importance', 'feature_all'\", \"'Feature Importance (All)', 'boundary'\", \"'Decision Boundary', 'lift'\", \"'Lift Chart', 'gain'\", \"'Gain Chart', 'tree'\", \"'Decision Tree', 'ks'\", \"'KS Statistic Plot'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=constant, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=None, numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=None, normalize=False, normalize_method=zscore, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=True, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=True, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=True, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=stratifiedkfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=False, session_id=None, log_experiment=False, experiment_name=None, experiment_custom_tags=None, log_plots=False, log_profile=False, log_data=False, silent=True, verbose=True, profile=False, profile_kwargs=None, display=None)\"]\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'Checking environment']\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'python_version', '3.7.13']\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'python_build', \"('default', 'Apr 24 2022 01\", '04', \"09')\"]\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'machine', 'x86_64']\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'platform', 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic']\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'Memory', 'svmem(total=13617745920, available=12175511552, percent=10.6, used=1224278016, free=7739871232, active=1658605568, inactive=3863793664, buffers=169963520, cached=4483633152, shared=1228800, slab=266518528)']\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'Physical Core', '1']\n",
            "['2022-06-22 12', '48', '46,867', 'INFO', 'Logical Core', '2']\n",
            "['2022-06-22 12', '48', '46,868', 'INFO', 'Checking libraries']\n",
            "['2022-06-22 12', '48', '46,868', 'INFO', 'pd==1.3.5']\n",
            "['2022-06-22 12', '48', '46,868', 'INFO', 'numpy==1.19.5']\n",
            "['2022-06-22 12', '48', '46,868', 'INFO', 'sklearn==0.23.2']\n",
            "['2022-06-22 12', '48', '46,868', 'INFO', 'lightgbm==3.3.2']\n",
            "['2022-06-22 12', '48', '46,868', 'WARNING', 'catboost not found']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'xgboost==0.90']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'mlflow==1.26.1']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'Checking Exceptions']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'Declaring global variables']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'USI', 'c231']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'pycaret_globals', \"{'imputation_regressor', 'X_test', 'html_param', 'X_train', '_all_metrics', 'fix_imbalance_param', 'USI', 'iterative_imputation_iters_param', 'transform_target_method_param', '_internal_pipeline', 'target_param', 'y_train', '_all_models_internal', 'experiment__', 'fold_groups_param', 'y', 'n_jobs_param', 'imputation_classifier', 'dashboard_logger', 'transform_target_param', 'data_before_preprocess', 'fix_imbalance_method_param', 'y_test', 'create_model_container', 'display_container', 'stratify_param', 'prep_pipe', '_all_models', 'fold_param', 'master_model_container', 'X', '_available_plots', 'fold_generator', 'logging_param', 'pycaret_globals', '_ml_usecase', 'log_plots_param', 'seed', 'fold_shuffle_param', 'gpu_param', 'fold_groups_param_full', 'exp_name_log', '_gpu_n_jobs_param'}\"]\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'Importing libraries']\n",
            "['2022-06-22 12', '48', '46,915', 'INFO', 'Copying data for preprocessing']\n",
            "['2022-06-22 12', '48', '46,916', 'INFO', 'Declaring preprocessing parameters']\n",
            "['2022-06-22 12', '48', '46,919', 'INFO', 'Creating preprocessing pipeline']\n",
            "['2022-06-22 12', '48', '46,932', 'INFO', 'Preprocessing pipeline created successfully']\n",
            "['2022-06-22 12', '48', '46,932', 'ERROR', '(Process Exit)', \"setup has been interupted with user command 'quit'. setup must rerun.\"]\n",
            "['2022-06-22 12', '48', '46,932', 'INFO', 'Creating global containers']\n",
            "['2022-06-22 12', '48', '46,933', 'INFO', 'Internal pipeline', \"Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\"]\n",
            "['2022-06-22 12', '50', '45,890', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-22 12', '50', '45,891', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-22 12', '50', '45,980', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-22 12', '50', '45,981', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-22 12', '50', '45,981', 'INFO', 'Creating grid variables']\n",
            "['2022-06-22 12', '50', '46,045', 'INFO', 'create_model_container', '0']\n",
            "['2022-06-22 12', '50', '46,045', 'INFO', 'master_model_container', '0']\n",
            "['2022-06-22 12', '50', '46,045', 'INFO', 'display_container', '1']\n",
            "['2022-06-22 12', '50', '46,051', 'INFO', 'Pipeline(memory=None,']\n",
            "[\"steps=[('dtypes',\"]\n",
            "['DataTypes_Auto_infer(categorical_features=[],']\n",
            "['display_types=False, features_todrop=[],']\n",
            "['id_columns=[],']\n",
            "[\"ml_usecase='classification',\"]\n",
            "[\"numerical_features=[], target='NSP',\"]\n",
            "['time_features=[])),']\n",
            "[\"('imputer',\"]\n",
            "[\"Simple_Imputer(categorical_strategy='not_available',\"]\n",
            "['fill_value_categorical=None,']\n",
            "['fill_value_numerical=None,']\n",
            "['numeric_strateg...']\n",
            "[\"('dummy', Dummify(target='NSP')),\"]\n",
            "[\"('fix_perfect', Remove_100(target='NSP')),\"]\n",
            "[\"('clean_names', Clean_Colum_Names()),\"]\n",
            "[\"('feature_select', 'passthrough'),\"]\n",
            "[\"('fix_multi',\"]\n",
            "['Fix_multicollinearity(correlation_with_target_preference=None,']\n",
            "['correlation_with_target_threshold=0.0,']\n",
            "[\"target_variable='NSP', threshold=0.7)),\"]\n",
            "[\"('dfs', 'passthrough'), ('pca', 'passthrough')],\"]\n",
            "['verbose=False)']\n",
            "['2022-06-22 12', '50', '46,052', 'INFO', 'setup() succesfully completed......................................']\n",
            "['2022-06-22 12', '50', '46,142', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-22 12', '50', '46,142', 'INFO', 'get_config(variable=X)']\n",
            "['2022-06-22 12', '50', '46,257', 'INFO', 'Global variable', 'X returned as            e  ...  Date_is_month_start_0']\n",
            "['0      357.0  ...                    0.0']\n",
            "['1      632.0  ...                    1.0']\n",
            "['2      779.0  ...                    1.0']\n",
            "['3     1192.0  ...                    1.0']\n",
            "['4     1147.0  ...                    1.0']\n",
            "['...      ...  ...                    ...']\n",
            "['2121  2867.0  ...                    1.0']\n",
            "['2122  2867.0  ...                    1.0']\n",
            "['2123  2596.0  ...                    1.0']\n",
            "['2124  3049.0  ...                    1.0']\n",
            "['2125  3415.0  ...                    1.0']\n",
            "['[2126 rows x 86 columns]']\n",
            "['2022-06-22 12', '50', '46,257', 'INFO', 'get_config() succesfully completed......................................']\n",
            "['2022-06-22 12', '50', '46,257', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-22 12', '50', '46,257', 'INFO', 'get_config(variable=y)']\n",
            "['2022-06-22 12', '50', '46,258', 'INFO', 'Global variable', 'y returned as 0       2.0']\n",
            "['1       1.0']\n",
            "['2       1.0']\n",
            "['3       1.0']\n",
            "['4       1.0']\n",
            "['...']\n",
            "['2121    2.0']\n",
            "['2122    2.0']\n",
            "['2123    2.0']\n",
            "['2124    2.0']\n",
            "['2125    1.0']\n",
            "['Name', 'NSP, Length', '2126, dtype', 'float32']\n",
            "['2022-06-22 12', '50', '46,258', 'INFO', 'get_config() succesfully completed......................................']\n",
            "to_csv_config = ( <class 'dict'> ) {'index': False, 'path_or_buf': '/tmp/my_local_dir_for_test/CTG_data-prep.csv'}\n",
            "rclone_config = ( <class 'dict'> ) {'RCLONE_CONFIG_REMOTEREAD_TYPE': 's3', 'RCLONE_CONFIG_REMOTEREAD_PROVIDER': 'AWS', 'RCLONE_CONFIG_REMOTEREAD_REGION': 'us-west-2'}\n",
            "local_datastore_read_dir: /tmp/my_local_read-bgxyd30i\n",
            "rclone -v mount remoteread:humor-detection-pds /tmp/my_local_read-bgxyd30i --daemon\n",
            "2022/06/22 12:50:47 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "\n",
            "parse_config = ( <class 'dict'> ) {'filepath_or_buffer': '/tmp/my_local_read-bgxyd30i/Non-humours-biased.csv'}\n",
            "                                               question  ... label\n",
            "0              What is the size range of these patterns  ...     0\n",
            "1               Are the instructions in uk or us terms?  ...     0\n",
            "2                 Is this manual good for 2014 model c3  ...     0\n",
            "3      Hello does the light have adjustable brightness?  ...     0\n",
            "4      Will these inks work with the wf-7720dtwf model?  ...     0\n",
            "...                                                 ...  ...   ...\n",
            "9566  Does the 68lm 6 inch give an audible alarm if ...  ...     0\n",
            "9567  Do you need 2 phone sockets / connection boxes...  ...     0\n",
            "9568  I buy it on 05.11.2015 and today my usb device...  ...     0\n",
            "9569  Hi What is the warranty period on this? I noti...  ...     0\n",
            "9570                      can you use proper golf balls  ...     0\n",
            "\n",
            "[9571 rows x 4 columns]\n",
            "pycaret version =  2.3.10\n",
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n",
            "setup_config = ( <class 'dict'> ) {'preprocess': False, 'ignore_features': ['image_url'], 'target': 'label', 'data':                                                question  \\\n",
            "0              What is the size range of these patterns   \n",
            "1               Are the instructions in uk or us terms?   \n",
            "2                 Is this manual good for 2014 model c3   \n",
            "3      Hello does the light have adjustable brightness?   \n",
            "4      Will these inks work with the wf-7720dtwf model?   \n",
            "...                                                 ...   \n",
            "9566  Does the 68lm 6 inch give an audible alarm if ...   \n",
            "9567  Do you need 2 phone sockets / connection boxes...   \n",
            "9568  I buy it on 05.11.2015 and today my usb device...   \n",
            "9569  Hi What is the warranty period on this? I noti...   \n",
            "9570                      can you use proper golf balls   \n",
            "\n",
            "                                    product_description  \\\n",
            "0     The Colette Sewing Handbook: 5 Fundamentals fo...   \n",
            "1                           Absolutely Gorgeous Doilies   \n",
            "2     Citroen C3 Petrol & Diesel Service and Repair ...   \n",
            "3          Mighty Bright Blue Xtraflex 2 LED Book Light   \n",
            "4     24 High Capacity ink cartridge to Replace 27XX...   \n",
            "...                                                 ...   \n",
            "9566  Garmin Nuvi 68LM 6 inch Satellite Navigation w...   \n",
            "9567  iDECT Eclipse Plus Dect Phone with Call Blocke...   \n",
            "9568  SanDisk SDCZ43-128G-G46 Ultra Fit 128 GB USB F...   \n",
            "9569  SanDisk SDCZ43-128G-G46 Ultra Fit 128 GB USB F...   \n",
            "9570  Haack Golf Net By SEC Coach Chris Haack by Rukket   \n",
            "\n",
            "                                              image_url  label  \n",
            "0     https://www.amazon.co.uk/ask/questions/Tx355VH...      0  \n",
            "1     https://www.amazon.co.uk/ask/questions/Tx3IRRI...      0  \n",
            "2     https://www.amazon.co.uk/ask/questions/Tx3S1FO...      0  \n",
            "3     https://www.amazon.co.uk/ask/questions/Tx38TXK...      0  \n",
            "4     https://www.amazon.co.uk/ask/questions/Tx1IVEI...      0  \n",
            "...                                                 ...    ...  \n",
            "9566  https://www.amazon.co.uk/ask/questions/Tx1IVDN...      0  \n",
            "9567  https://www.amazon.co.uk/ask/questions/TxI4UJL...      0  \n",
            "9568  https://www.amazon.co.uk/ask/questions/Tx3ORN8...      0  \n",
            "9569  https://www.amazon.co.uk/ask/questions/Tx3AM5L...      0  \n",
            "9570  https://www.amazon.co.uk/ask/questions/Tx2NNFV...      0  \n",
            "\n",
            "[9571 rows x 4 columns], 'log_experiment': False, 'data_split_shuffle': False, 'html': False, 'silent': True}\n",
            "Setup Succesfully Completed!\n",
            "              Description             Value\n",
            "0              session_id              3153\n",
            "1                  Target             label\n",
            "2             Target Type            Binary\n",
            "3           Label Encoded              None\n",
            "4           Original Data         (9571, 4)\n",
            "5          Missing Values             False\n",
            "6        Numeric Features                 0\n",
            "7    Categorical Features                 2\n",
            "8   Transformed Train Set         (6699, 2)\n",
            "9    Transformed Test Set         (2872, 2)\n",
            "10     Shuffle Train-Test             False\n",
            "11    Stratify Train-Test             False\n",
            "12         Fold Generator   StratifiedKFold\n",
            "13            Fold Number                10\n",
            "14               CPU Jobs                -1\n",
            "15                Use GPU             False\n",
            "16         Log Experiment             False\n",
            "17        Experiment Name  clf-default-name\n",
            "18                    USI              130f\n",
            "19          Fix Imbalance             False\n",
            "20   Fix Imbalance Method             SMOTE\n",
            "====== PREPARED DATA ====\n",
            "                                               question  ... label\n",
            "0              What is the size range of these patterns  ...     0\n",
            "1               Are the instructions in uk or us terms?  ...     0\n",
            "2                 Is this manual good for 2014 model c3  ...     0\n",
            "3      Hello does the light have adjustable brightness?  ...     0\n",
            "4      Will these inks work with the wf-7720dtwf model?  ...     0\n",
            "...                                                 ...  ...   ...\n",
            "9566  Does the 68lm 6 inch give an audible alarm if ...  ...     0\n",
            "9567  Do you need 2 phone sockets / connection boxes...  ...     0\n",
            "9568  I buy it on 05.11.2015 and today my usb device...  ...     0\n",
            "9569  Hi What is the warranty period on this? I noti...  ...     0\n",
            "9570                      can you use proper golf balls  ...     0\n",
            "\n",
            "[9571 rows x 3 columns]\n",
            "=========================\n",
            "['2022-06-22 12', '50', '51,018', 'INFO', 'PyCaret Supervised Module']\n",
            "['2022-06-22 12', '50', '51,018', 'INFO', 'ML Usecase', 'classification']\n",
            "['2022-06-22 12', '50', '51,018', 'INFO', 'version 2.3.10']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'Initializing setup()']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', \"setup(target=label, ml_usecase=classification, available_plots={'parameter'\", \"'Hyperparameters', 'auc'\", \"'AUC', 'confusion_matrix'\", \"'Confusion Matrix', 'threshold'\", \"'Threshold', 'pr'\", \"'Precision Recall', 'error'\", \"'Prediction Error', 'class_report'\", \"'Class Report', 'rfe'\", \"'Feature Selection', 'learning'\", \"'Learning Curve', 'manifold'\", \"'Manifold Learning', 'calibration'\", \"'Calibration Curve', 'vc'\", \"'Validation Curve', 'dimension'\", \"'Dimensions', 'feature'\", \"'Feature Importance', 'feature_all'\", \"'Feature Importance (All)', 'boundary'\", \"'Decision Boundary', 'lift'\", \"'Lift Chart', 'gain'\", \"'Gain Chart', 'tree'\", \"'Decision Tree', 'ks'\", \"'KS Statistic Plot'}, train_size=0.7, test_data=None, preprocess=False, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=constant, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=None, numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['image_url'], normalize=False, normalize_method=zscore, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=False, multicollinearity_threshold=0.9, remove_perfect_collinearity=True, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=stratifiedkfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=False, session_id=None, log_experiment=False, experiment_name=None, experiment_custom_tags=None, log_plots=False, log_profile=False, log_data=False, silent=True, verbose=True, profile=False, profile_kwargs=None, display=None)\"]\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'Checking environment']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'python_version', '3.7.13']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'python_build', \"('default', 'Apr 24 2022 01\", '04', \"09')\"]\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'machine', 'x86_64']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'platform', 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'Memory', 'svmem(total=13617745920, available=12242563072, percent=10.1, used=1165078528, free=7694204928, active=1710727168, inactive=3870076928, buffers=170311680, cached=4588150784, shared=1232896, slab=267227136)']\n",
            "['2022-06-22 12', '50', '51,019', 'INFO', 'Physical Core', '1']\n",
            "['2022-06-22 12', '50', '51,020', 'INFO', 'Logical Core', '2']\n",
            "['2022-06-22 12', '50', '51,020', 'INFO', 'Checking libraries']\n",
            "['2022-06-22 12', '50', '51,020', 'INFO', 'pd==1.3.5']\n",
            "['2022-06-22 12', '50', '51,020', 'INFO', 'numpy==1.19.5']\n",
            "['2022-06-22 12', '50', '51,020', 'INFO', 'sklearn==0.23.2']\n",
            "['2022-06-22 12', '50', '51,020', 'INFO', 'lightgbm==3.3.2']\n",
            "['2022-06-22 12', '50', '51,020', 'WARNING', 'catboost not found']\n",
            "['2022-06-22 12', '50', '51,027', 'INFO', 'xgboost==0.90']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'mlflow==1.26.1']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'Checking Exceptions']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'Declaring global variables']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'USI', '130f']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'pycaret_globals', \"{'fix_imbalance_method_param', '_all_models', '_gpu_n_jobs_param', 'imputation_regressor', 'dashboard_logger', 'master_model_container', 'X', 'fold_param', '_available_plots', 'data_before_preprocess', '_internal_pipeline', 'experiment__', 'transform_target_param', 'fold_groups_param', 'X_test', '_ml_usecase', 'prep_pipe', 'y_test', 'n_jobs_param', 'imputation_classifier', 'seed', 'transform_target_method_param', 'display_container', 'fold_groups_param_full', 'X_train', 'iterative_imputation_iters_param', 'logging_param', 'exp_name_log', 'html_param', 'stratify_param', 'USI', '_all_models_internal', 'target_param', 'pycaret_globals', 'fold_shuffle_param', 'fix_imbalance_param', 'fold_generator', 'y_train', 'gpu_param', 'y', 'log_plots_param', 'create_model_container', '_all_metrics'}\"]\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'Importing libraries']\n",
            "['2022-06-22 12', '50', '51,028', 'INFO', 'Copying data for preprocessing']\n",
            "['2022-06-22 12', '50', '51,029', 'INFO', 'Declaring preprocessing parameters']\n",
            "['2022-06-22 12', '50', '51,031', 'INFO', 'Creating preprocessing pipeline']\n",
            "['2022-06-22 12', '50', '51,038', 'INFO', 'Preprocessing pipeline created successfully']\n",
            "['2022-06-22 12', '50', '51,038', 'ERROR', '(Process Exit)', \"setup has been interupted with user command 'quit'. setup must rerun.\"]\n",
            "['2022-06-22 12', '50', '51,038', 'INFO', 'Creating global containers']\n",
            "['2022-06-22 12', '50', '51,039', 'INFO', 'Internal pipeline', \"Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\"]\n",
            "['2022-06-22 12', '50', '51,204', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-22 12', '50', '51,205', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-22 12', '50', '51,302', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-22 12', '50', '51,302', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-22 12', '50', '51,303', 'INFO', 'Creating grid variables']\n",
            "['2022-06-22 12', '50', '51,355', 'INFO', 'create_model_container', '0']\n",
            "['2022-06-22 12', '50', '51,355', 'INFO', 'master_model_container', '0']\n",
            "['2022-06-22 12', '50', '51,355', 'INFO', 'display_container', '1']\n",
            "['2022-06-22 12', '50', '51,356', 'INFO', 'Pipeline(memory=None,']\n",
            "[\"steps=[('dtypes',\"]\n",
            "['DataTypes_Auto_infer(categorical_features=[],']\n",
            "['display_types=False,']\n",
            "[\"features_todrop=['image_url'],\"]\n",
            "['id_columns=[],']\n",
            "[\"ml_usecase='classification',\"]\n",
            "[\"numerical_features=[], target='label',\"]\n",
            "['time_features=[]))],']\n",
            "['verbose=False)']\n",
            "['2022-06-22 12', '50', '51,356', 'INFO', 'setup() succesfully completed......................................']\n",
            "['2022-06-22 12', '50', '51,445', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-22 12', '50', '51,445', 'INFO', 'get_config(variable=X)']\n",
            "['2022-06-22 12', '50', '51,453', 'INFO', 'Global variable', 'X returned as                                                question                                product_description']\n",
            "['0              What is the size range of these patterns  The Colette Sewing Handbook', '5 Fundamentals fo...']\n",
            "['1               Are the instructions in uk or us terms?                        Absolutely Gorgeous Doilies']\n",
            "['2                 Is this manual good for 2014 model c3  Citroen C3 Petrol & Diesel Service and Repair ...']\n",
            "['3      Hello does the light have adjustable brightness?       Mighty Bright Blue Xtraflex 2 LED Book Light']\n",
            "['4      Will these inks work with the wf-7720dtwf model?  24 High Capacity ink cartridge to Replace 27XX...']\n",
            "['...                                                 ...                                                ...']\n",
            "['9566  Does the 68lm 6 inch give an audible alarm if ...  Garmin Nuvi 68LM 6 inch Satellite Navigation w...']\n",
            "['9567  Do you need 2 phone sockets / connection boxes...  iDECT Eclipse Plus Dect Phone with Call Blocke...']\n",
            "['9568  I buy it on 05.11.2015 and today my usb device...  SanDisk SDCZ43-128G-G46 Ultra Fit 128 GB USB F...']\n",
            "['9569  Hi What is the warranty period on this? I noti...  SanDisk SDCZ43-128G-G46 Ultra Fit 128 GB USB F...']\n",
            "['9570                      can you use proper golf balls  Haack Golf Net By SEC Coach Chris Haack by Rukket']\n",
            "['[9571 rows x 2 columns]']\n",
            "['2022-06-22 12', '50', '51,453', 'INFO', 'get_config() succesfully completed......................................']\n",
            "['2022-06-22 12', '50', '51,453', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-22 12', '50', '51,453', 'INFO', 'get_config(variable=y)']\n",
            "['2022-06-22 12', '50', '51,454', 'INFO', 'Global variable', 'y returned as 0       0']\n",
            "['1       0']\n",
            "['2       0']\n",
            "['3       0']\n",
            "['4       0']\n",
            "['..']\n",
            "['9566    0']\n",
            "['9567    0']\n",
            "['9568    0']\n",
            "['9569    0']\n",
            "['9570    0']\n",
            "['Name', 'label, Length', '9571, dtype', 'int64']\n",
            "['2022-06-22 12', '50', '51,454', 'INFO', 'get_config() succesfully completed......................................']\n",
            "to_csv_config = ( <class 'dict'> ) {'index': False, 'path_or_buf': '/tmp/my_local_dir_for_test/Non-humours-biased_data-prep.csv'}\n",
            "test-validation done successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install kfp==1.8.12"
      ],
      "metadata": {
        "id": "jhPf6ZSOnv0m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d5fe531-a3e5-4742-bea7-0bfa119c4cc3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kfp==1.8.12\n",
            "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (1.1.0)\n",
            "Requirement already satisfied: PyYAML<6,>=5.3 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (5.4.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (1.31.6)\n",
            "Collecting google-cloud-storage<2,>=1.20.0\n",
            "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 54.4 MB/s \n",
            "\u001b[?25hCollecting kubernetes<19,>=8.0.0\n",
            "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (1.12.11)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (1.35.0)\n",
            "Collecting requests-toolbelt<1,>=0.8.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
            "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (0.8.9)\n",
            "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (7.1.2)\n",
            "Collecting Deprecated<2,>=1.2.7\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting strip-hints<1,>=0.1.8\n",
            "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
            "Collecting docstring-parser<1,>=0.7.3\n",
            "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
            "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
            "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
            "Collecting fire<1,>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (3.17.3)\n",
            "Requirement already satisfied: uritemplate<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (3.0.1)\n",
            "Requirement already satisfied: pydantic<2,>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (1.8.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from kfp==1.8.12) (0.4.1)\n",
            "Collecting typing-extensions<4,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12) (1.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire<1,>=0.3.1->kfp==1.8.12) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<1,>=0.3.1->kfp==1.8.12) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (1.56.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (2022.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (2.28.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (21.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.12) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.12) (0.0.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.2.4)\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12) (4.11.4)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12) (0.18.1)\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.12) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (2.0.12)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12) (0.37.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.8.12) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12) (3.2.0)\n",
            "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=c0ec05eb5a064e69f2643f278ba0fb531e96204134bfd191b9b55dc1739a7a52\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=45ba519b13ece37936cbb7a02d52e24be29429214d38983a490d59d58aad195a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=1be290849d974f7bd1bbad583ebd04506e3862a8edd71b09de602e5147ef5e79\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
            "  Building wheel for strip-hints (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=589dfcf45cf061132a595513a67a90a0a3af8e5c85ac4d2fbcae256b09de171b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
            "Successfully built kfp fire kfp-server-api strip-hints\n",
            "Installing collected packages: typing-extensions, google-crc32c, google-resumable-media, google-cloud-core, strip-hints, requests-toolbelt, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, google-cloud-storage, fire, docstring-parser, Deprecated, cloudpickle, kfp\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "nbclient 0.6.4 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "nbclient 0.6.4 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.3.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.13 cloudpickle-2.1.0 docstring-parser-0.14.1 fire-0.4.0 google-cloud-core-2.3.1 google-cloud-storage-1.44.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 jsonschema-3.2.0 kfp-1.8.12 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.2 kubernetes-18.20.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First validate the component.yaml file in http://www.yamllint.com/. Once component.yaml file is corrected, execute the below cell to finally check"
      ],
      "metadata": {
        "id": "tQalOIRBDzSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "\n",
        "csv_data_prepare_op_out_to_artifact = kfp.components.load_component_from_file('component_input_using_rclone_output_as_artifact.yaml')\n",
        "csv_data_prepare_op_both_in_out_artifact = kfp.components.load_component_from_file('component_both_input_output_as_artifact.yaml')\n",
        "\n",
        "@kfp.dsl.pipeline(name=\"mysamplepipeline\")\n",
        "def my_sample_pipeline():\n",
        "    csv_prepared_file_step1 = csv_data_prepare_op_out_to_artifact(rclone_environment_var='{\"RCLONE_CONFIG_REMOTEREAD_TYPE\":\"http\", \"RCLONE_CONFIG_REMOTEREAD_URL\":\"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/\"}',\n",
        "                                            input_datasource_file_name='CTG.csv',\n",
        "                                            type_of_data_analysis_task='classification',\n",
        "                                            target_variable_name='NSP',\n",
        "                                            data_preparations_options='{\"ignore_low_variance\":true, \"remove_outliers\":true}', \n",
        "                                            additional_options_csv_writing='{\"index\":false}'\n",
        "                                            ).outputs['output_datasource_local_file_path_when_rclone_bypassed']\n",
        "\n",
        "    csv_data_prepare_op_both_in_out_artifact(input_datasource_local_file_path_when_rclone_bypassed = csv_prepared_file_step1,\n",
        "                                            type_of_data_analysis_task='classification',\n",
        "                                            target_variable_name='NSP',\n",
        "                                            data_preparations_options='{\"remove_multicollinearity\":true, \"multicollinearity_threshold\":0.7}', \n",
        "                                            additional_options_csv_writing='{\"index\":false}'\n",
        "                                            )\n",
        "\n",
        "\n",
        "kfp.compiler.Compiler().compile(\n",
        "    pipeline_func=my_sample_pipeline,\n",
        "    package_path='my_sample_pipeline.yaml')\n"
      ],
      "metadata": {
        "id": "54vGWKvUDu8E"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfp.v2.compiler.Compiler().compile(\n",
        "    pipeline_func=my_sample_pipeline,\n",
        "    package_path='my_sample_pipeline_v2.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I1kb0j45dBc",
        "outputId": "8aa69a02-56c9-4299-9ea6-488895ba45d3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
            "  category=FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Push the code to github"
      ],
      "metadata": {
        "id": "a62pERh-Lasc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before commiting code to github, install github client (gh) by following instruction mentioned in https://github.com/cli/cli/blob/trunk/docs/install_linux.md (Choose Debian, Ubuntu Linux way of installation) \n",
        "\n",
        "Use the colab's 'Terminal' icon present in left vertical pane to open linux terminal to type commands. Once 'gh' is installed, type **$gh auth login** (refer https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git) to follow onscreen prompts. For colab, use **Paste an authentication token** option. Personal tokens can be generated in https://github.com/settings/tokens\n",
        "\n",
        "You can use Shift+Ctrl+v shortcut to paste any string in colab console"
      ],
      "metadata": {
        "id": "2Nix3R2joPdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G12wQfVT1ann",
        "outputId": "a742fb88-21c4-4e97-c461-086391ea93a8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -Rf kfpcomponent"
      ],
      "metadata": {
        "id": "JVc_RmC9MrgG"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShaswataJash/kfpcomponent.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5LCKX0SXUxs",
        "outputId": "c7b16460-9d2a-4463-8257-8aa17543a68a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kfpcomponent'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/209)\u001b[K\rremote: Counting objects:   1% (3/209)\u001b[K\rremote: Counting objects:   2% (5/209)\u001b[K\rremote: Counting objects:   3% (7/209)\u001b[K\rremote: Counting objects:   4% (9/209)\u001b[K\rremote: Counting objects:   5% (11/209)\u001b[K\rremote: Counting objects:   6% (13/209)\u001b[K\rremote: Counting objects:   7% (15/209)\u001b[K\rremote: Counting objects:   8% (17/209)\u001b[K\rremote: Counting objects:   9% (19/209)\u001b[K\rremote: Counting objects:  10% (21/209)\u001b[K\rremote: Counting objects:  11% (23/209)\u001b[K\rremote: Counting objects:  12% (26/209)\u001b[K\rremote: Counting objects:  13% (28/209)\u001b[K\rremote: Counting objects:  14% (30/209)\u001b[K\rremote: Counting objects:  15% (32/209)\u001b[K\rremote: Counting objects:  16% (34/209)\u001b[K\rremote: Counting objects:  17% (36/209)\u001b[K\rremote: Counting objects:  18% (38/209)\u001b[K\rremote: Counting objects:  19% (40/209)\u001b[K\rremote: Counting objects:  20% (42/209)\u001b[K\rremote: Counting objects:  21% (44/209)\u001b[K\rremote: Counting objects:  22% (46/209)\u001b[K\rremote: Counting objects:  23% (49/209)\u001b[K\rremote: Counting objects:  24% (51/209)\u001b[K\rremote: Counting objects:  25% (53/209)\u001b[K\rremote: Counting objects:  26% (55/209)\u001b[K\rremote: Counting objects:  27% (57/209)\u001b[K\rremote: Counting objects:  28% (59/209)\u001b[K\rremote: Counting objects:  29% (61/209)\u001b[K\rremote: Counting objects:  30% (63/209)\u001b[K\rremote: Counting objects:  31% (65/209)\u001b[K\rremote: Counting objects:  32% (67/209)\u001b[K\rremote: Counting objects:  33% (69/209)\u001b[K\rremote: Counting objects:  34% (72/209)\u001b[K\rremote: Counting objects:  35% (74/209)\u001b[K\rremote: Counting objects:  36% (76/209)\u001b[K\rremote: Counting objects:  37% (78/209)\u001b[K\rremote: Counting objects:  38% (80/209)\u001b[K\rremote: Counting objects:  39% (82/209)\u001b[K\rremote: Counting objects:  40% (84/209)\u001b[K\rremote: Counting objects:  41% (86/209)\u001b[K\rremote: Counting objects:  42% (88/209)\u001b[K\rremote: Counting objects:  43% (90/209)\u001b[K\rremote: Counting objects:  44% (92/209)\u001b[K\rremote: Counting objects:  45% (95/209)\u001b[K\rremote: Counting objects:  46% (97/209)\u001b[K\rremote: Counting objects:  47% (99/209)\u001b[K\rremote: Counting objects:  48% (101/209)\u001b[K\rremote: Counting objects:  49% (103/209)\u001b[K\rremote: Counting objects:  50% (105/209)\u001b[K\rremote: Counting objects:  51% (107/209)\u001b[K\rremote: Counting objects:  52% (109/209)\u001b[K\rremote: Counting objects:  53% (111/209)\u001b[K\rremote: Counting objects:  54% (113/209)\u001b[K\rremote: Counting objects:  55% (115/209)\u001b[K\rremote: Counting objects:  56% (118/209)\u001b[K\rremote: Counting objects:  57% (120/209)\u001b[K\rremote: Counting objects:  58% (122/209)\u001b[K\rremote: Counting objects:  59% (124/209)\u001b[K\rremote: Counting objects:  60% (126/209)\u001b[K\rremote: Counting objects:  61% (128/209)\u001b[K\rremote: Counting objects:  62% (130/209)\u001b[K\rremote: Counting objects:  63% (132/209)\u001b[K\rremote: Counting objects:  64% (134/209)\u001b[K\rremote: Counting objects:  65% (136/209)\u001b[K\rremote: Counting objects:  66% (138/209)\u001b[K\rremote: Counting objects:  67% (141/209)\u001b[K\rremote: Counting objects:  68% (143/209)\u001b[K\rremote: Counting objects:  69% (145/209)\u001b[K\rremote: Counting objects:  70% (147/209)\u001b[K\rremote: Counting objects:  71% (149/209)\u001b[K\rremote: Counting objects:  72% (151/209)\u001b[K\rremote: Counting objects:  73% (153/209)\u001b[K\rremote: Counting objects:  74% (155/209)\u001b[K\rremote: Counting objects:  75% (157/209)\u001b[K\rremote: Counting objects:  76% (159/209)\u001b[K\rremote: Counting objects:  77% (161/209)\u001b[K\rremote: Counting objects:  78% (164/209)\u001b[K\rremote: Counting objects:  79% (166/209)\u001b[K\rremote: Counting objects:  80% (168/209)\u001b[K\rremote: Counting objects:  81% (170/209)\u001b[K\rremote: Counting objects:  82% (172/209)\u001b[K\rremote: Counting objects:  83% (174/209)\u001b[K\rremote: Counting objects:  84% (176/209)\u001b[K\rremote: Counting objects:  85% (178/209)\u001b[K\rremote: Counting objects:  86% (180/209)\u001b[K\rremote: Counting objects:  87% (182/209)\u001b[K\rremote: Counting objects:  88% (184/209)\u001b[K\rremote: Counting objects:  89% (187/209)\u001b[K\rremote: Counting objects:  90% (189/209)\u001b[K\rremote: Counting objects:  91% (191/209)\u001b[K\rremote: Counting objects:  92% (193/209)\u001b[K\rremote: Counting objects:  93% (195/209)\u001b[K\rremote: Counting objects:  94% (197/209)\u001b[K\rremote: Counting objects:  95% (199/209)\u001b[K\rremote: Counting objects:  96% (201/209)\u001b[K\rremote: Counting objects:  97% (203/209)\u001b[K\rremote: Counting objects:  98% (205/209)\u001b[K\rremote: Counting objects:  99% (207/209)\u001b[K\rremote: Counting objects: 100% (209/209)\u001b[K\rremote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 209 (delta 115), reused 131 (delta 59), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (209/209), 99.76 KiB | 4.53 MiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow directory structure according to https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#organizing-the-component-files"
      ],
      "metadata": {
        "id": "AIwZrmfnCXPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret\n",
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret/src\n",
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret/tests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWT-29j1XYhi",
        "outputId": "d9b72d46-9433-45c0-efec-89a6e87c2277"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘kfpcomponent/TabularDataPreparationUsingPycaret’: File exists\n",
            "mkdir: cannot create directory ‘kfpcomponent/TabularDataPreparationUsingPycaret/src’: File exists\n",
            "mkdir: cannot create directory ‘kfpcomponent/TabularDataPreparationUsingPycaret/tests’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#it will ensure file is coped in git repo only if file content is changed by checking checksum of file content\n",
        "!rsync -c data_preparation.py kfpcomponent/TabularDataPreparationUsingPycaret/src\n",
        "!rsync -c component_both_input_output_as_artifact.yaml kfpcomponent/TabularDataPreparationUsingPycaret/component_both_input_output_as_artifact.yaml\n",
        "!rsync -c component_both_input_output_using_rclone.yaml kfpcomponent/TabularDataPreparationUsingPycaret/component_both_input_output_using_rclone.yaml\n",
        "!rsync -c component_input_as_artifact_output_using_rclone.yaml kfpcomponent/TabularDataPreparationUsingPycaret/component_input_as_artifact_output_using_rclone.yaml\n",
        "!rsync -c component_input_using_rclone_output_as_artifact.yaml kfpcomponent/TabularDataPreparationUsingPycaret/component_input_using_rclone_output_as_artifact.yaml\n",
        "!rsync -c test_validation.py kfpcomponent/TabularDataPreparationUsingPycaret/tests\n",
        "!rsync -c Dockerfile kfpcomponent/TabularDataPreparationUsingPycaret/\n",
        "!rsync -c run_tests.sh kfpcomponent/TabularDataPreparationUsingPycaret/\n",
        "!rsync -c docker-compose.test.yml kfpcomponent/TabularDataPreparationUsingPycaret/"
      ],
      "metadata": {
        "id": "n3b7gcGhXde6"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd kfpcomponent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RUDYMnmXlIW",
        "outputId": "b102768a-6fc0-4840-fdd9-a992899bbb10"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kfpcomponent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add -A"
      ],
      "metadata": {
        "id": "iAyaTjClX9S0"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW4fzKBsYCGO",
        "outputId": "8921b1c2-9846-48e8-a325-35fe92eb3be4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mmodified:   TabularDataPreparationUsingPycaret/component_both_input_output_as_artifact.yaml\u001b[m\n",
            "\t\u001b[32mmodified:   TabularDataPreparationUsingPycaret/component_both_input_output_using_rclone.yaml\u001b[m\n",
            "\t\u001b[32mmodified:   TabularDataPreparationUsingPycaret/component_input_as_artifact_output_using_rclone.yaml\u001b[m\n",
            "\t\u001b[32mmodified:   TabularDataPreparationUsingPycaret/component_input_using_rclone_output_as_artifact.yaml\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For git-user who has set their email visibility as private, git provides alternate email address to use in web-based Git operations, e.g., edits and merges. The alias email can be viewed in https://github.com/settings/emails"
      ],
      "metadata": {
        "id": "bmHCKK7CzXNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"29448766+ShaswataJash@users.noreply.github.com\""
      ],
      "metadata": {
        "id": "FBLk0UVR04lj"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -a -m \"corrected docker url\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbaNqafyYHV7",
        "outputId": "1defe470-1522-4132-ef57-2f2b113713c1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 7fdb8cd] corrected docker url\n",
            " 4 files changed, 4 insertions(+), 4 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLZfkrleaqQ3",
        "outputId": "3ef80855-a864-4476-a7a4-1eddde66757f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 7, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  14% (1/7)   \rCompressing objects:  28% (2/7)   \rCompressing objects:  42% (3/7)   \rCompressing objects:  57% (4/7)   \rCompressing objects:  71% (5/7)   \rCompressing objects:  85% (6/7)   \rCompressing objects: 100% (7/7)   \rCompressing objects: 100% (7/7), done.\n",
            "Writing objects:  14% (1/7)   \rWriting objects:  28% (2/7)   \rWriting objects:  42% (3/7)   \rWriting objects:  57% (4/7)   \rWriting objects:  71% (5/7)   \rWriting objects: 100% (7/7)   \rWriting objects: 100% (7/7), 1.58 KiB | 1.58 MiB/s, done.\n",
            "Total 7 (delta 6), reused 0 (delta 0)\n",
            "remote: Resolving deltas:   0% (0/6)\u001b[K\rremote: Resolving deltas:  16% (1/6)\u001b[K\rremote: Resolving deltas:  33% (2/6)\u001b[K\rremote: Resolving deltas:  50% (3/6)\u001b[K\rremote: Resolving deltas:  66% (4/6)\u001b[K\rremote: Resolving deltas:  83% (5/6)\u001b[K\rremote: Resolving deltas: 100% (6/6)\u001b[K\rremote: Resolving deltas: 100% (6/6), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/ShaswataJash/kfpcomponent.git\n",
            "   5877369..7fdb8cd  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwhyFmmz5bnu",
        "outputId": "64bcaefd-4219-4109-a55f-96cbb54d61dd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}