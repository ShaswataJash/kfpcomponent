{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TabularDataPreparationUsingPycaret.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMa2eASsRy2QwewpbdNlRR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/kfpcomponent/blob/main/TabularDataPreparationUsingPycaret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is the development workflow for kubeflow pipeline component of the same name as this notebook. Refer https://github.com/ShaswataJash/kfpcomponent"
      ],
      "metadata": {
        "id": "_fMb4fVRFsPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install required softwares"
      ],
      "metadata": {
        "id": "HYI5LKdFLCTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoSae8JMvVgq",
        "outputId": "36a4ec70-7468-433e-cde9-83402fdf1993"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 8a647a2648cf 5.4.188+ #1 SMP Sun Apr 24 10:03:06 PDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZTBdke9vme4",
        "outputId": "eda5aa5a-96df-4d26-ea93-a4a09dc51d3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.5 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4orJiv6orBy",
        "outputId": "16059360-0d41-49f8-ee2e-fbebef442b09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lus4TEd-8DbB",
        "outputId": "31f50a6e-c686-4076-e57b-207510c2100f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycaret==2.3.10\n",
            "  Downloading pycaret-2.3.10-py3-none-any.whl (320 kB)\n",
            "\u001b[K     |████████████████████████████████| 320 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting kmodes>=0.10.1\n",
            "  Downloading kmodes-0.12.1-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (5.5.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (7.7.0)\n",
            "Collecting lightgbm>=2.3.1\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.3.5)\n",
            "Requirement already satisfied: pyyaml<6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: cufflinks>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.17.3)\n",
            "Collecting pandas-profiling>=2.8.0\n",
            "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.15.3)\n",
            "Collecting pyod\n",
            "  Downloading pyod-1.0.1.tar.gz (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.2.5)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 23.3 MB/s \n",
            "\u001b[?25hCollecting imbalanced-learn==0.7.0\n",
            "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 77.6 MB/s \n",
            "\u001b[?25hCollecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (2.2.4)\n",
            "Collecting mlxtend>=0.17.0\n",
            "  Downloading mlxtend-0.20.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba<0.55 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (0.51.2)\n",
            "Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (3.6.0)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (5.5.0)\n",
            "Requirement already satisfied: yellowbrick>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.4)\n",
            "Requirement already satisfied: scipy<=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.4.1)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.8 MB 33.5 MB/s \n",
            "\u001b[?25hCollecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 38.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret==2.3.10) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.7.0->pycaret==2.3.10) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->pycaret==2.3.10) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (1.15.0)\n",
            "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (0.3.0)\n",
            "Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret==2.3.10) (57.4.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->pycaret==2.3.10) (6.0.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret==2.3.10) (2.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret==2.3.10) (5.4.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (5.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.3.1->pycaret==2.3.10) (0.37.1)\n",
            "Collecting mlxtend>=0.17.0\n",
            "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret==2.3.10) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pycaret==2.3.10) (4.2.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (2.15.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (4.11.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->pycaret==2.3.10) (3.8.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<0.55->pycaret==2.3.10) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pycaret==2.3.10) (2022.1)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 36.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6.0.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Collecting markupsafe~=2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (0.5.1)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (4.64.0)\n",
            "Collecting multimethod>=1.4\n",
            "  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
            "Collecting tangled-up-in-unicode==0.2.0\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret==2.3.10) (2.11.3)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting scipy<=1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.4.1->pycaret==2.3.10) (8.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->pycaret==2.3.10) (0.2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret==2.3.10) (2.0.12)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (0.9.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (1.0.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret==2.3.10) (7.4.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->pycaret==2.3.10) (23.0.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.7.0)\n",
            "Collecting yellowbrick>=1.0.1\n",
            "  Downloading yellowbrick-1.3.post1-py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.13.3\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 282 kB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret==2.3.10) (1.3.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.1.4)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (7.1.2)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 77.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.4.36)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (3.17.3)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (0.4.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret==2.3.10) (21.3)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 863 kB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 79.4 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret==2.3.10) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret==2.3.10) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow->pycaret==2.3.10) (1.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret==2.3.10) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret==2.3.10) (1.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (5.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pycaret==2.3.10) (0.5.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow->pycaret==2.3.10) (0.14.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (0.16.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret==2.3.10) (2.8.1)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 38.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod->pycaret==2.3.10) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod->pycaret==2.3.10) (0.5.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 44.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: htmlmin, imagehash, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=bb26fe00eb2318fac2cfae86e3c8c025f39bcb53aab879922d9cd35e162e7bf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=6d17c632618fd13b08347ab09e1c229409ba45b76b8e4f47c17dab39547f2c7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=55c71b18dab9445e35778dab38250ed85213e7f638aac27452df1de6644129d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135617 sha256=b9a779872f8635a9343b9191f8ad543ef3a1ed4238f7a726247fe795067e24f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.0.1-py3-none-any.whl size=147473 sha256=798679ac063f0bbbf1a49bcc9bdd39ca204dd03ca2141c1ac05e5bf6dc7a7281\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/c4/29/67ad87835b209f72e4706369c683741b09490f2829d64ea768\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=4e3d2a13b8f90b85c1975673d01e022c51f82d5a2ad98478bbc2c2c3adcd3d30\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=47d28f462709f4d5d27da86abb059a91dd0656d068411a5bf4636048cd93e68a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built htmlmin imagehash databricks-cli pyLDAvis pyod umap-learn pynndescent\n",
            "Installing collected packages: markupsafe, numpy, tangled-up-in-unicode, smmap, scipy, multimethod, websocket-client, visions, scikit-learn, requests, pyjwt, Mako, imagehash, gitdb, querystring-parser, pyyaml, pynndescent, pydantic, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, yellowbrick, umap-learn, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: yellowbrick\n",
            "    Found existing installation: yellowbrick 1.4\n",
            "    Uninstalling yellowbrick-1.4:\n",
            "      Successfully uninstalled yellowbrick-1.4\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "  Attempting uninstall: mlxtend\n",
            "    Found existing installation: mlxtend 0.14.0\n",
            "    Uninstalling mlxtend-0.14.0:\n",
            "      Successfully uninstalled mlxtend-0.14.0\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.8.1\n",
            "    Uninstalling imbalanced-learn-0.8.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Boruta-0.3 Mako-1.2.0 alembic-1.8.0 databricks-cli-0.16.6 docker-5.0.3 funcy-1.17 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 kmodes-0.12.1 lightgbm-3.3.2 markupsafe-2.1.1 mlflow-1.26.1 mlxtend-0.19.0 multimethod-1.8 numpy-1.19.5 pandas-profiling-3.2.0 phik-0.12.2 prometheus-flask-exporter-0.20.2 pyLDAvis-3.2.2 pycaret-2.3.10 pydantic-1.9.1 pyjwt-2.4.0 pynndescent-0.5.7 pyod-1.0.1 pyyaml-5.4.1 querystring-parser-1.2.4 requests-2.27.1 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 tangled-up-in-unicode-0.2.0 umap-learn-0.5.3 visions-0.7.4 websocket-client-1.3.2 yellowbrick-1.3.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pycaret==2.3.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ref: https://github.com/pycaret/pycaret/issues/2490\n",
        "!pip install Jinja2==3.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKAa1jnEnDL3",
        "outputId": "8e4833aa-98de-4d94-fdda-960e9eb76c25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Jinja2==3.1.2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2==3.1.2) (2.1.1)\n",
            "Installing collected packages: Jinja2\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Jinja2-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ca-certificates fuse tzdata curl && \\\n",
        "  echo \"user_allow_other\" >> /etc/fuse.conf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p89zTHfYqwc",
        "outputId": "28f97941-b68f-4254-b9d7-75c8663ca3ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fuse is already the newest version (2.9.7-1ubuntu1).\n",
            "ca-certificates is already the newest version (20210119~18.04.2).\n",
            "curl is already the newest version (7.58.0-2ubuntu3.18).\n",
            "tzdata is already the newest version (2022a-0ubuntu0.18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://rclone.org/install.sh | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGe2hZo0U7DL",
        "outputId": "b87bb87a-3a67-4793-ac87-473a37004c03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4497  100  4497    0     0   6442      0 --:--:-- --:--:-- --:--:--  6433\n",
            "Archive:  rclone-current-linux-amd64.zip\n",
            "   creating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/\n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/rclone  [binary]\n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/README.html  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/README.txt  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/git-log.txt  [text]  \n",
            "  inflating: tmp_unzip_dir_for_rclone/rclone-v1.58.1-linux-amd64/rclone.1  [text]  \n",
            "Purging old database entries in /usr/share/man...\n",
            "Processing manual pages under /usr/share/man...\n",
            "Purging old database entries in /usr/share/man/ko...\n",
            "Processing manual pages under /usr/share/man/ko...\n",
            "Purging old database entries in /usr/share/man/sv...\n",
            "Processing manual pages under /usr/share/man/sv...\n",
            "Purging old database entries in /usr/share/man/pt...\n",
            "Processing manual pages under /usr/share/man/pt...\n",
            "Purging old database entries in /usr/share/man/nl...\n",
            "Processing manual pages under /usr/share/man/nl...\n",
            "Purging old database entries in /usr/share/man/it...\n",
            "Processing manual pages under /usr/share/man/it...\n",
            "Purging old database entries in /usr/share/man/de...\n",
            "Processing manual pages under /usr/share/man/de...\n",
            "Purging old database entries in /usr/share/man/tr...\n",
            "Processing manual pages under /usr/share/man/tr...\n",
            "Purging old database entries in /usr/share/man/da...\n",
            "Processing manual pages under /usr/share/man/da...\n",
            "Purging old database entries in /usr/share/man/ru...\n",
            "Processing manual pages under /usr/share/man/ru...\n",
            "Purging old database entries in /usr/share/man/hu...\n",
            "Processing manual pages under /usr/share/man/hu...\n",
            "Purging old database entries in /usr/share/man/pl...\n",
            "Processing manual pages under /usr/share/man/pl...\n",
            "Purging old database entries in /usr/share/man/fi...\n",
            "Processing manual pages under /usr/share/man/fi...\n",
            "Purging old database entries in /usr/share/man/cs...\n",
            "Processing manual pages under /usr/share/man/cs...\n",
            "Purging old database entries in /usr/share/man/zh_CN...\n",
            "Processing manual pages under /usr/share/man/zh_CN...\n",
            "Purging old database entries in /usr/share/man/zh_TW...\n",
            "Processing manual pages under /usr/share/man/zh_TW...\n",
            "Purging old database entries in /usr/share/man/es...\n",
            "Processing manual pages under /usr/share/man/es...\n",
            "Purging old database entries in /usr/share/man/ja...\n",
            "Processing manual pages under /usr/share/man/ja...\n",
            "Purging old database entries in /usr/share/man/id...\n",
            "Processing manual pages under /usr/share/man/id...\n",
            "Purging old database entries in /usr/share/man/fr...\n",
            "Processing manual pages under /usr/share/man/fr...\n",
            "Purging old database entries in /usr/share/man/pt_BR...\n",
            "Processing manual pages under /usr/share/man/pt_BR...\n",
            "Purging old database entries in /usr/share/man/sr...\n",
            "Processing manual pages under /usr/share/man/sr...\n",
            "Purging old database entries in /usr/local/man...\n",
            "Processing manual pages under /usr/local/man...\n",
            "Updating index cache for path `/usr/local/man/man1'. Wait...done.\n",
            "Checking for stray cats under /usr/local/man...\n",
            "Checking for stray cats under /var/cache/man/oldlocal...\n",
            "1 man subdirectory contained newer manual pages.\n",
            "1 manual page was added.\n",
            "0 stray cats were added.\n",
            "19 old database entries were purged.\n",
            "\n",
            "rclone v1.58.1 has successfully installed.\n",
            "Now run \"rclone config\" for setup. Check https://rclone.org/docs/ for more details.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rclone --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byk9g7UWUtJO",
        "outputId": "fa8fc957-e810-4cdb-c4dd-2ebc7312e072"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rclone v1.58.1\n",
            "- os/version: ubuntu 18.04 (64 bit)\n",
            "- os/kernel: 5.4.188+ (x86_64)\n",
            "- os/type: linux\n",
            "- os/arch: amd64\n",
            "- go/version: go1.17.9\n",
            "- go/linking: static\n",
            "- go/tags: none\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Develop source code files"
      ],
      "metadata": {
        "id": "EMVyzkX_LIoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_preparation.py\n",
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "\n",
        "# Defining and parsing the command-line arguments\n",
        "parser = argparse.ArgumentParser(description='kubeflow pipeline component to read csv file and prepare the data')\n",
        "parser.add_argument('--input-datasource-directory-mountable', default=False, action=\"store_true\", help='whether input csv file is present in mountable remote location')\n",
        "parser.add_argument('--input-datasource-directory-to-be-mounted', type=str, help='if input-datasource-directory-mountable=True, name of the mountable directory (e.g. bucket name for s3)')\n",
        "parser.add_argument('--input-datasource-file-name', type=str, help='name of the csv file including file extension (if any)')\n",
        "parser.add_argument('--additional-options-csv-parsing', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pandas.read_csv()')\n",
        "parser.add_argument('--type-of-data-analysis-task', choices=['classification', 'regression', 'clustering', 'anomaly_detection'])\n",
        "parser.add_argument('--target-variable-name', type=str, help='for classification and regression, specify the column name holding target variable')\n",
        "parser.add_argument('--target-emptyindicator', type=str, default='', help='if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc')\n",
        "parser.add_argument('--data-preparations-options', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pycaret setup() function')\n",
        "parser.add_argument('--additional-options-csv-writing', type=str, default= '{}', help='json formatted key-value pairs of strings which will be passed to pandas.to_csv()')\n",
        "parser.add_argument('--output-datasource-directory-mountable', default=False, action=\"store_true\", help='whether output csv file will be written in mountable remote location')\n",
        "parser.add_argument('--output-datasource-containing-directory', type=str, help='name of the directory (e.g. bucket name for s3) where csv file will be written')\n",
        "parser.add_argument('--output-datasource-file-name', type=str, help='filename of the prepared data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "import tempfile\n",
        "local_datastore_read_dir = tempfile.mkdtemp(prefix=\"my_local_read-\")\n",
        "print('local_datastore_read_dir:',local_datastore_read_dir)\n",
        "\n",
        "local_datastore_write_dir = tempfile.mkdtemp(prefix=\"my_local_write-\")\n",
        "print('local_datastore_write_dir:',local_datastore_write_dir)\n",
        "\n",
        "#input file handling\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "if args.input_datasource_directory_mountable:\n",
        "    input_data_read_cmd = \"rclone -v mount remoteread:\" + args.input_datasource_directory_to_be_mounted + ' ' + local_datastore_read_dir + ' --daemon'\n",
        "else:\n",
        "    input_data_read_cmd = \"rclone -v copy remoteread:\" + args.input_datasource_file_name + ' ' + local_datastore_read_dir\n",
        "input_data_read_call = subprocess.run(input_data_read_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(input_data_read_call.stdout)\n",
        "if input_data_read_call.returncode != 0:\n",
        "    print(\"Error in rclone, errorcode=\", input_data_read_call.returncode)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as rclone returned error in context of reading\")\n",
        "\n",
        "#output file handling\n",
        "if args.output_datasource_directory_mountable:\n",
        "    output_data_write_cmd = \"rclone -v mount remotewrite:\" + args.output_datasource_containing_directory + ' ' + local_datastore_write_dir + ' --daemon'\n",
        "    output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(output_data_write_call.stdout)\n",
        "    if output_data_write_call.returncode != 0:\n",
        "        print(\"Error in rclone, errorcode=\", output_data_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as rclone returned error in context of mounted writing\")\n",
        "\n",
        "#handling input csv file reading\n",
        "import pandas\n",
        "import json\n",
        "\n",
        "try:\n",
        "    parse_config = json.loads(args.additional_options_csv_parsing)\n",
        "    print('parse_config = (', type(parse_config), ')', parse_config)\n",
        "    parse_config['filepath_or_buffer'] = os.path.join(local_datastore_read_dir,args.input_datasource_file_name)\n",
        "    my_data = pandas.read_csv(**parse_config)\n",
        "    print(my_data)\n",
        "    \n",
        "except BaseException as err:\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while parsing input csv file\")\n",
        "\n",
        "#handling data preprocessing\n",
        "import pycaret\n",
        "try:\n",
        "    if os.path.exists(\"logs.log\"):\n",
        "        os.remove(\"logs.log\") #removing any content from log which pycaret will internally use for its own logging\n",
        "    print('pycaret version = ', pycaret.utils.version())\n",
        "    setup_config = json.loads(args.data_preparations_options)\n",
        "    print('setup_config = (', type(setup_config), ')', setup_config)\n",
        "    if args.type_of_data_analysis_task == 'classification':\n",
        "        import pycaret.classification\n",
        "        setup_fn = pycaret.classification.setup\n",
        "        get_config_fn = pycaret.classification.get_config\n",
        "        setup_config['target'] = args.target_variable_name\n",
        "        \n",
        "    elif args.type_of_data_analysis_task == 'regression':\n",
        "        import pycaret.regression\n",
        "        setup_fn = pycaret.regression.setup\n",
        "        get_config_fn = pycaret.regression.get_config\n",
        "        setup_config['target'] = args.target_variable_name\n",
        "\n",
        "    elif args.type_of_data_analysis_task == 'clustering':\n",
        "        import pycaret.clustering\n",
        "        setup_fn = pycaret.clustering.setup\n",
        "        get_config_fn = pycaret.clustering.get_config\n",
        "\n",
        "    elif args.type_of_data_analysis_task == 'anomaly':\n",
        "        import pycaret.anomaly\n",
        "        setup_fn = pycaret.anomaly.setup\n",
        "        get_config_fn = pycaret.anomaly.get_config\n",
        "        \n",
        "    #as part of pycaret's data cleaning the rows with target column = nan are not being cleaned up. Thus, cleaning those rows explicitely\n",
        "    if len(args.target_emptyindicator) > 0:\n",
        "        #ref: https://stackoverflow.com/questions/49291740/delete-rows-if-there-are-null-values-in-a-specific-column-in-pandas-dataframe\n",
        "        import numpy as np\n",
        "        my_data[args.target_variable_name] = my_data[args.target_variable_name].replace(args.target_emptyindicator, np.nan)\n",
        "        my_data = my_data.dropna(axis=0, subset=[args.target_variable_name])\n",
        "\n",
        "    setup_config['data'] = my_data\n",
        "    setup_config['log_experiment'] = False\n",
        "    setup_config['data_split_shuffle'] = False\n",
        "    setup_config['html'] = False\n",
        "    setup_config['silent'] = True\n",
        "    setup_fn(**setup_config)\n",
        "    #ref: https://www.kdnuggets.com/2020/11/5-things-doing-wrong-pycaret.html\n",
        "    X_transformed = get_config_fn('X')\n",
        "    my_transformed_data = X_transformed\n",
        "    if args.type_of_data_analysis_task == 'classification' or args.type_of_data_analysis_task == 'regression':\n",
        "        y_transformed = get_config_fn('y')\n",
        "        my_transformed_data = X_transformed.merge(y_transformed,left_index=True, right_index=True)\n",
        "    print(my_transformed_data)\n",
        "\n",
        "    pycaret.utils.get_system_logs() #this will print the pycaret's own log into console\n",
        "    \n",
        "except BaseException as err:\n",
        "    pycaret.utils.get_system_logs()\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while transforming input dataframe\")\n",
        "\n",
        "#handling output csv file writing\n",
        "try:\n",
        "    to_csv_config = json.loads(args.additional_options_csv_writing)\n",
        "    print('to_csv_config = (', type(to_csv_config), ')', to_csv_config)\n",
        "    to_csv_config['path_or_buf'] = os.path.join(local_datastore_write_dir,args.output_datasource_file_name)\n",
        "    my_transformed_data.to_csv(**to_csv_config)\n",
        "except BaseException as err:\n",
        "    print(\"Error=\", err, ' ', type(err))\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while trying to write prepared data\")\n",
        "\n",
        "if not args.output_datasource_directory_mountable:\n",
        "    output_data_write_cmd = \"rclone -v copy \" + os.path.join(local_datastore_write_dir,args.output_datasource_file_name) + \" remotewrite:\" + args.output_datasource_containing_directory + '/'\n",
        "    output_data_write_call = subprocess.run(output_data_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(output_data_write_call.stdout)\n",
        "    if output_data_write_call.returncode != 0:\n",
        "        print(\"Error in rclone, errorcode=\", output_data_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as rclone returned error in context of writing final csv file (copy mode)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU9Vh7P4P4_h",
        "outputId": "5e222703-21f2-42bf-8c07-6a5059dddb38"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_preparation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.7.13-slim\n",
        "\n",
        "RUN python3 -m pip install pycaret==2.3.10\n",
        "#installing jinja2 additionally due to Ref: https://github.com/pycaret/pycaret/issues/2490\n",
        "RUN python3 -m pip install Jinja2==3.1.2\n",
        "\n",
        "RUN apt update\n",
        "RUN apt-get -y install ca-certificates tzdata fuse curl && echo \"user_allow_other\" >> /etc/fuse.conf\n",
        "RUN curl https://rclone.org/install.sh | bash\n",
        "RUN rclone --version\n",
        "\n",
        "#libgomp1 installation for pycaret in python-slim\n",
        "RUN apt-get -y install libgomp1\n",
        "\n",
        "COPY src/data_preparation.py /tmp\n",
        "COPY tests/test_validation.py /tmp\n",
        "COPY run_tests.sh /tmp\n",
        "RUN chmod 544 /tmp/run_tests.sh"
      ],
      "metadata": {
        "id": "hk00hb780Qo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64c1bc7-66aa-43b3-a4dd-88dc41218a66"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_tests.sh\n",
        "#!/bin/bash\n",
        "\n",
        "export RCLONE_CONFIG_REMOTEREAD_TYPE='http'\n",
        "export RCLONE_CONFIG_REMOTEREAD_URL='https://raw.githubusercontent.com/pycaret/datasets/main/data/common/'\n",
        "\n",
        "export RCLONE_CONFIG_REMOTEWRITE_TYPE='local'\n",
        "export RCLONE_CONFIG_REMOTEWRITE_NOUNC='true'\n",
        "\n",
        "mkdir /tmp/my_local_dir_for_test\n",
        "\n",
        "#Test: csv reading source from http, rclone read in copy, rclone write in mount mode\n",
        "python /tmp/data_preparation.py --input-datasource-file-name 'CTG.csv' --additional-options-csv-parsing '{\"sep\":\",\" , \"header\":0}' \\\n",
        "    --type-of-data-analysis-task 'classification' --target-variable-name 'NSP' \\\n",
        "    --data-preparations-options '{\"ignore_low_variance\":true, \"remove_outliers\":true, \"remove_multicollinearity\":true, \"multicollinearity_threshold\":0.7}' \\\n",
        "    --output-datasource-directory-mountable --output-datasource-containing-directory '/tmp/my_local_dir_for_test' --output-datasource-file-name 'CTG_data-prep.csv'\n",
        "\n",
        "#https://registry.opendata.aws/humor-detection/\n",
        "export RCLONE_CONFIG_REMOTEREAD_TYPE='s3'\n",
        "export RCLONE_CONFIG_REMOTEREAD_PROVIDER='AWS'\n",
        "export RCLONE_CONFIG_REMOTEREAD_REGION='us-west-2' \n",
        "\n",
        "#Test: csv reading source from s3(AWS provider), rclone read in mount, rclone write in copy mode\n",
        "python /tmp/data_preparation.py --input-datasource-directory-mountable --input-datasource-directory-to-be-mounted 'humor-detection-pds/' --input-datasource-file-name 'Non-humours-biased.csv' \\\n",
        "    --type-of-data-analysis-task 'classification' --target-variable-name 'label' \\\n",
        "    --data-preparations-options '{\"preprocess\":false, \"ignore_features\":[\"image_url\"]}' \\\n",
        "    --output-datasource-containing-directory '/tmp/my_local_dir_for_test' --output-datasource-file-name 'Non-humours-biased_data-prep.csv'\n",
        "\n",
        "python /tmp/test_validation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzJg29vTVGcf",
        "outputId": "9f503816-266e-4e3d-adb7-156d62d1249e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_tests.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_validation.py\n",
        "#!/usr/bin/env python3\n",
        "import pandas\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/CTG_data-prep.csv')\n",
        "assert len(df.index) == 2126 #original data had 2129 rows, amongst that 3 rows have no target\n",
        "assert df.isnull().sum().sum() == 0 #pycaret will remove all missing values\n",
        "\n",
        "df = pandas.read_csv(filepath_or_buffer = '/tmp/my_local_dir_for_test/Non-humours-biased_data-prep.csv')\n",
        "assert len(df.columns) == 3 #original data had 4 columns, amongst that one will be dropped\n",
        "print ('test-validation done successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7maMc-RVqDV5",
        "outputId": "7675df43-1b6e-48f1-c7b8-22749818ef74"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_validation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer: https://github.com/RealOrangeOne/docker-rclone-mount/blob/master/docker-compose.yml\n",
        "\n",
        "If we have to use mount feature of rclone, it needs to have fuse support in underneath linux kernel. For that we are adding SYS_ADMIN in capability. But note without using mount feature also, we can do testing. in that case, rclone will use only copy feature."
      ],
      "metadata": {
        "id": "YRSP8Vk8vyoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile docker-compose.test.yml\n",
        "services:\n",
        "  sut:\n",
        "    build: .\n",
        "    command: /tmp/run_tests.sh\n",
        "    cap_add:\n",
        "      - SYS_ADMIN\n",
        "    security_opt:\n",
        "      - apparmor:unconfined\n",
        "    devices:\n",
        "      - \"/dev/fuse:/dev/fuse\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVrXyQZhSIm_",
        "outputId": "901bae10-1e2d-4093-a0d6-322eddfb1637"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing docker-compose.test.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py"
      ],
      "metadata": {
        "id": "W7uyWGT6Sccr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile component.yaml\n",
        "name: TabularDataPreparationUsingPycaret\n",
        "description: |\n",
        "    Prepare tabular data (csv file) using pycaret library. (For pycaret's data pre-processing capabilities, refer https://pycaret.gitbook.io/docs/get-started/preprocessing)\n",
        "    pycaret internally uses pandas dataframe to read and write csv file. \n",
        "    Input and processed csv file are stored in rclone compatible storage. Both mount and copy mode are supported. (refer: https://rclone.org/)\n",
        "    rclone configurations have to be shared through environment variables (refer: https://rclone.org/docs/#environment-variables). \n",
        "    Thus, before using this component in kubeflow pipeline, those environment variables have to be set from pipeline.\n",
        "    Create rclone read and write configuration file name as 'REMOTEREAD' and 'REMOTEWRITE'. Because the same are used within code.\n",
        "    So convention for creating any environment variables related to rclone should start either with 'RCLONE_CONFIG_REMOTEREAD' or 'RCLONE_CONFIG_REMOTEWRITE' \n",
        "\n",
        "inputs:\n",
        "- {name: input-datasource-directory-mountable, optional, description: 'whether input csv file is present in mountable remote location'}\n",
        "- {name: input-datasource-directory-to-be-mounted, type: String, description: 'if input-datasource-directory-mountable=True, name of the mountable directory (e.g. bucket name for s3)'}\n",
        "- {name: input-datasource-file-name, type: String, description: 'name of the csv file including file extension (if any)'}\n",
        "- {name: additional-options-csv-parsing, type: String, default= '{}', description: 'json formatted key-value pairs of strings which will be passed to pandas.read_csv()'}\n",
        "- {name: type-of-data-analysis-task, type: List, description: 'choice amongst classification, regression, clustering, anomaly_detection'}\n",
        "- {name: target-variable-name, type: String, description: 'for classification and regression, specify the column name holding target variable'}\n",
        "- {name: target-emptyindicator, type: String, default='', description: 'if target variable column holds null or na, those rows will be dropped. Sometime empty can be indicated by other representative string like - or *** etc'}\n",
        "- {name: data-preparations-options, type: String, default= '{}', description: 'json formatted key-value pairs of strings which will be passed to pycaret setup() function'}\n",
        "- {name: additional-options-csv-writing, type: String, default= '{}', description: 'json formatted key-value pairs of strings which will be passed to pandas.to_csv()'}\n",
        "- {name: output-datasource-directory-mountable, optional, description: 'whether output csv file will be written in mountable remote location'}\n",
        "- {name: output-datasource-containing-directory, type: String, description: 'name of the directory (e.g. bucket name for s3) where csv file will be written'}\n",
        "- {name: output-datasource-file-name, type: String, description: 'filename of the prepared data'}\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: hub.docker.com/shasjash/kfpcomponents/TabularDataPreparationUsingPycaret_devlatest\n",
        "    # command is a list of strings (command-line arguments). \n",
        "    # The YAML language has two syntaxes for lists and you can use either of them. \n",
        "    # Here we use the \"flow syntax\" - comma-separated strings inside square brackets.\n",
        "    command: [\n",
        "      python3, \n",
        "      # Path of the program inside the container\n",
        "      /tmp/data_preparation.py,\n",
        "      --input-datasource-directory-mountable,\n",
        "      {inputValue: input-datasource-directory-mountable},\n",
        "      --input-datasource-directory-to-be-mounted, \n",
        "      {inputValue: input-datasource-directory-to-be-mounted},\n",
        "      --input-datasource-file-name, \n",
        "      {inputValue: input-datasource-file-name},\n",
        "      --additional-options-csv-parsing, \n",
        "      {inputValue: additional-options-csv-parsing},\n",
        "      --type-of-data-analysis-task, \n",
        "      {inputValue: type-of-data-analysis-task},\n",
        "      --target-variable-name, \n",
        "      {inputValue: target-variable-name},\n",
        "      --target-emptyindicator, \n",
        "      {inputValue: target-emptyindicator},\n",
        "      --data-preparations-options, \n",
        "      {inputValue:data-preparations-options},\n",
        "      --additional-options-csv-writing, \n",
        "      {inputValue: additional-options-csv-writing},\n",
        "      --output-datasource-directory-mountable, \n",
        "      {inputValue: output-datasource-directory-mountable},\n",
        "      --output-datasource-containing-directory, \n",
        "      {inputValue: output-datasource-containing-directory},\n",
        "      --output-datasource-file-name, \n",
        "      {inputValue: output-datasource-file-name},\n",
        "    ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhXccnmvIo7u",
        "outputId": "7264fba5-db7e-4a45-bcc9-b5eb7857b878"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing component.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Software testing"
      ],
      "metadata": {
        "id": "9uiUWKu4LWUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us simulate testing what will be done by docker-hub infrastructure as part of auto-testing by using docker-compose.test.yml present in github source repository."
      ],
      "metadata": {
        "id": "i3TK5UnMvfMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/my_local_dir_for_test\n",
        "!chmod 544 run_tests.sh\n",
        "!cp data_preparation.py /tmp\n",
        "!cp test_validation.py /tmp\n",
        "!./run_tests.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9h_W3T4H87",
        "outputId": "88a7c639-e8a6-4920-d273-7405f14cf8d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "local_datastore_read_dir: /tmp/my_local_read-bcz_2830\n",
            "local_datastore_write_dir: /tmp/my_local_write-w9aknu8p\n",
            "2022/06/07 09:23:56 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "2022/06/07 09:23:56 INFO  : CTG.csv: Copied (new)\n",
            "2022/06/07 09:23:56 INFO  : \n",
            "Transferred:   \t  277.715 KiB / 277.715 KiB, 100%, 0 B/s, ETA -\n",
            "Transferred:            1 / 1, 100%\n",
            "Elapsed time:         0.6s\n",
            "\n",
            "\n",
            "2022/06/07 09:23:56 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "\n",
            "parse_config = ( <class 'dict'> ) {'sep': ',', 'header': 0}\n",
            "          FileName  ...  NSP\n",
            "0     Variab10.txt  ...  2.0\n",
            "1       Fmcs_1.txt  ...  1.0\n",
            "2       Fmcs_1.txt  ...  1.0\n",
            "3       Fmcs_1.txt  ...  1.0\n",
            "4       Fmcs_1.txt  ...  1.0\n",
            "...            ...  ...  ...\n",
            "2124  S8001045.dsp  ...  2.0\n",
            "2125  S8001045.dsp  ...  1.0\n",
            "2126           NaN  ...  NaN\n",
            "2127           NaN  ...  NaN\n",
            "2128           NaN  ...  NaN\n",
            "\n",
            "[2129 rows x 40 columns]\n",
            "pycaret version =  2.3.10\n",
            "setup_config = ( <class 'dict'> ) {'ignore_low_variance': True, 'remove_outliers': True, 'remove_multicollinearity': True, 'multicollinearity_threshold': 0.7}\n",
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n",
            "Setup Succesfully Completed!\n",
            "                               Description             Value\n",
            "0                               session_id              3655\n",
            "1                                   Target               NSP\n",
            "2                              Target Type        Multiclass\n",
            "3                            Label Encoded              None\n",
            "4                            Original Data        (2129, 40)\n",
            "5                           Missing Values              True\n",
            "6                         Numeric Features                25\n",
            "7                     Categorical Features                13\n",
            "8                         Ordinal Features             False\n",
            "9                High Cardinality Features             False\n",
            "10                 High Cardinality Method              None\n",
            "11                   Transformed Train Set        (1415, 86)\n",
            "12                    Transformed Test Set         (636, 86)\n",
            "13                      Shuffle Train-Test             False\n",
            "14                     Stratify Train-Test             False\n",
            "15                          Fold Generator   StratifiedKFold\n",
            "16                             Fold Number                10\n",
            "17                                CPU Jobs                -1\n",
            "18                                 Use GPU             False\n",
            "19                          Log Experiment             False\n",
            "20                         Experiment Name  clf-default-name\n",
            "21                                     USI              5e68\n",
            "22                         Imputation Type            simple\n",
            "23          Iterative Imputation Iteration              None\n",
            "24                         Numeric Imputer              mean\n",
            "25      Iterative Imputation Numeric Model              None\n",
            "26                     Categorical Imputer          constant\n",
            "27  Iterative Imputation Categorical Model              None\n",
            "28           Unknown Categoricals Handling    least_frequent\n",
            "29                               Normalize             False\n",
            "30                        Normalize Method              None\n",
            "31                          Transformation             False\n",
            "32                   Transformation Method              None\n",
            "33                                     PCA             False\n",
            "34                              PCA Method              None\n",
            "35                          PCA Components              None\n",
            "36                     Ignore Low Variance              True\n",
            "37                     Combine Rare Levels             False\n",
            "38                    Rare Level Threshold              None\n",
            "39                         Numeric Binning             False\n",
            "40                         Remove Outliers              True\n",
            "41                      Outliers Threshold              0.05\n",
            "42                Remove Multicollinearity              True\n",
            "43             Multicollinearity Threshold               0.7\n",
            "44             Remove Perfect Collinearity              True\n",
            "45                              Clustering             False\n",
            "46                    Clustering Iteration              None\n",
            "47                     Polynomial Features             False\n",
            "48                       Polynomial Degree              None\n",
            "49                    Trignometry Features             False\n",
            "50                    Polynomial Threshold              None\n",
            "51                          Group Features             False\n",
            "52                       Feature Selection             False\n",
            "53                Feature Selection Method           classic\n",
            "54            Features Selection Threshold              None\n",
            "55                     Feature Interaction             False\n",
            "56                           Feature Ratio             False\n",
            "57                   Interaction Threshold              None\n",
            "58                           Fix Imbalance             False\n",
            "59                    Fix Imbalance Method             SMOTE\n",
            "           e  ...  NSP\n",
            "0      357.0  ...  2.0\n",
            "1      632.0  ...  1.0\n",
            "2      779.0  ...  1.0\n",
            "3     1192.0  ...  1.0\n",
            "4     1147.0  ...  1.0\n",
            "...      ...  ...  ...\n",
            "2121  2867.0  ...  2.0\n",
            "2122  2867.0  ...  2.0\n",
            "2123  2596.0  ...  2.0\n",
            "2124  3049.0  ...  2.0\n",
            "2125  3415.0  ...  1.0\n",
            "\n",
            "[2126 rows x 87 columns]\n",
            "['2022-06-07 09', '23', '59,582', 'INFO', 'PyCaret Supervised Module']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'ML Usecase', 'classification']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'version 2.3.10']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'Initializing setup()']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', \"setup(target=NSP, ml_usecase=classification, available_plots={'parameter'\", \"'Hyperparameters', 'auc'\", \"'AUC', 'confusion_matrix'\", \"'Confusion Matrix', 'threshold'\", \"'Threshold', 'pr'\", \"'Precision Recall', 'error'\", \"'Prediction Error', 'class_report'\", \"'Class Report', 'rfe'\", \"'Feature Selection', 'learning'\", \"'Learning Curve', 'manifold'\", \"'Manifold Learning', 'calibration'\", \"'Calibration Curve', 'vc'\", \"'Validation Curve', 'dimension'\", \"'Dimensions', 'feature'\", \"'Feature Importance', 'feature_all'\", \"'Feature Importance (All)', 'boundary'\", \"'Decision Boundary', 'lift'\", \"'Lift Chart', 'gain'\", \"'Gain Chart', 'tree'\", \"'Decision Tree', 'ks'\", \"'KS Statistic Plot'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=constant, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=None, numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=None, normalize=False, normalize_method=zscore, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=True, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=True, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=True, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=stratifiedkfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=False, session_id=None, log_experiment=False, experiment_name=None, experiment_custom_tags=None, log_plots=False, log_profile=False, log_data=False, silent=True, verbose=True, profile=False, profile_kwargs=None, display=None)\"]\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'Checking environment']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'python_version', '3.7.13']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'python_build', \"('default', 'Apr 24 2022 01\", '04', \"09')\"]\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'machine', 'x86_64']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'platform', 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic']\n",
            "['2022-06-07 09', '23', '59,583', 'INFO', 'Memory', 'svmem(total=13617745920, available=12584595456, percent=7.6, used=776605696, free=11635802112, active=1150386176, inactive=632107008, buffers=88776704, cached=1116561408, shared=1232896, slab=142557184)']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'Physical Core', '1']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'Logical Core', '2']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'Checking libraries']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'pd==1.3.5']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'numpy==1.19.5']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'sklearn==0.23.2']\n",
            "['2022-06-07 09', '23', '59,584', 'INFO', 'lightgbm==3.3.2']\n",
            "['2022-06-07 09', '23', '59,584', 'WARNING', 'catboost not found']\n",
            "['2022-06-07 09', '23', '59,599', 'INFO', 'xgboost==0.90']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'mlflow==1.26.1']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'Checking Exceptions']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'Declaring global variables']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'USI', '5e68']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'pycaret_globals', \"{'dashboard_logger', 'seed', 'fold_groups_param', 'prep_pipe', 'pycaret_globals', 'fold_shuffle_param', 'experiment__', 'target_param', 'transform_target_param', '_ml_usecase', 'create_model_container', '_all_models', 'data_before_preprocess', 'X_train', 'imputation_classifier', 'y_test', 'imputation_regressor', 'n_jobs_param', 'y_train', 'fold_param', '_gpu_n_jobs_param', 'exp_name_log', '_all_metrics', 'gpu_param', 'display_container', 'stratify_param', 'fix_imbalance_param', 'fix_imbalance_method_param', 'y', '_all_models_internal', 'transform_target_method_param', '_internal_pipeline', '_available_plots', 'iterative_imputation_iters_param', 'USI', 'master_model_container', 'X_test', 'log_plots_param', 'html_param', 'fold_generator', 'fold_groups_param_full', 'X', 'logging_param'}\"]\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'Importing libraries']\n",
            "['2022-06-07 09', '23', '59,600', 'INFO', 'Copying data for preprocessing']\n",
            "['2022-06-07 09', '23', '59,601', 'INFO', 'Declaring preprocessing parameters']\n",
            "['2022-06-07 09', '23', '59,605', 'INFO', 'Creating preprocessing pipeline']\n",
            "['2022-06-07 09', '23', '59,620', 'INFO', 'Preprocessing pipeline created successfully']\n",
            "['2022-06-07 09', '23', '59,620', 'ERROR', '(Process Exit)', \"setup has been interupted with user command 'quit'. setup must rerun.\"]\n",
            "['2022-06-07 09', '23', '59,620', 'INFO', 'Creating global containers']\n",
            "['2022-06-07 09', '23', '59,621', 'INFO', 'Internal pipeline', \"Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\"]\n",
            "['2022-06-07 09', '26', '00,747', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-07 09', '26', '00,748', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-07 09', '26', '00,835', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-07 09', '26', '00,835', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-07 09', '26', '00,836', 'INFO', 'Creating grid variables']\n",
            "['2022-06-07 09', '26', '00,892', 'INFO', 'create_model_container', '0']\n",
            "['2022-06-07 09', '26', '00,892', 'INFO', 'master_model_container', '0']\n",
            "['2022-06-07 09', '26', '00,893', 'INFO', 'display_container', '1']\n",
            "['2022-06-07 09', '26', '00,899', 'INFO', 'Pipeline(memory=None,']\n",
            "[\"steps=[('dtypes',\"]\n",
            "['DataTypes_Auto_infer(categorical_features=[],']\n",
            "['display_types=False, features_todrop=[],']\n",
            "['id_columns=[],']\n",
            "[\"ml_usecase='classification',\"]\n",
            "[\"numerical_features=[], target='NSP',\"]\n",
            "['time_features=[])),']\n",
            "[\"('imputer',\"]\n",
            "[\"Simple_Imputer(categorical_strategy='not_available',\"]\n",
            "['fill_value_categorical=None,']\n",
            "['fill_value_numerical=None,']\n",
            "['numeric_strateg...']\n",
            "[\"('dummy', Dummify(target='NSP')),\"]\n",
            "[\"('fix_perfect', Remove_100(target='NSP')),\"]\n",
            "[\"('clean_names', Clean_Colum_Names()),\"]\n",
            "[\"('feature_select', 'passthrough'),\"]\n",
            "[\"('fix_multi',\"]\n",
            "['Fix_multicollinearity(correlation_with_target_preference=None,']\n",
            "['correlation_with_target_threshold=0.0,']\n",
            "[\"target_variable='NSP', threshold=0.7)),\"]\n",
            "[\"('dfs', 'passthrough'), ('pca', 'passthrough')],\"]\n",
            "['verbose=False)']\n",
            "['2022-06-07 09', '26', '00,899', 'INFO', 'setup() succesfully completed......................................']\n",
            "['2022-06-07 09', '26', '00,993', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-07 09', '26', '00,993', 'INFO', 'get_config(variable=X)']\n",
            "['2022-06-07 09', '26', '01,110', 'INFO', 'Global variable', 'X returned as            e  ...  Date_is_month_start_0']\n",
            "['0      357.0  ...                    0.0']\n",
            "['1      632.0  ...                    1.0']\n",
            "['2      779.0  ...                    1.0']\n",
            "['3     1192.0  ...                    1.0']\n",
            "['4     1147.0  ...                    1.0']\n",
            "['...      ...  ...                    ...']\n",
            "['2121  2867.0  ...                    1.0']\n",
            "['2122  2867.0  ...                    1.0']\n",
            "['2123  2596.0  ...                    1.0']\n",
            "['2124  3049.0  ...                    1.0']\n",
            "['2125  3415.0  ...                    1.0']\n",
            "['[2126 rows x 86 columns]']\n",
            "['2022-06-07 09', '26', '01,111', 'INFO', 'get_config() succesfully completed......................................']\n",
            "['2022-06-07 09', '26', '01,111', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-07 09', '26', '01,111', 'INFO', 'get_config(variable=y)']\n",
            "['2022-06-07 09', '26', '01,112', 'INFO', 'Global variable', 'y returned as 0       2.0']\n",
            "['1       1.0']\n",
            "['2       1.0']\n",
            "['3       1.0']\n",
            "['4       1.0']\n",
            "['...']\n",
            "['2121    2.0']\n",
            "['2122    2.0']\n",
            "['2123    2.0']\n",
            "['2124    2.0']\n",
            "['2125    1.0']\n",
            "['Name', 'NSP, Length', '2126, dtype', 'float32']\n",
            "['2022-06-07 09', '26', '01,112', 'INFO', 'get_config() succesfully completed......................................']\n",
            "to_csv_config = ( <class 'dict'> ) {}\n",
            "local_datastore_read_dir: /tmp/my_local_read-3gigmjia\n",
            "local_datastore_write_dir: /tmp/my_local_write-g3ja9ax4\n",
            "2022/06/07 09:26:01 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "\n",
            "parse_config = ( <class 'dict'> ) {}\n",
            "                                               question  ... label\n",
            "0              What is the size range of these patterns  ...     0\n",
            "1               Are the instructions in uk or us terms?  ...     0\n",
            "2                 Is this manual good for 2014 model c3  ...     0\n",
            "3      Hello does the light have adjustable brightness?  ...     0\n",
            "4      Will these inks work with the wf-7720dtwf model?  ...     0\n",
            "...                                                 ...  ...   ...\n",
            "9566  Does the 68lm 6 inch give an audible alarm if ...  ...     0\n",
            "9567  Do you need 2 phone sockets / connection boxes...  ...     0\n",
            "9568  I buy it on 05.11.2015 and today my usb device...  ...     0\n",
            "9569  Hi What is the warranty period on this? I noti...  ...     0\n",
            "9570                      can you use proper golf balls  ...     0\n",
            "\n",
            "[9571 rows x 4 columns]\n",
            "pycaret version =  2.3.10\n",
            "setup_config = ( <class 'dict'> ) {'preprocess': False, 'ignore_features': ['image_url']}\n",
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n",
            "Setup Succesfully Completed!\n",
            "              Description             Value\n",
            "0              session_id              3751\n",
            "1                  Target             label\n",
            "2             Target Type            Binary\n",
            "3           Label Encoded              None\n",
            "4           Original Data         (9571, 4)\n",
            "5          Missing Values             False\n",
            "6        Numeric Features                 0\n",
            "7    Categorical Features                 2\n",
            "8   Transformed Train Set         (6699, 2)\n",
            "9    Transformed Test Set         (2872, 2)\n",
            "10     Shuffle Train-Test             False\n",
            "11    Stratify Train-Test             False\n",
            "12         Fold Generator   StratifiedKFold\n",
            "13            Fold Number                10\n",
            "14               CPU Jobs                -1\n",
            "15                Use GPU             False\n",
            "16         Log Experiment             False\n",
            "17        Experiment Name  clf-default-name\n",
            "18                    USI              53dd\n",
            "19          Fix Imbalance             False\n",
            "20   Fix Imbalance Method             SMOTE\n",
            "                                               question  ... label\n",
            "0              What is the size range of these patterns  ...     0\n",
            "1               Are the instructions in uk or us terms?  ...     0\n",
            "2                 Is this manual good for 2014 model c3  ...     0\n",
            "3      Hello does the light have adjustable brightness?  ...     0\n",
            "4      Will these inks work with the wf-7720dtwf model?  ...     0\n",
            "...                                                 ...  ...   ...\n",
            "9566  Does the 68lm 6 inch give an audible alarm if ...  ...     0\n",
            "9567  Do you need 2 phone sockets / connection boxes...  ...     0\n",
            "9568  I buy it on 05.11.2015 and today my usb device...  ...     0\n",
            "9569  Hi What is the warranty period on this? I noti...  ...     0\n",
            "9570                      can you use proper golf balls  ...     0\n",
            "\n",
            "[9571 rows x 3 columns]\n",
            "['2022-06-07 09', '26', '05,082', 'INFO', 'PyCaret Supervised Module']\n",
            "['2022-06-07 09', '26', '05,082', 'INFO', 'ML Usecase', 'classification']\n",
            "['2022-06-07 09', '26', '05,082', 'INFO', 'version 2.3.10']\n",
            "['2022-06-07 09', '26', '05,082', 'INFO', 'Initializing setup()']\n",
            "['2022-06-07 09', '26', '05,082', 'INFO', \"setup(target=label, ml_usecase=classification, available_plots={'parameter'\", \"'Hyperparameters', 'auc'\", \"'AUC', 'confusion_matrix'\", \"'Confusion Matrix', 'threshold'\", \"'Threshold', 'pr'\", \"'Precision Recall', 'error'\", \"'Prediction Error', 'class_report'\", \"'Class Report', 'rfe'\", \"'Feature Selection', 'learning'\", \"'Learning Curve', 'manifold'\", \"'Manifold Learning', 'calibration'\", \"'Calibration Curve', 'vc'\", \"'Validation Curve', 'dimension'\", \"'Dimensions', 'feature'\", \"'Feature Importance', 'feature_all'\", \"'Feature Importance (All)', 'boundary'\", \"'Decision Boundary', 'lift'\", \"'Lift Chart', 'gain'\", \"'Gain Chart', 'tree'\", \"'Decision Tree', 'ks'\", \"'KS Statistic Plot'}, train_size=0.7, test_data=None, preprocess=False, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=constant, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=None, numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['image_url'], normalize=False, normalize_method=zscore, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=False, multicollinearity_threshold=0.9, remove_perfect_collinearity=True, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=stratifiedkfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=False, session_id=None, log_experiment=False, experiment_name=None, experiment_custom_tags=None, log_plots=False, log_profile=False, log_data=False, silent=True, verbose=True, profile=False, profile_kwargs=None, display=None)\"]\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'Checking environment']\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'python_version', '3.7.13']\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'python_build', \"('default', 'Apr 24 2022 01\", '04', \"09')\"]\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'machine', 'x86_64']\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'platform', 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic']\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'Memory', 'svmem(total=13617745920, available=12534235136, percent=8.0, used=806072320, free=11723505664, active=1285165056, inactive=405245952, buffers=82202624, cached=1005965312, shared=1232896, slab=139878400)']\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'Physical Core', '1']\n",
            "['2022-06-07 09', '26', '05,083', 'INFO', 'Logical Core', '2']\n",
            "['2022-06-07 09', '26', '05,084', 'INFO', 'Checking libraries']\n",
            "['2022-06-07 09', '26', '05,084', 'INFO', 'pd==1.3.5']\n",
            "['2022-06-07 09', '26', '05,084', 'INFO', 'numpy==1.19.5']\n",
            "['2022-06-07 09', '26', '05,084', 'INFO', 'sklearn==0.23.2']\n",
            "['2022-06-07 09', '26', '05,084', 'INFO', 'lightgbm==3.3.2']\n",
            "['2022-06-07 09', '26', '05,084', 'WARNING', 'catboost not found']\n",
            "['2022-06-07 09', '26', '05,091', 'INFO', 'xgboost==0.90']\n",
            "['2022-06-07 09', '26', '05,091', 'INFO', 'mlflow==1.26.1']\n",
            "['2022-06-07 09', '26', '05,091', 'INFO', 'Checking Exceptions']\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'Declaring global variables']\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'USI', '53dd']\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'pycaret_globals', \"{'master_model_container', 'fold_param', '_ml_usecase', '_available_plots', 'html_param', 'fold_groups_param_full', 'prep_pipe', 'X_train', 'y_train', 'X', 'iterative_imputation_iters_param', 'transform_target_method_param', 'experiment__', 'target_param', 'imputation_regressor', 'fix_imbalance_method_param', 'dashboard_logger', 'stratify_param', 'USI', '_internal_pipeline', 'create_model_container', 'log_plots_param', '_gpu_n_jobs_param', 'n_jobs_param', 'logging_param', 'gpu_param', '_all_models_internal', 'y', 'X_test', '_all_models', 'display_container', 'seed', 'exp_name_log', 'fold_groups_param', 'fold_generator', '_all_metrics', 'fold_shuffle_param', 'transform_target_param', 'fix_imbalance_param', 'y_test', 'data_before_preprocess', 'pycaret_globals', 'imputation_classifier'}\"]\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'Preparing display monitor']\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'Importing libraries']\n",
            "['2022-06-07 09', '26', '05,092', 'INFO', 'Copying data for preprocessing']\n",
            "['2022-06-07 09', '26', '05,093', 'INFO', 'Declaring preprocessing parameters']\n",
            "['2022-06-07 09', '26', '05,095', 'INFO', 'Creating preprocessing pipeline']\n",
            "['2022-06-07 09', '26', '05,102', 'INFO', 'Preprocessing pipeline created successfully']\n",
            "['2022-06-07 09', '26', '05,102', 'ERROR', '(Process Exit)', \"setup has been interupted with user command 'quit'. setup must rerun.\"]\n",
            "['2022-06-07 09', '26', '05,102', 'INFO', 'Creating global containers']\n",
            "['2022-06-07 09', '26', '05,102', 'INFO', 'Internal pipeline', \"Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\"]\n",
            "['2022-06-07 09', '26', '05,233', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-07 09', '26', '05,234', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-07 09', '26', '05,313', 'WARNING', 'Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90']\n",
            "['2022-06-07 09', '26', '05,314', 'WARNING', \"Couldn't import catboost.CatBoostClassifier\"]\n",
            "['2022-06-07 09', '26', '05,314', 'INFO', 'Creating grid variables']\n",
            "['2022-06-07 09', '26', '05,363', 'INFO', 'create_model_container', '0']\n",
            "['2022-06-07 09', '26', '05,363', 'INFO', 'master_model_container', '0']\n",
            "['2022-06-07 09', '26', '05,363', 'INFO', 'display_container', '1']\n",
            "['2022-06-07 09', '26', '05,364', 'INFO', 'Pipeline(memory=None,']\n",
            "[\"steps=[('dtypes',\"]\n",
            "['DataTypes_Auto_infer(categorical_features=[],']\n",
            "['display_types=False,']\n",
            "[\"features_todrop=['image_url'],\"]\n",
            "['id_columns=[],']\n",
            "[\"ml_usecase='classification',\"]\n",
            "[\"numerical_features=[], target='label',\"]\n",
            "['time_features=[]))],']\n",
            "['verbose=False)']\n",
            "['2022-06-07 09', '26', '05,365', 'INFO', 'setup() succesfully completed......................................']\n",
            "['2022-06-07 09', '26', '05,450', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-07 09', '26', '05,450', 'INFO', 'get_config(variable=X)']\n",
            "['2022-06-07 09', '26', '05,456', 'INFO', 'Global variable', 'X returned as                                                question                                product_description']\n",
            "['0              What is the size range of these patterns  The Colette Sewing Handbook', '5 Fundamentals fo...']\n",
            "['1               Are the instructions in uk or us terms?                        Absolutely Gorgeous Doilies']\n",
            "['2                 Is this manual good for 2014 model c3  Citroen C3 Petrol & Diesel Service and Repair ...']\n",
            "['3      Hello does the light have adjustable brightness?       Mighty Bright Blue Xtraflex 2 LED Book Light']\n",
            "['4      Will these inks work with the wf-7720dtwf model?  24 High Capacity ink cartridge to Replace 27XX...']\n",
            "['...                                                 ...                                                ...']\n",
            "['9566  Does the 68lm 6 inch give an audible alarm if ...  Garmin Nuvi 68LM 6 inch Satellite Navigation w...']\n",
            "['9567  Do you need 2 phone sockets / connection boxes...  iDECT Eclipse Plus Dect Phone with Call Blocke...']\n",
            "['9568  I buy it on 05.11.2015 and today my usb device...  SanDisk SDCZ43-128G-G46 Ultra Fit 128 GB USB F...']\n",
            "['9569  Hi What is the warranty period on this? I noti...  SanDisk SDCZ43-128G-G46 Ultra Fit 128 GB USB F...']\n",
            "['9570                      can you use proper golf balls  Haack Golf Net By SEC Coach Chris Haack by Rukket']\n",
            "['[9571 rows x 2 columns]']\n",
            "['2022-06-07 09', '26', '05,456', 'INFO', 'get_config() succesfully completed......................................']\n",
            "['2022-06-07 09', '26', '05,456', 'INFO', 'Initializing get_config()']\n",
            "['2022-06-07 09', '26', '05,456', 'INFO', 'get_config(variable=y)']\n",
            "['2022-06-07 09', '26', '05,457', 'INFO', 'Global variable', 'y returned as 0       0']\n",
            "['1       0']\n",
            "['2       0']\n",
            "['3       0']\n",
            "['4       0']\n",
            "['..']\n",
            "['9566    0']\n",
            "['9567    0']\n",
            "['9568    0']\n",
            "['9569    0']\n",
            "['9570    0']\n",
            "['Name', 'label, Length', '9571, dtype', 'int64']\n",
            "['2022-06-07 09', '26', '05,457', 'INFO', 'get_config() succesfully completed......................................']\n",
            "to_csv_config = ( <class 'dict'> ) {}\n",
            "2022/06/07 09:26:05 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\n",
            "2022/06/07 09:26:05 INFO  : Non-humours-biased_data-prep.csv: Copied (new)\n",
            "2022/06/07 09:26:05 INFO  : \n",
            "Transferred:   \t    1.433 MiB / 1.433 MiB, 100%, 0 B/s, ETA -\n",
            "Transferred:            1 / 1, 100%\n",
            "Elapsed time:         0.0s\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/test_validation.py\", line 8, in <module>\n",
            "    assert len(df.columns) == 3 #original data had 4 columns, amongst that one will be dropped\n",
            "AssertionError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before commiting code to github, install github client (gh) by following instruction mentioned in https://github.com/cli/cli/blob/trunk/docs/install_linux.md (Choose Debian, Ubuntu Linux way of installation) \n",
        "\n",
        "Use the colab's 'Terminal' icon present in left vertical pane to open linux terminal to type commands. Once 'gh' is installed, type **$gh auth login** (refer https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git) to follow onscreen prompts. For colab, use **Paste an authentication token** option. Personal tokens can be generated in https://github.com/settings/tokens\n",
        "\n",
        "You can use Shift+Ctrl+v shortcut to paste any string in colab console"
      ],
      "metadata": {
        "id": "2Nix3R2joPdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Push the code to github"
      ],
      "metadata": {
        "id": "a62pERh-Lasc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G12wQfVT1ann",
        "outputId": "d8f9b4da-654d-4b12-ee3f-96753a3d51f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -Rf kfpcomponent"
      ],
      "metadata": {
        "id": "JVc_RmC9MrgG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShaswataJash/kfpcomponent.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5LCKX0SXUxs",
        "outputId": "88476425-356a-466e-f345-13bb21766893"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kfpcomponent'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 102 (delta 44), reused 78 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 40.41 KiB | 713.00 KiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow directory structure according to https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#organizing-the-component-files"
      ],
      "metadata": {
        "id": "AIwZrmfnCXPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret\n",
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret/src\n",
        "!mkdir kfpcomponent/TabularDataPreparationUsingPycaret/tests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWT-29j1XYhi",
        "outputId": "c6249dde-ce17-4822-d1eb-f6b49a5faecf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘kfpcomponent/TabularDataPreparationUsingPycaret’: File exists\n",
            "mkdir: cannot create directory ‘kfpcomponent/TabularDataPreparationUsingPycaret/src’: File exists\n",
            "mkdir: cannot create directory ‘kfpcomponent/TabularDataPreparationUsingPycaret/tests’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#it will ensure file is coped in git repo only if file content is changed by checking checksum of file content\n",
        "!rsync -c data_preparation.py kfpcomponent/TabularDataPreparationUsingPycaret/src\n",
        "!rsync -c component.yaml kfpcomponent/TabularDataPreparationUsingPycaret/component.yaml\n",
        "!rsync -c test_validation.py kfpcomponent/TabularDataPreparationUsingPycaret/tests\n",
        "!rsync -c Dockerfile kfpcomponent/TabularDataPreparationUsingPycaret/\n",
        "!rsync -c run_tests.sh kfpcomponent/TabularDataPreparationUsingPycaret/\n",
        "!rsync -c docker-compose.test.yml kfpcomponent/TabularDataPreparationUsingPycaret/"
      ],
      "metadata": {
        "id": "n3b7gcGhXde6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd kfpcomponent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RUDYMnmXlIW",
        "outputId": "bcec3a21-34e7-4fb2-fa9a-cbe58e0c0477"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kfpcomponent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add -A"
      ],
      "metadata": {
        "id": "iAyaTjClX9S0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW4fzKBsYCGO",
        "outputId": "28a55b50-d498-434c-d63d-eb223ecd5f31"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mmodified:   TabularDataPreparationUsingPycaret/Dockerfile\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For git-user who has set their email visibility as private, git provides alternate email address to use in web-based Git operations, e.g., edits and merges. The alias email can be viewed in https://github.com/settings/emails"
      ],
      "metadata": {
        "id": "bmHCKK7CzXNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"29448766+ShaswataJash@users.noreply.github.com\""
      ],
      "metadata": {
        "id": "FBLk0UVR04lj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -a -m \"changed docker to install libgomp1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbaNqafyYHV7",
        "outputId": "c764ce92-540b-47df-8ced-cc704ae59175"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 42a9e42] changed docker to install libgomp1\n",
            " 1 file changed, 2 insertions(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLZfkrleaqQ3",
        "outputId": "3eca20cb-c091-490b-9919-0b4c37b0cec4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 4, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  25% (1/4)   \rCompressing objects:  50% (2/4)   \rCompressing objects:  75% (3/4)   \rCompressing objects: 100% (4/4)   \rCompressing objects: 100% (4/4), done.\n",
            "Writing objects:  25% (1/4)   \rWriting objects:  50% (2/4)   \rWriting objects:  75% (3/4)   \rWriting objects: 100% (4/4)   \rWriting objects: 100% (4/4), 465 bytes | 465.00 KiB/s, done.\n",
            "Total 4 (delta 3), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
            "To https://github.com/ShaswataJash/kfpcomponent.git\n",
            "   31d731c..42a9e42  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwhyFmmz5bnu",
        "outputId": "7aff8d82-16e2-46a2-d1d2-efd2402e8021"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}